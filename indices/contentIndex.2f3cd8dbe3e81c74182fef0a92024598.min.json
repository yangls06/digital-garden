{"/":{"title":"🪴 无人之路.","content":"\n\n欢迎来到“无人之路”，走进我的数字花园。在这里，我汇集、整理自己在数据系统、自动驾驶、个人管理、人生体悟等方面的涉猎和思考，构建自己的知识系统和精神世界。\n\n\n\n个人的角色是多重的，对应的精神世界应该是丰富的。我有下面的各种身份：\n\n* 长期来看，我是一个数据工程师，致力于构建数据系统，以辅助数据化决策和AI系统。\n* 短期来看，我在自动驾驶行业工作，帮助自动驾驶团队构建从数据采集、数据传输、数据存储、数据挖掘、数据标注到数据消费的data pipeline，以实现数据闭环，助力自动驾驶的研发。\n* 终身来看，我是一个丈夫、父亲和儿子，想要经营一个和谐、进取的家庭；其基石是：我是一个在思想上不断拓展、追求自由的个人。\n\n\n\n无论上面的哪一方面，想要做好，都要付出持续的努力，走上不断积累、不断精进的道路。我期待在这个小小的数字花园里，翻土、播种、浇水、修建、施肥、授粉，打理出属于自己的一片世界。\n\n\n\nMy Digital Garden include: \n\n* [Python Programming](notes/python-programming.md)\n* [Data System](notes/data-system.md)\n* [Web Development](notes/web-development.md)\n* [DevOps](notes/devops.md)\n\n","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/Modern-Data-Stack-by-ChatGPT/TOC-by-ChatGPT":{"title":"","content":"# TOC\n\n\n\n## I. Introduction\n\n- What is a modern data stack?\n- The evolution of data processing and storage\n- Why is it important?\n\n## II. Data Collection and Ingestion\n\n- Types of data sources\n- Batch and real-time data ingestion\n- Tools and techniques for data collection and ingestion\n\n## III. Data Storage\n\n- Relational databases\n- NoSQL databases\n- Cloud data storage solutions\n- Data lake concepts and implementation\n\n## IV. Data Processing\n\n- Batch processing\n- Stream processing\n- Interactive processing\n- Distributed computing frameworks\n\n## V. Data Analysis and Visualization\n\n- Big data analysis tools and techniques\n- Machine learning and artificial intelligence\n- Data visualization tools and techniques\n- Dashboarding and reporting solutions\n\n## VI. Data Governance and Security\n\n- Data privacy and security\n- Data management and organization\n- Metadata management\n- Data quality and cleansing\n\n## VII. Conclusion\n\n- The future of the modern data stack\n- Best practices and recommendations\n- Common pitfalls and challenges\n- Final thoughts and next steps.\n\n\n\n## Introduction\n\n### What is a modern data stack?\n\n\n\nA modern data stack refers to the set of tools, technologies, and processes used for collecting, storing, processing, analyzing, and visualizing data in an efficient and scalable manner. A modern data stack typically consists of multiple components, each serving a specific purpose in the overall data pipeline.\n\nThe data stack starts with data collection and ingestion, where data is collected from various sources and loaded into a data storage solution. This can be done in a batch or real-time manner, depending on the requirements. The data storage can be a relational database, a NoSQL database, a cloud data storage solution, or a data lake, among others.\n\nOnce the data is stored, it can be processed using various tools and frameworks for batch processing, stream processing, or interactive processing. The processed data can then be analyzed using big data analysis tools and techniques, machine learning and artificial intelligence algorithms, and visualized using data visualization tools and techniques, dashboarding, and reporting solutions.\n\nFinally, data governance and security play a crucial role in ensuring the quality, privacy, and security of the data. This includes data management and organization, metadata management, data privacy and security, and data quality and cleansing.\n\n```mermaid\ngraph LR\nsubgraph Data Collection and Ingestion\n    ci[Data Collection and Ingestion]\nend\nsubgraph Data Storage\n    ds[Data Storage]\nend\nsubgraph Data Processing\n    dp[Data Processing]\nend\nsubgraph Data Analysis and Visualization\n    dav[Data Analysis and Visualization]\nend\nsubgraph Data Governance and Security\n    dgs[Data Governance and Security]\nend\nci --\u003e ds\nds --\u003e dp\ndp --\u003e dav\ndav --\u003e dgs\n```\n\n### The components of a Modern Data Stack\n\nA modern data stack typically includes the following components:\n\n1. Data Ingestion: This refers to the process of collecting and importing data into the data stack. This can be done through various methods, such as batch processing, real-time streaming, or data synchronization with external systems.\n2. Data Storage: This refers to the solution or technology used to store the data. This can be a traditional relational database, a NoSQL database, a data warehouse, or a cloud-based data storage solution.\n3. Data Processing: This refers to the solution or technology used to process and transform the data. This can be a batch processing solution, a real-time streaming solution, or a data processing engine.\n4. Data Governance: This refers to the tools and technologies used to manage the quality and governance of the data. This can include data catalogs, data quality tools, data lineage solutions, and data auditing tools.\n5. Data Security: This refers to the tools and technologies used to protect the data from cyber threats and data breaches. This can include data encryption, data access controls, and data auditing tools.\n6. Data Analytics: This refers to the tools and technologies used to analyze and gain insights from the data. This can include data visualization tools, machine learning algorithms, and predictive analytics models.\n\nEach component of a modern data stack is important and plays a crucial role in enabling organizations to process and store their data effectively. By having a complete and integrated data stack, organizations can ensure that they have a flexible, scalable, and secure infrastructure for their data, enabling them to make informed decisions and drive business growth.\n\n\n\n### The evolution of data processing and storage\n\n\n\nThe evolution of data processing and storage has gone through several phases, starting from the early days of computing to the modern era of big data and cloud computing.\n\n1. The Early Days of Computing: In the early days of computing, data was processed and stored on mainframe computers. Data was entered into these computers through punch cards and was stored on magnetic tapes. Data processing was performed in batch mode, and data storage was limited by the amount of physical storage available on the computer.\n2. The Emergence of Relational Databases: In the 1970s and 1980s, relational databases emerged as a new way of processing and storing data. Relational databases were designed to store structured data, and they used the relational model to store data in tables, with relationships between the tables defined by keys. Relational databases provided a flexible and scalable way of processing and storing data, and they became widely used in organizations for business applications and data management.\n3. The Rise of Big Data: In the late 2000s and early 2010s, the amount of data being generated and collected by organizations exploded, giving rise to the concept of big data. Big data refers to the large and complex datasets that organizations generate and collect, often from multiple sources and in multiple formats. The rise of big data presented new challenges for data processing and storage, as traditional relational databases were no longer able to handle the scale and complexity of the data.\n4. The Emergence of NoSQL Databases: In response to the challenges of big data, NoSQL databases emerged as a new way of processing and storing data. NoSQL databases were designed to handle large and complex datasets, and they used different data models, such as key-value, document, and graph, to store data. NoSQL databases provided organizations with a flexible and scalable way of processing and storing big data, and they became widely used for big data applications and data management.\n5. The Rise of Cloud Computing: In the 2010s and early 2020s, cloud computing emerged as a new way of delivering computing resources, including data processing and storage, over the internet. Cloud computing provided organizations with a flexible and scalable infrastructure for data processing and storage, without the need for expensive hardware and software. The rise of cloud computing has had a profound impact on the evolution of data processing and storage, as organizations can now leverage cloud-based data processing and storage solutions to handle large and complex datasets, without the need for expensive hardware and software.\n\nIn summary, the evolution of data processing and storage has gone through several phases, from the early days of computing to the modern era of big data and cloud computing. Each phase has brought new challenges and opportunities for data processing and storage, and has enabled organizations to process and store their data more effectively and efficiently.\n\n\n\nHere's a short timeline of the evolution of data processing and storage:\n\n- 1960s-1970s: Introduction of the first relational databases\n- 1980s-1990s: Development of enterprise-level relational databases (Oracle, SQL Server)\n- 2000s: Emergence of big data and NoSQL databases (Hadoop, MongoDB, Cassandra)\n- 2010s: Adoption of cloud computing and cloud data storage solutions (AWS, Azure, Google Cloud)\n- 2020s: Advancements in AI and machine learning and their integration into data processing and storage solutions.\n\nThis timeline highlights the key milestones in the evolution of data processing and storage and shows how new technologies and solutions have emerged over the years to meet the growing demand for more efficient and scalable data processing and storage solutions.\n\n\n\n### Why is it important?\n\nThe modern data stack refers to the combination of technologies, tools, and solutions that organizations use to process and store their data. In today's digital economy, where data is the new oil, it's essential for organizations to have a modern data stack in place for several reasons:\n\n1. Improved Business Insights: A modern data stack enables organizations to collect, process, and analyze large amounts of data from various sources, including structured and unstructured data. This helps organizations gain a comprehensive understanding of their customers, operations, and market trends, which can lead to more informed decision-making.\n2. Increased Data Agility: The modern data stack provides organizations with a flexible and scalable infrastructure for processing and storing data. This enables organizations to quickly adapt to changing business needs, such as new data sources, new analytics requirements, and new business initiatives, without being limited by their data infrastructure.\n3. Better Data Governance: A modern data stack includes solutions for data governance, such as data catalogs, data quality tools, and data lineage solutions. These tools help organizations manage their data more effectively, by providing a centralized repository for data definitions, data quality rules, and data lineage information.\n4. Improved Data Security: A modern data stack includes solutions for data security, such as data encryption, data access controls, and data auditing tools. These tools help organizations protect their sensitive data from cyber threats and data breaches, ensuring that their data remains secure and compliant with regulatory requirements.\n5. Lower Total Cost of Ownership (TCO): By using a modern data stack, organizations can reduce their TCO for data processing and storage. For example, cloud-based data processing and storage solutions can provide organizations with cost-effective solutions for processing and storing large amounts of data, without the need for expensive hardware and software.\n\nIn summary, the modern data stack is essential for organizations in today's digital economy, as it provides organizations with the tools and technologies they need to manage their data more effectively, gain better insights from their data, and reduce their TCO for data processing and storage.\n\n\n\n## Data Collection and Ingestion\n\nData collection and ingestion is the process of acquiring and processing data from various sources and preparing it for storage and analysis. The data collection and ingestion process is a crucial part of the modern data stack, as it determines the quality and reliability of the data that will be used for analysis and decision-making.\n\n\n\n### Data Sources\n\n\n\nData sources refer to the places where data originates and can be collected. Data sources can come in various forms, including internal systems and databases, external APIs, text files, and images. The following are the most common data sources:\n\n1. Internal Systems and Databases: This type of data source refers to the data stored within an organization's own systems and databases. This data can include customer data, sales data, and employee data.\n2. External APIs: An API (Application Programming Interface) is a set of protocols and tools for building software applications. APIs can be used to access data from external sources, such as social media platforms, weather services, and online marketplaces.\n3. Text Files: Data can also be collected from text files such as CSV (Comma-Separated Values) or JSON (JavaScript Object Notation) files. These files can contain structured or unstructured data, depending on the format in which they are stored.\n4. Images: Data can also be collected from images, such as satellite images, drone images, and medical images. These images can be analyzed using computer vision techniques to extract valuable information.\n\nData sources can be either structured or unstructured. Structured data refers to data that is organized in a specific format, such as tables, while unstructured data refers to data that does not have a specific format, such as text and images.\n\nIn conclusion, data sources are the places where data originates and can be collected. Data sources can range from internal systems and databases to external APIs, text files, and images. The type and format of the data collected from these sources will determine the type of data collection and ingestion process that will be used.\n\n\n\n### Data Ingestion\n\n\n\n#### Types\n\nData ingestion refers to the process of importing data from various sources into a centralized storage system for further processing and analysis. The following are the most common methods of data ingestion:\n\n1. Batch Ingestion: Batch ingestion is the process of importing data in large amounts into a centralized storage system. This method is used when the data is available in a static form and needs to be processed in bulk.\n\n2. Real-time Ingestion: Real-time ingestion is the process of importing data into a centralized storage system as soon as it becomes available. This method is used when the data is generated in real-time and needs to be processed immediately.\n\n3. Stream Ingestion: Stream ingestion is a combination of batch and real-time ingestion, where data is processed in real-time but stored in batch mode. This method is used when the data is generated in real-time but needs to be processed in bulk for analysis purposes.\n\n   \n\nIn Chapter 3.1: Data Sources, we discussed the various types of data sources including databases, log files, cloud services, and social media platforms. The following are the most common methods of data ingestion for each data source:\n\n1. Databases: Data can be ingested from databases using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n2. Log Files: Log files can be ingested using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n3. Cloud Services: Cloud services can be ingested using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n4. Social Media Platforms: Social media platforms can be ingested using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n\nData ingestion is a crucial step in the modern data stack as it determines the quality and reliability of the data that will be used for further analysis. To ensure data quality, data ingestion processes should include steps for data validation, data cleaning, and data normalization.\n\nIn conclusion, data ingestion refers to the process of importing data from various sources into a centralized storage system. The method used for data ingestion will depend on the type and format of the data being collected and the data source it is being collected from. It is important to have a robust data ingestion process in place to ensure data quality and reliability for further analysis.\n\n\n\n#### ETL\n\nThe ETL (Extract, Transform, Load) process is a widely used method for data ingestion in the modern data stack. The ETL process involves the following steps:\n\n1. Extract: The first step of the ETL process is to extract data from various sources such as databases, log files, cloud services, and social media platforms. Data extraction is typically performed using data extraction tools and scripts.\n2. Transform: The second step of the ETL process is to transform the extracted data into a format that is suitable for analysis and storage. Data transformation involves various activities such as data cleaning, data normalization, data enrichment, and data integration. This step is performed using data transformation tools and scripts.\n3. Load: The final step of the ETL process is to load the transformed data into a centralized storage system such as a data lake or a data warehouse. This step is performed using data loading tools and scripts.\n\nThe ETL process is important in the modern data stack as it enables organizations to collect and process large amounts of data from various sources in a consistent and reliable manner. The transformed data can then be used for further analysis and reporting purposes.\n\nIt is important to note that the ETL process can be time-consuming and resource-intensive, especially for large data sets. To minimize the impact on system performance, organizations often use parallel processing and distributed systems to perform the ETL process. Additionally, organizations may use cloud-based ETL solutions to take advantage of the scalability and cost-effectiveness of cloud computing.\n\nIn conclusion, the ETL process is a widely used method for data ingestion in the modern data stack. It involves the steps of extracting data from various sources, transforming the data into a suitable format, and loading the transformed data into a centralized storage system. The ETL process is important for collecting and processing large amounts of data in a consistent and reliable manner.\n\n\n\n### Data Quality\n\nData quality is a critical aspect of data ingestion in the modern data stack. It refers to the accuracy, completeness, consistency, and reliability of the data being ingested. Poor data quality can lead to incorrect insights and decisions, as well as wasted time and resources.\n\nIn the context of data ingestion, data quality checks are performed during the transform step of the ETL process. During this step, data quality checks are used to identify and correct issues such as missing values, duplicate records, and inconsistent data formats.\n\nThere are several data quality checks that are commonly performed during the transform step of the ETL process, including:\n\n1. Data Validation: Data validation checks are performed to ensure that the data conforms to certain rules or constraints. For example, data validation checks can be used to ensure that a date field contains a valid date format or that a string field contains only alphanumeric characters.\n2. Data Cleansing: Data cleansing is the process of removing or correcting invalid, incomplete, or inconsistent data. This step is important for ensuring that the data is accurate and consistent, and can be used for further analysis and reporting purposes.\n3. Data Normalization: Data normalization is the process of transforming data into a standardized format. This step is important for ensuring that data from different sources is consistent and can be easily integrated.\n4. Data Enrichment: Data enrichment is the process of adding additional data to the data being ingested. This step is often used to add contextual information or to enhance the data with additional information from external sources.\n\nIt is important to note that data quality checks can be time-consuming and resource-intensive, especially for large data sets. To minimize the impact on system performance, organizations often use parallel processing and distributed systems to perform the data quality checks. Additionally, organizations may use cloud-based data quality solutions to take advantage of the scalability and cost-effectiveness of cloud computing.\n\nIn conclusion, data quality is a critical aspect of data ingestion in the modern data stack. Ensuring data quality is important for ensuring that the data being ingested is accurate, complete, consistent, and reliable, and can be used for further analysis and reporting purposes. Data quality checks are performed during the transform step of the ETL process, and may include data validation, data cleansing, data normalization, and data enrichment.\n\n\n\n### Data Ingestion Tools\n\nData ingestion tools are software solutions used to collect and import data into a data repository. These tools play a crucial role in the modern data stack and are essential for processing and storing data effectively.\n\nThere are several types of data ingestion tools, each with its own set of features and capabilities. These include:\n\n1. Batch Ingestion Tools: These tools are designed for importing large amounts of data into a data storage system in a single transaction. Examples include Apache Nifi and Talend Open Studio.\n2. Real-time Ingestion Tools: These tools are designed for continuously streaming data from various sources into a data storage system in real-time. Examples include Apache Kafka and Amazon Kinesis.\n3. File-based Ingestion Tools: These tools are designed for processing and importing structured or semi-structured data in the form of files, such as CSV or JSON. Examples include Apache Flume and Logstash.\n4. API-based Ingestion Tools: These tools allow for data ingestion via APIs. They can be used to integrate data from cloud-based applications such as Salesforce and Google Analytics into a data storage system. Examples include Talend Cloud and MuleSoft.\n5. Cloud-based Data Integration Tools: These tools are specifically designed for data integration in cloud environments. They provide a set of tools and services to move and integrate data between cloud-based data sources and storage systems. Examples include FiveTran and AirByte.\n6. Custom Data Ingestion Tools: These are specialized data ingestion tools that are built for specific use cases or to address specific challenges. They can be built from scratch or using open-source tools and technologies. Examples include custom scripts and data pipelines.\n\nRegardless of the type of data ingestion tool used, it is important to ensure that data quality is maintained throughout the ingestion process. This involves validating and transforming the data as it is being imported, and ensuring that the data is properly formatted and structured.\n\n\n\n#### Apache NiFi\n\nApache NiFi is a powerful, open-source software platform for automating and managing the flow of data between systems. It is designed to be highly scalable, fault-tolerant, and flexible, making it an ideal choice for handling large amounts of data in real-time.\n\nAt its core, Apache NiFi provides a web-based interface for designing and managing data flows. Users can drag and drop a variety of data processing and transformation components onto a canvas and connect them to create complex data pipelines.\n\nOne of the key benefits of Apache NiFi is its ability to handle a wide range of data sources and formats. It can ingest data from various sources, such as databases, message queues, web services, and sensors, and can process data in real-time. Additionally, Apache NiFi provides many built-in data transformation and processing functions, allowing users to perform tasks such as data enrichment, aggregation, and filtering with ease.\n\nAnother advantage of Apache NiFi is its ability to handle large volumes of data with ease. It can scale horizontally to handle large amounts of data, and it can be easily configured to handle different levels of data volume and velocity. This makes it an ideal choice for organizations that need to process large amounts of data in real-time.\n\nOverall, Apache NiFi is a powerful and flexible tool for data ingestion and management, offering a wide range of capabilities for organizations that need to process large amounts of data in real-time.\n\n\n\nApache NiFi has several use cases:\n\n1. Data routing and transformation: NiFi can be used to route, transform and process data from various sources to its final destination.\n2. Data ingestion for big data systems: NiFi is commonly used to ingest large amounts of data into big data systems like Hadoop and Apache Spark.\n3. Data integration: NiFi can be used to integrate data from multiple systems and format it into a standardized format.\n4. Real-time streaming data: NiFi can be used to process real-time streaming data and feed it into data lakes, data warehouses or other big data systems.\n5. Internet of Things (IoT) data management: NiFi is well-suited for handling large amounts of data from IoT devices and sensors, making it a popular choice for IoT data management.\n6. Security information and event management (SIEM): NiFi can be used as a SIEM to collect, store, and analyze log data from various sources for security purposes.\n\n\n\n#### Apache Kafka\n\nApache Kafka is a distributed streaming platform that is used for building real-time data pipelines and streaming applications. Some key features and benefits of Apache Kafka include:\n\n1. Scalability: Apache Kafka is designed to handle high volumes of data in real-time and can scale horizontally by adding more brokers to the cluster.\n2. Durability: Apache Kafka stores all data on disk, providing durability in case of failures and ensuring that data is not lost.\n3. Performance: Apache Kafka is optimized for high performance and low latency, making it suitable for processing large amounts of data in real-time.\n4. Real-time streaming: Apache Kafka enables real-time data streaming and processing, making it a popular choice for use cases such as log aggregation, event sourcing, and IoT.\n5. Publish-Subscribe Model: Apache Kafka uses a publish-subscribe model, where producers publish messages to topics and consumers subscribe to topics to receive messages. This model provides decoupling between producers and consumers and enables horizontal scaling of consumers.\n6. Multi-language support: Apache Kafka provides APIs for multiple programming languages, making it easy to integrate with a wide range of systems and applications.\n\n\n\nThe internal architecture of Apache Kafka consists of four core components:\n\n1. Topics: Topics are categories or feed names to which messages are published.\n2. Partitions: A topic can be divided into multiple partitions, allowing parallel processing of the data stream. Each partition is an ordered, immutable sequence of messages.\n3. Brokers: A Kafka cluster consists of one or more servers, known as brokers, that run Kafka. They are responsible for maintaining the data in the topics and for serving client requests.\n4. Producers: Producers are the clients that publish messages to topics in the broker.\n5. Consumers: Consumers are the clients that subscribe to topics and process the published messages.\n6. Zookeeper: Apache Zookeeper is used to manage the configuration of the Kafka cluster, to coordinate the actions of the brokers and to provide failover in case of broker failures.\n\nSome use cases of Apache Kafka include:\n\n- Real-time data processing and analysis\n- Data integration between multiple systems\n- Building event-driven architectures\n- Log aggregation and analysis\n- Monitoring and alerting.\n\nSome popular examples of companies using Apache Kafka include Netflix, LinkedIn, and Uber.\n\n\n\n#### Logstash\n\nLogstash is an open source data collection and processing pipeline tool. It is part of the larger Elastic Stack, which also includes Elasticsearch, Kibana, and Beats. Logstash is designed to ingest, process, and transfer data from a variety of sources and output it to a variety of destinations.\n\nOne of the key features of Logstash is its ability to process and manipulate data in real-time. It has a wide range of input plugins that can be used to collect data from various sources, including files, databases, message brokers, and HTTP endpoints. Logstash also provides a number of filter plugins for transforming and manipulating data, including data enrichment, data normalization, and data filtering. The output plugins of Logstash allow the processed data to be sent to a variety of destinations, including Elasticsearch, Apache Kafka, files, and databases.\n\nLogstash is highly configurable and can be used to create custom data processing pipelines to fit specific use cases. It supports many programming languages, including Ruby, Java, and Groovy, and provides a rich set of APIs for data processing. It can be run on-premises, in the cloud, or in a hybrid deployment, making it a flexible tool for data collection and processing.\n\nSome examples of use cases for Logstash include log management, security event analysis, and IoT data processing. It is often used in combination with the other components of the Elastic Stack for centralized logging, real-time analytics, and visualizing data.\n\n\n\n#### Talend Cloud\n\nTalend Cloud is a cloud-based data integration tool that helps organizations to manage, process, and integrate large amounts of data from various sources into a single platform. It provides a wide range of tools and features to streamline data integration, including data ingestion, data transformation, and data delivery. The tool supports various data sources, including databases, big data platforms, cloud services, and file systems, and can be used to extract, transform, and load data into data warehouses, data lakes, or other data storage platforms.\n\nSome of the key features of Talend Cloud include:\n\n- Data Quality and Governance: Talend Cloud provides data quality and governance features to ensure the accuracy, completeness, and consistency of data.\n- Data Mapping: The tool provides an intuitive drag-and-drop interface for mapping data from source to target systems.\n- Real-time Processing: Talend Cloud supports real-time data processing, enabling organizations to make decisions based on real-time data.\n- Scalability: Talend Cloud is designed to scale with organizations as they grow, enabling them to integrate and manage large amounts of data.\n- Security: The tool provides robust security features to protect sensitive data, including encryption, access control, and audit trails.\n\nExamples of organizations that use Talend Cloud include KPMG, GE Healthcare, and Coca-Cola.\n\n\n\n#### FiveTran\n\nFiveTran is a cloud-based data integration tool that simplifies the process of connecting and collecting data from various sources into a central data warehouse. It provides a fast and reliable solution to automate the process of extracting, transforming, and loading (ETL) data into a data warehouse. FiveTran supports over 100 different data sources, including popular cloud applications like Salesforce, Hubspot, and Google Analytics, as well as databases and APIs.\n\nOne of the key features of FiveTran is its ability to support real-time data replication, which ensures that the data in the data warehouse is always up-to-date. Additionally, it provides a user-friendly interface to manage the entire data pipeline, making it easy for users to monitor and troubleshoot any issues that may arise during the data ingestion process.\n\nSome of the use cases for FiveTran include:\n\n- Centralizing data from disparate sources for business intelligence and data analysis.\n- Automating the process of moving data from multiple sources into a data warehouse for analysis and reporting.\n- Enabling real-time data analysis and reporting by ensuring that data in the data warehouse is up-to-date.\n\nOverall, FiveTran is an ideal solution for organizations that are looking to streamline and simplify their data integration process, allowing them to focus on data analysis and insights instead of dealing with complex data pipelines.\n\n\n\n#### AirByte\n\nAirbyte is a cloud-based data integration platform that helps businesses centralize, clean and transform data from various sources. It's designed for ease of use and can be set up in a matter of minutes. Airbyte allows businesses to quickly connect to popular data sources such as databases, cloud-based software, and SaaS tools, and automatically replicate data to a central location. The platform includes built-in data transformation and enrichment capabilities, which make it easy to clean and standardize data. Additionally, Airbyte allows users to track and visualize data lineage, which helps with data governance and auditability. Some common use cases for Airbyte include data warehousing, data lake ingestion, data analysis, and reporting.\n\nAirbyte and FiveTran are both cloud-based data integration tools. They are designed to help organizations easily collect, clean, and move data from various sources to a centralized data warehouse.\n\nFiveTran is a fully managed, cloud-native data integration platform that provides a simple, scalable, and automated way to move data from various sources to your data warehouse. It provides out-of-the-box integrations for popular applications and data sources, allowing you to quickly and easily set up your data pipeline.\n\nAirbyte, on the other hand, is an open-source data integration platform that provides a similar range of functionality to FiveTran, but allows for greater customization and control over your data pipeline. It supports a wide range of sources, including databases, SaaS apps, and cloud storage, and provides a flexible, modular architecture that allows you to easily add custom transformations and manipulate data as it is being processed.\n\nIn terms of differences, FiveTran may be a better choice for organizations that need a simple, turn-key solution that can be set up quickly, while Airbyte may be a better choice for organizations that require more control and customization over their data pipeline. Additionally, FiveTran is a commercial product, while Airbyte is open source, so organizations that want to save money may prefer Airbyte.\n\n\n\n#### Apache Airflow \n\nApache Airflow is an open-source workflow management platform that allows users to programmatically author, schedule, and monitor workflows. It was initially developed by Airbnb, and has since become a widely adopted tool for managing complex workflows in the data engineering and data science fields.\n\nOne of the key features of Apache Airflow is its ability to handle multi-task workflows. It allows users to define a series of tasks and the dependencies between them, and then execute those tasks in the correct order. Tasks can be executed on a schedule, triggered by events, or run on-demand.\n\nAnother important feature of Apache Airflow is its extensibility. It provides a number of plugins that allow users to extend the functionality of the platform, including integrations with popular data storage and processing technologies like Apache Hive, Apache Spark, and Apache Pig.\n\nFinally, Apache Airflow includes a number of tools for monitoring and troubleshooting workflows. It provides a web-based user interface that provides real-time updates on the status of running workflows, as well as the ability to view logs, track errors, and view performance metrics.\n\n\n\nOne of the key features of Airflow is its UI, which provides a visual representation of your workflows and their status, making it easy to understand what's happening in your pipelines. Additionally, Airflow has a large and active community, which means there is a wealth of resources and plugins available to help you solve common problems.\n\nAirflow is used by many organizations in a variety of industries, from technology companies to financial services and beyond. Some of the most common use cases for Airflow include:\n\n- ETL pipelines: Airflow is often used to automate the process of extracting, transforming, and loading data from a variety of sources into data warehouses or data lakes.\n- Machine learning workflows: Airflow can be used to manage the steps involved in training machine learning models, including data preparation, model training, and model evaluation.\n- Monitoring workflows: Airflow can be used to automate tasks related to system and application monitoring, such as pulling log data, sending notifications, and generating reports.\n\nOverall, Apache Airflow is a highly flexible and scalable platform for managing data workflows, making it a popular choice for organizations that need to process large amounts of data in a reliable and efficient manner.\n\n\n\n## Data Storage\n\n### Overview\n\nData storage refers to the process of preserving data in a persistent manner, either on physical or virtual storage mediums. With the increasing amount of data generated every day, modern data storage systems are designed to store and manage vast amounts of data efficiently, securely, and cost-effectively.\n\n\n\nTypes of Data Storage\n\nThere are several types of data storage systems available, including:\n\n- Relational databases (RDBMS): These databases store data in tables and columns, and support Structured Query Language (SQL) for querying and manipulating data. Examples include MySQL, PostgreSQL, and Microsoft SQL Server.\n- NoSQL databases: These databases store data in a non-relational manner and support flexible, semi-structured data. Examples include MongoDB, Cassandra, and Couchbase.\n- Data warehousing: Data warehousing systems are designed to store large amounts of structured data and support fast querying and analysis. Examples include Amazon Redshift, Google BigQuery, and Snowflake.\n- Distributed file systems: These systems store and manage large amounts of unstructured data across multiple servers and storage devices. Examples include Apache Hadoop HDFS, Apache Cassandra FS, and GlusterFS.\n\n\n\nData Storage Considerations\n\nWhen selecting a data storage system, there are several factors to consider, including:\n\n- Scalability: The ability to increase the capacity of the system as needed to accommodate growing amounts of data.\n- Performance: The speed and efficiency with which data can be stored, retrieved, and processed.\n- Reliability: The ability of the system to maintain data integrity and availability in the event of failures or errors.\n- Security: The measures in place to protect data from unauthorized access or manipulation.\n- Cost: The cost of purchasing, maintaining, and scaling the system.\n\n\n\nConclusion\n\nData storage is a critical component of a modern data stack, and there are many different types of data storage systems available to suit different requirements. Choosing the right data storage system requires careful consideration of factors such as scalability, performance, reliability, security, and cost.\n\n\n\n### Relational databases\n\nRelational databases, also known as RDBMS (Relational Database Management Systems), are databases that store data in tables with rows and columns, forming a structured relationship between data elements. These databases are based on the relational model proposed by E.F. Codd in 1970.\n\nIn a relational database, data is organized into tables, with each table representing a specific type of data, such as customer information, product information, or sales transactions. Each table has a set of columns that describe the properties or attributes of the data in that table. For example, a customer information table might have columns for first name, last name, email address, and postal code. Rows in the table represent individual records, with each row representing a single customer.\n\nRelational databases use a language called Structured Query Language (SQL) to query and manipulate data stored in tables. This allows for the creation of complex queries and the ability to join data from multiple tables to produce a single result set.\n\nRelational databases are widely used for a variety of applications and are a popular choice for managing structured data due to their ability to enforce relationships between data, ensuring data integrity and consistency. They also provide robust security features, scalability, and support for transactions. Examples of relational databases include MySQL, Oracle, and Microsoft SQL Server.\n\n\n\n### NoSQL databases\n\nNoSQL databases are a type of non-relational database that are designed to handle large amounts of data that don't fit neatly into a traditional relational database structure. Unlike relational databases, which use tables and rows to store data, NoSQL databases use a variety of data storage structures, such as key-value pairs, document databases, column-family databases, and graph databases. NoSQL databases are often used for large-scale, high-performance web and mobile applications, as they are designed to be flexible, scalable, and highly available.\n\nSome of the key benefits of NoSQL databases include:\n\n1. Scalability: NoSQL databases can easily scale horizontally, meaning that more servers can be added to the system as needed to handle growing amounts of data.\n2. Flexibility: NoSQL databases can handle semi-structured or unstructured data, which is common in many modern applications.\n3. Performance: NoSQL databases are designed to be fast and efficient, making them well-suited for high-performance applications.\n4. Cost-effectiveness: NoSQL databases are often open source and have lower hardware requirements compared to relational databases, making them a cost-effective option for many organizations.\n\nSome examples of popular NoSQL databases include MongoDB, Cassandra, Redis, and Couchbase.\n\n\n\nMore details about MongoDB, Cassandra, Redis, and Couchbase:\n\nMongoDB: MongoDB is a popular NoSQL database that uses a document-based data model. This means that data is stored as documents within a collection, rather than in tables with columns and rows like a relational database. MongoDB is designed for high performance, with automatic sharding that distributes data across multiple servers, and a flexible schema that makes it easy to store and query complex data.\n\nCassandra: Cassandra is a distributed NoSQL database that is optimized for scalability and high availability. It uses a column-family data model, where data is organized into columns instead of rows. Cassandra provides linear scalability and can handle high write loads, making it well-suited for applications that need to store and process large amounts of data.\n\nRedis: Redis is an in-memory data store that can be used as a database, cache, or message broker. It uses a key-value data model, where data is stored as key-value pairs in memory, making it very fast for read and write operations. Redis also supports advanced data structures such as lists, sets, and hashes, making it a good choice for applications that require complex data structures.\n\nCouchbase: Couchbase is a NoSQL database that combines the scalability and performance of a NoSQL database with the ease of use and functionality of a relational database. It uses a document-based data model and supports indexing, querying, and full-text search, making it a good choice for applications that need to store and query complex data structures. Couchbase also includes a built-in caching layer and supports automatic data replication and failover, ensuring high availability and data durability.\n\n\n\n### Cloud data storage solutions\n\nCloud data storage solutions refer to the data storage infrastructure provided by cloud computing service providers. They provide a scalable and cost-effective alternative to traditional on-premise data storage solutions. The data is stored on remote servers and can be accessed from anywhere with an internet connection.\n\nSome examples of cloud data storage solutions include:\n\n- Amazon Simple Storage Service (S3)\n- Microsoft Azure Blob Storage\n- Google Cloud Storage\n- IBM Cloud Object Storage\n\nCloud data storage solutions offer several advantages such as scalability, reliability, cost savings, and flexibility. They allow users to store unlimited amounts of data without having to worry about the physical storage space or maintaining the infrastructure. The storage capacity can be easily increased or decreased based on the user's requirements, and the cost is typically charged on a pay-as-you-go basis. Additionally, the data stored in the cloud is highly secure and can be accessed from anywhere with an internet connection, making it an ideal solution for organizations with a remote workforce.\n\n\n\nTake S3 for example. \n\n\n\nAmazon Simple Storage Service (S3) is a cloud-based object storage service offered by Amazon Web Services (AWS). S3 provides scalable and durable storage for a variety of data types, including binary files, text files, and structured data. S3 stores data as objects in a bucket, and objects are accessible via a unique identifier known as a key.\n\nS3 offers a range of storage classes, including standard, infrequent access, and archive, that allow customers to store data at different levels of availability and cost. S3 also offers built-in data management features, such as versioning and lifecycle policies, that make it easy to manage the data stored in the service. Additionally, S3 provides a robust security model that includes the ability to encrypt data at rest and in transit, control access to objects with fine-grained IAM policies, and monitor access to objects with AWS CloudTrail.\n\nS3 is widely used as a data lake for big data analytics, as well as for storing backups and archiving data. Its scalability, durability, and low cost make it an attractive option for many organizations looking to store large amounts of data in the cloud.\n\n\n\n### Data lake concepts and implementation\n\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. The main idea behind a data lake is to store data in its raw form, without any pre-processing or transformation, and then process the data on an as-needed basis. This allows organizations to store massive amounts of data in a cost-effective way, while maintaining the flexibility to process the data in the future as business needs change.\n\nData lakes are implemented using cloud-based storage solutions, such as Amazon Simple Storage Service (S3), Microsoft Azure Data Lake Storage, or Google Cloud Storage. These storage solutions are scalable and cost-effective, making it easy for organizations to store large amounts of data.\n\nOnce the data is stored in a data lake, it can be processed using big data tools such as Apache Hadoop, Apache Spark, or Apache Hive. These tools allow organizations to perform complex data processing and analysis, without the need for expensive hardware or software.\n\n\n\nA data lake application example could be a retail company using a data lake to store and process data from multiple sources such as customer purchase history, website clicks, and marketing campaign data. The company can then use this data to perform advanced analytics and generate insights, such as identifying the most popular products, tracking customer behavior, and optimizing marketing campaigns. The data lake enables the company to store structured, semi-structured, and unstructured data in its raw form, providing a centralized repository for all their data. The company can then use tools such as Apache Spark or Apache Hive to perform big data processing on the data stored in the data lake and generate insights for business decision making.\n\n\n\nIn summary, a data lake provides organizations with a centralized repository for storing all their data, while providing the flexibility to process the data in the future as business needs change. The use of cloud-based storage solutions and big data tools make data lakes a cost-effective solution for storing and processing large amounts of data.\n\n\n\n## Data Processing\n\n### Data Processing Overview\n\nThis chapter provides an overview of data processing, explaining the different types of data processing such as batch processing, real-time processing, and stream processing. It covers the purpose of data processing and why it is important in the context of big data.\n\n\n\nData Processing is a critical aspect of the data pipeline. It involves transforming raw data into a format that can be analyzed and used to derive insights. The data processing phase is crucial in ensuring that the data is clean, consistent, and in a format that can be easily utilized by data analysts and scientists.\n\nThere are several key components of data processing, including data extraction, data cleaning, data transformation, and data enrichment. Data extraction involves retrieving the raw data from various sources, including databases, APIs, and file systems. Data cleaning involves removing any irrelevant, inconsistent, or duplicate data. Data transformation involves converting the raw data into a format that can be easily analyzed, such as aggregating data, normalizing data, or pivoting data. Data enrichment involves adding additional information to the data, such as geolocation data or demographic data.\n\nIn order to achieve high-quality data processing, it is important to implement automated processes, as well as to apply best practices such as data validation and data quality checks. This can help to ensure that the data is accurate, consistent, and of high quality, which is essential for making informed decisions and achieving meaningful insights.\n\n\n\n### Batch Processing\n\n\n\nBatch processing refers to the method of processing large data sets in a batch mode, rather than processing data in real-time. This approach is typically used for offline processing and is often used to process high volumes of data in a cost-effective manner. The batch processing is performed in a sequential order, processing data in chunks or \"batches\" rather than processing one data record at a time. This method of processing is well suited for large data sets that do not require immediate processing or real-time results.\n\nIn batch processing, data is first collected, typically from various sources such as databases, file systems, or other data storage systems. The data is then processed in a predefined sequence, often using specialized software or tools, to transform the data into a format that is suitable for analysis, reporting, or further processing. The transformed data is then typically stored in a data store such as a data warehouse or a data lake for future analysis.\n\nOne of the primary benefits of batch processing is its ability to handle large volumes of data in a cost-effective manner. Batch processing can also handle complex data transformations and manipulation tasks, making it ideal for data cleansing, data integration, and other data management tasks. Additionally, batch processing can be scheduled to run at specific times, allowing organizations to plan and manage their data processing resources more effectively.\n\nExamples of batch processing include payroll processing, sales data analysis, and monthly reporting. Batch processing can be performed using a variety of tools and technologies, including Apache Hadoop, Apache Spark, and Apache Storm, among others.\n\n\n\n#### Hadoop\n\nApache Hadoop is an open-source software framework for storing and processing big data. It provides a scalable and fault-tolerant platform for distributed processing of large datasets. It was created by the Apache Software Foundation and is widely used for big data processing, storage and analysis. Hadoop is designed to work in a cluster environment, where multiple computers work together to process large amounts of data.\n\nHadoop has two main components: Hadoop Distributed File System (HDFS) and MapReduce. HDFS is a distributed file system that stores large data files across multiple nodes in a cluster. This allows for processing and analysis of very large datasets. MapReduce is a programming model for processing large datasets in parallel. It takes input data, divides it into smaller chunks, processes the chunks on different nodes in parallel, and combines the results into a final output.\n\nHadoop also includes other components such as YARN (Yet Another Resource Negotiator), which is a resource management system that allocates resources to the various processing tasks, and Pig, a high-level data processing language that can be used to write MapReduce programs.\n\nHadoop has become an important tool for big data processing and is widely used by organizations of all sizes for tasks such as log analysis, recommendation systems, fraud detection and more.\n\n\n\n#### Spark\n\nApache Spark is a fast, open-source, and general-purpose big data processing engine. It provides an in-memory computing framework that allows data scientists and engineers to perform distributed computing tasks faster and more efficiently than traditional batch processing frameworks like Hadoop MapReduce. Spark supports multiple programming languages such as Scala, Java, and Python, and provides APIs for high-level data analysis and machine learning tasks.\n\nOne of the main advantages of Apache Spark over Hadoop MapReduce is its ability to perform data processing in memory, as well as disk, which leads to faster processing speeds. Spark also provides an optimized engine for executing SQL-like queries, streaming data processing, machine learning algorithms, and graph processing.\n\nApache Spark has a large and active open-source community and has been adopted by many organizations due to its performance, scalability, and ease of use. Additionally, Spark integrates with popular big data storage systems such as Hadoop Distributed File System (HDFS), Apache Cassandra, and Apache HBase, which makes it a popular choice for big data processing.\n\n\n\n### Real-time Processing\n\nReal-time processing refers to the processing of data as soon as it is generated or received. It is a form of data processing that enables organizations to make decisions in near real-time, providing immediate business value. Real-time processing is becoming increasingly important for a variety of applications, including financial services, e-commerce, gaming, telecommunications, and more.\n\nTo achieve real-time processing, data must be processed as soon as it is received, with minimal latency. This requires the use of technologies such as in-memory data stores, distributed processing frameworks, and stream processing engines.\n\nOne of the key benefits of real-time processing is that it enables organizations to respond quickly to changing business conditions, such as market trends or customer behavior. For example, real-time processing can be used to monitor and analyze customer interactions, detect fraud, or provide recommendations based on customer behavior.\n\nSome popular real-time processing technologies include Apache Kafka, Apache Flink, and Apache Storm. These technologies provide a distributed and scalable architecture that can handle high volumes of data in real-time. They also provide the ability to process data in a stream-oriented manner, enabling organizations to process large amounts of data as soon as it is generated.\n\nOverall, real-time processing is a critical component of modern data architectures, enabling organizations to make faster and more informed decisions based on real-time data insights.\n\n\n\n### Apache Kafka\n\nApache Kafka is a distributed, high-throughput, and fault-tolerant data processing system originally developed by LinkedIn. It is designed for real-time streaming and processing of large scale data, allowing organizations to handle data in real-time with low latency.\n\nKafka uses a publish-subscribe model where data producers write data to Kafka topics, which are then consumed by data consumers. The system stores data in a partitioned and replicated manner, which provides reliability and scalability. The data stored in Kafka topics is retained for a configurable amount of time, providing a historical view of the data stream.\n\nOne of the key features of Apache Kafka is its ability to handle high volumes of data with low latency, making it suitable for use cases such as real-time analytics, fraud detection, and data integration. Additionally, Kafka provides strong durability guarantees, as data is replicated across multiple nodes and can be recovered in the event of failures.\n\nIn addition to its core functionality, Apache Kafka also provides a number of additional features such as security, data compression, and automatic data balancing. These features make it an attractive choice for organizations looking to process large amounts of real-time data, and it has been adopted by many organizations in a variety of industries, including finance, healthcare, and e-commerce.\n\n\n\n### Apache Flink\n\nApache Flink is an open-source, distributed stream processing framework designed to handle high-volume data processing in real-time. It supports batch processing and event-driven stream processing and can handle large-scale data processing across many parallel processing nodes.\n\nFlink was developed to address the limitations of existing stream processing frameworks, such as complex setup and configuration, limited scalability, and limited fault tolerance. It provides a high-level API for processing data in real-time, as well as a low-level API for implementing custom data processing logic.\n\nFlink's architecture is built around a data streaming model, with a central control system that manages the flow of data between different processing nodes. The framework supports a wide range of data sources, including Apache Kafka, Apache Cassandra, and Hadoop HDFS, and it can be integrated with other big data tools and technologies, such as Apache Hadoop and Apache Spark.\n\nOne of the key benefits of Flink is its ability to process and analyze data in real-time, without the need for batch processing. This enables organizations to make faster and more informed decisions based on the latest data, and to quickly respond to changing business conditions.\n\nIn addition, Flink provides a number of advanced features, such as windowing and state management, that enable it to handle complex data processing requirements, such as aggregating data over time windows and maintaining state information between events.\n\nOverall, Apache Flink is a powerful and flexible data processing framework that is well-suited for real-time data processing and analytics in big data environments.\n\n\n\n## Stream Processing\n\nStream Processing is a data processing paradigm in which data is processed in real-time as it is generated or received. In contrast to batch processing, where data is processed in large chunks at fixed intervals, stream processing allows for near-instant analysis of incoming data. This is especially useful for high-volume, high-velocity data, where traditional batch processing can quickly become overwhelmed.\n\nIn a stream processing system, incoming data is divided into small chunks, called events or records, which are processed one at a time in near real-time. This allows the system to detect and respond to events as soon as they occur, making it well-suited for use cases such as real-time monitoring, event-driven applications, and real-time analytics.\n\nStream processing systems typically include several key components, including data sources, stream processors, and data sinks. Data sources provide the incoming data stream, which is then processed by stream processors. Stream processors perform various operations on the incoming data, such as filtering, aggregating, and transforming, before finally sending the processed data to data sinks for storage or further processing.\n\n\n\n### Spark Streaming\n\nSpark Streaming is a real-time data processing framework for big data applications, built on top of the Apache Spark engine. It enables the processing of live data streams, making it possible to work with and analyze high-volume, high-velocity data from a variety of sources, including logs, sensors, social media, and financial data.\n\nSpark Streaming operates in near-real-time, with a batch processing interval as low as 100ms, and can handle millions of events per second. It provides a unified API for working with real-time data, making it possible to process both batch and real-time data in the same environment.\n\nSpark Streaming uses a fault-tolerant, scale-out architecture that supports high data processing performance. It also provides a wide range of built-in algorithms and libraries for machine learning, graph processing, and data analysis.\n\nSpark Streaming integrates with a variety of data sources and sinks, including Apache Kafka, Kinesis, Flume, and Cassandra, as well as Hadoop Distributed File System (HDFS). It also provides support for custom data sources and sinks, making it easy to integrate with new data sources as needed.\n\n\n\n## Interactive processing\n\nInteractive processing is a data processing technique that involves querying and analyzing data in real-time to support interactive decision-making. It is a contrast to batch processing, which processes data in large volumes after some delay. In interactive processing, data is queried and analyzed in real-time, often through OLAP (Online Analytical Processing) tools that enable users to slice and dice data in multiple dimensions.\n\nOLAP is a data processing technique that enables users to interactively analyze large datasets from multiple perspectives. OLAP tools provide a multidimensional view of data, allowing users to view data in different ways such as by time, geography, and product categories. OLAP tools allow users to drill down and aggregate data from the most granular level to the highest level, enabling users to gain insights into their data quickly.\n\nPresto is a distributed SQL query engine designed for interactive queries on large datasets. It was created by Facebook to support their massive data processing needs, including interactive analysis of petabytes of data. Presto can query data in multiple data sources, including Hadoop, Amazon S3, and traditional relational databases. It supports a wide range of SQL queries, including complex queries with multiple joins, subqueries, and aggregations.\n\nPresto is designed to work on a cluster of machines, making it easy to scale up and down as needed. It uses a cost-based optimizer to generate the best execution plan for each query, ensuring that queries are executed as efficiently as possible. Presto also supports caching of query results, which can significantly speed up subsequent queries that use the same data.\n\nInteractive processing, OLAP, and Presto are all critical components of modern data processing and analysis. They enable users to analyze large volumes of data quickly and easily, gaining insights into their data that were previously impossible. With the growing availability of massive datasets and the increasing need for real-time analysis, these technologies are becoming more important than ever.\n\n\n\n## Distributed computing frameworks\n\nDistributed computing frameworks are software frameworks used to build and run large-scale distributed applications, which are designed to process and analyze vast amounts of data across a distributed network of computers. Distributed computing frameworks have become essential for large-scale data processing, machine learning, and artificial intelligence applications that require high levels of parallelism, fault tolerance, and scalability.\n\nThe primary goal of distributed computing frameworks is to provide a transparent and consistent view of a distributed system by abstracting the complexity of distributed computing and providing a high-level programming model to developers. They provide a layer of abstraction to shield the developer from the underlying complexities of the distributed computing environment, such as data partitioning, replication, fault tolerance, and inter-node communication.\n\nSome popular examples of distributed computing frameworks include Apache Hadoop, Apache Spark, and Apache Flink. These frameworks provide a rich set of APIs for distributed data processing, including MapReduce, stream processing, batch processing, and interactive querying.\n\nDistributed computing frameworks are designed to work with commodity hardware, which means that they can scale horizontally by adding more machines to the cluster, making them highly cost-effective. This allows companies to process and analyze vast amounts of data without the need for expensive, proprietary hardware.\n\nIn summary, distributed computing frameworks are essential for large-scale data processing, machine learning, and artificial intelligence applications that require high levels of parallelism, fault tolerance, and scalability. They provide a layer of abstraction to shield developers from the complexities of distributed computing and offer a cost-effective way to process and analyze vast amounts of data.\n\n\n\n","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/Modern-Data-Stack-by-ChatGPT/What-is-a-modern-data-stack":{"title":"","content":"","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/data-system":{"title":"Data System","content":"\n\n\n# Data System\n\n## Data Exploring \u0026 Visualization\n\n* [Hands on Zeppelin](notes/hands-on-zeppelin.md)\n* [[hands-on-zeppelin]]","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/devops":{"title":"DevOps","content":"\n# DevOps\n\n## Docker\n\n* [Docker入门：step by step](notes/hands-on-docker_1.md)\n* [Docker进阶：step by step](notes/hands-on-docker_2.md)","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/hands-on-docker_1":{"title":"Docker入门","content":"\n\n\n# Docker入门：step by step\n\n\u003e 为上手学习Docker，我参照阮一峰老师写的教程，step by step地动手实践。此文记录。\n\n## 参考\n\n[阮一峰：Docker入门教程](https://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html)\n\n进一步实践，请移步：\n\n* [Docker进阶：step by step](notes/hands-on-docker_2.md)\n\n## 一、环境配置的难题\n\n软件开发最大的麻烦之一就是环境配置。每个人的机器的系统配置都不一样，包括操作系统、各种库与组件、环境变量等等。软件+运行环境，整体发布是解决这个问题的思路。Dockek是这种思路的实现。\n\n## 二、虚拟机\n\n在Docker之前，虚拟机是一种解决方案。但其缺点明显，包括：\n\n- **资源占用多**。虚拟机会独占一部分内存和硬盘空间。它运行的时候，其他程序就不能使用这些资源了。哪怕虚拟机里面的应用程序，真正使用的内存只有 1MB，虚拟机依然需要几百 MB 的内存才能运行。\n\n- **冗余步骤多**。虚拟机是完整的操作系统，一些系统级别的操作步骤，往往无法跳过，比如用户登录。\n\n- **启动慢。**启动操作系统需要多久，启动虚拟机就需要多久。可能要等几分钟，应用程序才能真正运行。\n\n## 三、Linux容器\n\nLinux容器（Linux Container， LXC）是另一种虚拟化技术。\n\n**Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。**或者说，在正常进程的外面套了一个[保护层](https://opensource.com/article/18/1/history-low-level-container-runtimes)。对于容器里面的进程来说，它接触到的各种资源都是虚拟的，从而实现与底层系统的隔离。\n\n由于容器是进程级别的，相比虚拟机有很多优势。\n\n- **启动快。**容器里面的应用，直接就是底层系统的一个进程，而不是虚拟机内部的进程。所以，启动容器相当于启动本机的一个进程，而不是启动一个操作系统，速度就快很多。\n\n- **资源占用少。**容器只占用需要的资源，不占用那些没有用到的资源；虚拟机由于是完整的操作系统，不可避免要占用所有资源。另外，多个容器可以共享资源，虚拟机都是独享资源。\n\n- **体积小。**容器只要包含用到的组件即可，而虚拟机是整个操作系统的打包，所以容器文件比虚拟机文件要小很多。\n\n总之，容器有点像轻量级的虚拟机，能够提供虚拟化的环境，但是成本开销小得多。\n\n## 四、Docker是什么？\n\n**Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。**它是目前最流行的 Linux 容器解决方案。\n\nDocker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。\n\n总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。\n\n## 五、Docker的用途\n\nDocker 的主要用途，目前有三大类。\n\n- **提供一次性的环境。**比如，本地测试他人的软件、持续集成的时候提供单元测试和构建的环境。\n\n- **提供弹性的云服务。**因为 Docker 容器可以随开随关，很适合动态扩容和缩容。\n\n- **组建微服务架构。**通过多个容器，一台机器可以跑多个服务，因此在本机就可以模拟出微服务架构。\n\n## 六、Docker 的安装\n\nDocker 是一个开源的商业产品，有两个版本：社区版（Community Edition，缩写为 CE）和企业版（Enterprise Edition，缩写为 EE）。企业版包含了一些收费服务，个人开发者一般用不到。下面的介绍都针对社区版。\n\nDocker CE 的安装请参考官方文档。\n\n\u003e - [Mac](https://docs.docker.com/docker-for-mac/install/)\n\u003e\n\u003e - [Windows](https://docs.docker.com/docker-for-windows/install/)\n\u003e\n\u003e - [Ubuntu](https://docs.docker.com/install/linux/docker-ce/ubuntu/)\n\u003e\n\u003e - 。。。\n\n安装完成后，运行下面的命令，验证是否安装成功。\n\n```Bash\n$ docker version\n# 或者\n$ docker info\n```\n\nDocker 需要用户具有 sudo 权限，为了避免每次命令都输入`sudo`，可以把用户加入 Docker 用户组（[官方文档](https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user)）。\n\n```Bash\nsudo usermod -aG docker $USER\n# 需要log out 才能生效，不让docker info命令的Sever部分会报错\n# 详情见：https://docs.docker.com/engine/install/linux-postinstall/\n```\n\nDocker 是服务器----客户端架构。命令行运行`docker`命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动（[官方文档](https://docs.docker.com/config/daemon/systemd/)）。\n\n```Bash\n# service 命令的用法\n$ sudo service docker start\n\n# systemctl 命令的用法\n$ sudo systemctl start docker\n```\n\n安装成功后，可以\n\n```bash\ndocker run hello-world\n```\n\n![image-20221214173436747](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214173436747.png)\n\n```\ndocker run -it ubuntu bash\n```\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/dcc8d661-5b86-4c2b-9cc3-e50e6a34719e.jpeg\" alt=\"dcc8d661-5b86-4c2b-9cc3-e50e6a34719e\" style=\"zoom:50%;\" /\u003e\n\n其中下载的image大小是30MB\n\n\u003e $ docker image ls\n\u003e\n\u003e REPOSITORY    TAG       IMAGE ID       CREATED         SIZE\n\u003e\n\u003e ubuntu        latest    df5de72bdb3b   2 weeks ago     77.8MB\n\u003e\n\u003e hello-world   latest    feb5d9fea6a5   11 months ago   13.3kB\n\n## 七、image 文件\n\n**Docker 把应用程序及其依赖，打包在 image 文件里面。**只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。\n\nimage 是二进制文件。实际开发中，一个 image 文件往往通过继承另一个 image 文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的 image。\n\nimage 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。\n\n为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 [Docker Hub](https://hub.docker.com/) 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。\n\n## 八、实例：hello world\n\n下面，我们通过最简单的 image 文件\"[hello world\"](https://hub.docker.com/r/library/hello-world/)，感受一下 Docker。\n\n需要说明的是，国内连接 Docker 的官方仓库很慢，还会断线，需要将默认仓库改成国内的镜像网站，具体的修改方法在[下一篇文章](https://www.ruanyifeng.com/blog/2018/02/docker-wordpress-tutorial.html)的第一节。有需要的朋友，可以先看一下。\n\n首先，运行下面的命令，将 image 文件从仓库抓取到本地。\n\n```Bash\n$ docker image pull library/hello-world\n```\n\n上面代码中，`docker image pull`是抓取 image 文件的命令。`library/hello-world`是 image 文件在仓库里面的位置，其中`library`是 image 文件所在的组，`hello-world`是 image 文件的名字。\n\n由于 Docker 官方提供的 image 文件，都放在`library`组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。\n\n```Bash\n$ docker image pull hello-world\n```\n\n抓取成功以后，就可以在本机看到这个 image 文件了。\n\n```Bash\n$ docker image ls\n```\n\n现在，运行这个 image 文件。\n\n```Bash\n$ docker container run hello-world\n```\n\n`docker container run`命令会从 image 文件，生成一个正在运行的容器实例。\n\n注意，`docker container run`命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的`docker image pull`命令并不是必需的步骤。\n\n如果运行成功，你会在屏幕上读到下面的输出。\n\n```Bash\n$ docker container run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.... ...\n```\n\n输出这段提示以后，`hello world`就会停止运行，容器自动终止。\n\n有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。\n\n```Bash\n$ docker container run -it ubuntu bash\n```\n\n对于那些不会自动终止的容器，必须使用`docker container kill` 命令手动终止。\n\n```Bash\n$ docker container kill [containID]\n\n## 如下例子\n$ docker container ls\nCONTAINER ID   IMAGE     COMMAND   CREATED          STATUS          PORTS     NAMES\nf45ef5fcc683   ubuntu    \"bash\"    21 minutes ago   Up 21 minutes             charming_montalcini\n\n# happy3 @ happy3-HX in ~ [15:46:39] \n$ docker kill f45ef5fcc683\nf45ef5fcc683\n\n# happy3 @ happy3-HX in ~ [15:46:47] \n$ docker container ls     \nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n# happy3 @ happy3-HX in ~ [15:46:54] \n$ \n```\n\n## 九、容器文件\n\n**image 文件生成的容器实例，本身也是一个文件，称为容器文件。**也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。\n\n上面命令的输出结果之中，包括容器的 ID。很多地方都需要提供这个 ID，比如上一节终止容器运行的`docker container kill`命令。\n\n终止运行的容器文件，依然会占据硬盘空间，可以使用`docker container rm`命令删除。\n\n```Bash\n$ docker container rm [containerID]\n```\n\n运行上面的命令之后，再使用`docker container ls --all`命令，就会发现被删除的容器文件已经消失了。\n\n## 十、Dockerfile 文件\n\n学会使用 image 文件以后，接下来的问题就是，如何可以生成 image 文件？如果你要推广自己的软件，势必要自己制作 image 文件。\n\n这就需要用到 Dockerfile 文件。它是一个文本文件，用来配置 image。Docker 根据 该文件生成二进制的 image 文件。\n\n下面通过一个实例，演示如何编写 Dockerfile 文件。\n\n## 十一、实例：制作自己的 Docker 容器\n\n下面我以 [koa-demos](https://www.ruanyifeng.com/blog/2017/08/koa.html) 项目为例，介绍怎么写 Dockerfile 文件，实现让用户在 Docker 容器里面运行 Koa 框架。\n\n作为准备工作，请先[下载源码](https://github.com/ruanyf/koa-demos/archive/master.zip)。\n\n```Bash\n$ git clone https://github.com/ruanyf/koa-demos.git\n$ cd koa-demos\n```\n\n#### 11.1 编写 Dockerfile 文件\n\n首先，在项目的根目录下，新建一个文本文件`.dockerignore`，写入下面的[内i容](https://github.com/ruanyf/koa-demos/blob/master/.dockerignore)。\n\n```Bash\n.git\nnode_modules\nnpm-debug.log\n```\n\n上面代码表示，这三个路径要排除，不要打包进入 image 文件。如果你没有路径要排除，这个文件可以不新建。\n\n然后，在项目的根目录下，新建一个文本文件 Dockerfile，写入下面的[内容](https://github.com/ruanyf/koa-demos/blob/master/Dockerfile)。\n\n```Bash\nFROM node:8.4\nCOPY . /app\nWORKDIR /app\nRUN npm install --registry=https://registry.npm.taobao.org\nEXPOSE 3000\n```\n\n上面代码一共五行，含义如下。\n\n\u003e - `FROM node:8.4`：该 image 文件继承官方的 node image，冒号表示标签，这里标签是`8.4`，即8.4版本的 node。\n\u003e\n\u003e - `COPY . /app`：将当前目录下的所有文件（除了`.dockerignore`排除的路径），都拷贝进入 image 文件的`/app`目录。\n\u003e\n\u003e - `WORKDIR /app`：指定接下来的工作路径为`/app`。\n\u003e\n\u003e - `RUN npm install`：在`/app`目录下，运行`npm install`命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。\n\u003e\n\u003e - `EXPOSE 3000`：将容器 3000 端口暴露出来， 允许外部连接这个端口。\n\n#### 11.2 创建 image 文件\n\n有了 Dockerfile 文件以后，就可以使用`docker image build`命令创建 image 文件了。\n\n```Bash\n$ docker image build -t koa-demo .\n# 或者\n$ docker image build -t koa-demo:0.0.1 .\n```\n\n上面代码中，`-t`参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是`latest`。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。\n\n如果运行成功，就可以看到新生成的 image 文件`koa-demo`了。\n\n```Bash\n$ docker image ls\n```\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/2c593f99-bc20-45bf-b5df-01300fa5ef4d.jpeg\" alt=\"2c593f99-bc20-45bf-b5df-01300fa5ef4d\" style=\"zoom:50%;\" /\u003e\n\n#### 11.3 生成容器\n\n`docker container run`命令会从 image 文件生成容器。\n\n```Bash\n$ docker container run -p 8000:3000 -it koa-demo /bin/bash\n# 或者\n$ docker container run -p 8000:3000 -it koa-demo:0.0.1 /bin/bash\n```\n\n上面命令的各个参数含义如下：\n\n\u003e - `-p`参数：容器的 3000 端口映射到本机的 8000 端口。\n\u003e\n\u003e - `-it`参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。\n\u003e\n\u003e - `koa-demo:0.0.1`：image 文件的名字（如果有标签，还需要提供标签，默认是 latest 标签）。\n\u003e\n\u003e - `/bin/bash`：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。\n\n如果一切正常，运行上面的命令以后，就会返回一个命令行提示符。\n\n```Bash\nroot@66d80f4aaf1e:/app#\n```\n\n这表示你已经在容器里面了，返回的提示符就是容器内部的 Shell 提示符。执行下面的命令。\n\n```Bash\nroot@66d80f4aaf1e:/app# node demos/01.js\n```\n\n这时，Koa 框架已经运行起来了。打开本机的浏览器，访问 http://127.0.0.1:8000，网页显示\"Not Found\"，这是因为这个 [demo](https://github.com/ruanyf/koa-demos/blob/master/demos/01.js) 没有写路由。\n\n这个例子中，Node 进程运行在 Docker 容器的虚拟环境里面，进程接触到的文件系统和网络接口都是虚拟的，与本机的文件系统和网络接口是隔离的，因此需要定义容器与物理机的端口映射（map）。\n\n现在，在容器的命令行，按下 Ctrl + c 停止 Node 进程，然后按下 Ctrl + d （或者输入 exit）退出容器。此外，也可以用`docker container kill`终止容器运行。\n\n```Bash\n# 在本机的另一个终端窗口，查出容器的 ID\n$ docker container ls\n\n# 停止指定的容器运行\n$ docker container kill [containerID]\n```\n\n容器停止运行之后，并不会消失，用下面的命令删除容器文件。\n\n```Bash\n# 查出容器的 ID\n$ docker container ls --all\n\n# 删除指定的容器文件\n$ docker container rm [containerID]\n```\n\n也可以使用`docker container run`命令的`--rm`参数，在容器终止运行后自动删除容器文件。\n\n```Bash\n$ docker container run --rm -p 8000:3000 -it koa-demo /bin/bash\n```\n\n#### 11.4 CMD 命令\n\n上一节的例子里面，容器启动以后，需要手动输入命令`node demos/01.js`。我们可以把这个命令写在 Dockerfile 里面，这样容器启动以后，这个命令就已经执行了，不用再手动输入了。\n\n```Bash\nFROM node:8.4\nCOPY . /app\nWORKDIR /app\nRUN npm install --registry=https://registry.npm.taobao.org\nEXPOSE 3000\nCMD node demos/01.js\n```\n\n上面的 Dockerfile 里面，多了最后一行`CMD node demos/01.js`，它表示容器启动后自动执行`node demos/01.js`。\n\n你可能会问，`RUN`命令与`CMD`命令的区别在哪里？简单说，`RUN`命令在 image 文件的构建阶段执行，执行结果都会打包进入 image 文件；`CMD`命令则是在容器启动后执行。另外，一个 Dockerfile 可以包含多个`RUN`命令，但是只能有一个`CMD`命令。\n\n注意，指定了`CMD`命令以后，`docker container run`命令就不能附加命令了（比如前面的`/bin/bash`），否则它会覆盖`CMD`命令。现在，启动容器可以使用下面的命令。\n\n```Bash\n$ docker container run --rm -p 8000:3000 -it koa-demo:0.0.1\n```\n\n#### 11.5 发布 image 文件\n\n容器运行成功后，就确认了 image 文件的有效性。这时，我们就可以考虑把 image 文件分享到网上，让其他人使用。\n\n首先，去 [hub.docker.com](https://hub.docker.com/) 或 [cloud.docker.com](https://cloud.docker.com/) 注册一个账户。然后，用下面的命令登录。\n\n```Bash\n$ docker login\n```\n\n接着，为本地的 image 标注用户名和版本。\n\n```Bash\n$ docker image tag [imageName] [username]/[repository]:[tag]\n# 实例\n$ docker image tag koa-demos:0.0.1 ruanyf/koa-demos:0.0.1\n```\n\n也可以不标注用户名，重新构建一下 image 文件。\n\n```Bash\n$ docker image build -t [username]/[repository]:[tag] .\n```\n\n最后，发布 image 文件。\n\n```Bash\n$ docker image push [username]/[repository]:[tag]\n```\n\n发布成功以后，登录 hub.docker.com，就可以看到已经发布的 image 文件。\n\n### 十二、其他有用的命令\n\ndocker 的主要用法就是上面这些，此外还有几个命令，也非常有用。\n\n**（1）docker container start**\n\n前面的`docker container run`命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用`docker container start`命令，它用来启动已经生成、已经停止运行的容器文件。\n\n```Bash\n$ docker container start [containerID]\n```\n\n**（2）docker container stop**\n\n前面的`docker container kill`命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而`docker container stop`命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号。\n\n```Bash\n$ docker container stop [containerID]\n```\n\n这两个信号的差别是，应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。\n\n**（3）docker container logs**\n\n`docker container logs`命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。如果`docker run`命令运行容器的时候，没有使用`-it`参数，就要用这个命令查看输出。\n\n```Bash\n$ docker container logs [containerID]\n```\n\n**（4）docker container exec**\n\n`docker container exec`命令用于进入一个正在运行的 docker 容器。如果`docker run`命令运行容器的时候，没有使用`-it`参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。\n\n```Bash\n$ docker container exec -it [containerID] /bin/bash\n```\n\n**（5）docker container cp**\n\n`docker container cp`命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。\n\n```Bash\n$ docker container cp [containID]:[/path/to/file] .\n```\n\n\n\nDone!\n\n","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/hands-on-docker_2":{"title":"Docker进阶","content":"\n# Docker进阶：step by step\n\n## 0. 参考\n\n[Docker 微服务教程](https://www.ruanyifeng.com/blog/2018/02/docker-wordpress-tutorial.html)\n\n前置教程：\n\n* [Docker入门：step by step](notes/hands-on-docker_1.md)\n\n### 一、预备工作：image 仓库的镜像网址\n\n本教程需要从仓库下载 image 文件，但是国内访问 Docker 的官方仓库很慢，还经常断线，所以要把仓库网址改成国内的镜像站。这里推荐使用官方镜像 registry.docker-cn.com 。下面是我的 Debian 系统的默认仓库修改方法，其他系统的修改方法参考[官方文档](https://www.docker-cn.com/registry-mirror)。\n\n打开`/etc/default/docker`文件（需要`sudo`权限），在文件的底部加上一行。\n\n```Bash\nDOCKER_OPTS=\"--registry-mirror=https://registry.docker-cn.com\"\n```\n\n然后，重启 Docker 服务。\n\n```Bash\n$ sudo service docker restart\n```\n\n现在就会自动从镜像仓库下载 image 文件了。\n\n### 二、方法 A：自建 WordPress 容器\n\n前面说过，本文会用三种方法演示 WordPress 的安装。第一种方法就是自建 WordPress 容器。\n\n#### 2.1 官方 的 PHP image\n\n首先，新建一个工作目录，并进入该目录。\n\n```Bash\n$ mkdir docker-demo \u0026\u0026 cd docker-demo\n```\n\n然后，执行下面的命令。\n\n```Bash\ndocker container run \\\n  --rm \\\n  --name wordpress \\\n  --volume \"$PWD/\":/var/www/html \\\n  php:5.6-apache\n```\n\n上面的命令基于`php`的 image 文件新建一个容器，并且运行该容器。`php`的标签是`5.6-apache`，说明装的是 PHP 5.6，并且自带 Apache 服务器。该命令的三个参数含义如下。\n\n```Bash\n--rm：停止运行后，自动删除容器文件。\n--name wordpress：容器的名字叫做wordpress。\n--volume \"$PWD/\":/var/www/html：将当前目录（$PWD）映射到容器的/var/www/html（Apache 对外访问的默认目录）。因此，当前目录的任何修改，都会反映到容器里面，进而被外部访问到。\n```\n\n运行上面的命令以后，如果一切正常，命令行会提示容器对外的 IP 地址，请记下这个地址，我们要用它来访问容器。我分配到的 IP 地址是 172.17.0.2。\n\n打开浏览器，访问 172.17.0.2，你会看到下面的提示。\n\n```Bash\nForbidden\nYou don't have permission to access / on this server.\n```\n\n是因为容器的`/var/www/html`目录（也就是本机的`docker-demo`目录）下面什么也没有，无法提供可以访问的内容。\n\n请在本机的`docker-demo`目录下面，添加一个最简单的 PHP 文件`index.php`。\n\n```Bash\n\u003c?php \nphpinfo();\n?\u003e\n```\n\n保存以后，浏览器刷新`172.17.0.2`，应该就会看到熟悉的`phpinfo`页面了。\n\n\n\n![135ee4ef-bf47-4239-935a-2eab8f896575](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/135ee4ef-bf47-4239-935a-2eab8f896575.jpeg)\n\n2.2 拷贝 WordPress 安装包\n\n既然本地的`docker-demo`目录可以映射到容器里面，那么把 WordPress 安装包拷贝到`docker-demo`目录下，不就可以通过容器访问到 WordPress 的安装界面了吗？\n\n首先，在`docker-demo`目录下，执行下面的命令，抓取并解压 WordPress 安装包。\n\n```Bash\nwget https://cn.wordpress.org/wordpress-4.9.4-zh_CN.tar.gz\ntar -xvf wordpress-4.9.4-zh_CN.tar.gz\n```\n\n解压以后，WordPress 的安装文件会在`docker-demo/wordpress`目录下。\n\n这时浏览器访问`http://172.17.0.2/wordpress`，就能看到 WordPress 的安装提示了。\n\n\n\n![555d7606-f3b7-45d4-b440-a3e78ac456a5](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/555d7606-f3b7-45d4-b440-a3e78ac456a5.jpeg)\n\n#### 2.3 官方的 MySQL 容器\n\nWordPress 必须有数据库才能安装，所以必须新建 MySQL 容器。\n\n打开一个新的命令行窗口，执行下面的命令。\n\n```Bash\ndocker container run \\\n  -d \\\n  --rm \\\n  --name wordpressdb \\\n  --env MYSQL_ROOT_PASSWORD=123456 \\\n  --env MYSQL_DATABASE=wordpress \\\n  mysql:5.7\n```\n\n上面的命令会基于 MySQL 的 image 文件（5.7版本）新建一个容器。该命令的五个命令行参数的含义如下。\n\n\u003e - `-d`：容器启动后，在后台运行。\n\u003e\n\u003e - `--rm`：容器终止运行后，自动删除容器文件。\n\u003e\n\u003e - `--name wordpressdb`：容器的名字叫做`wordpressdb`\n\u003e\n\u003e - `--env MYSQL_ROOT_PASSWORD=123456`：向容器进程传入一个环境变量`MYSQL_ROOT_PASSWORD`，该变量会被用作 MySQL 的根密码。\n\u003e\n\u003e - `--env MYSQL_DATABASE=wordpress`：向容器进程传入一个环境变量`MYSQL_DATABASE`，容器里面的 MySQL 会根据该变量创建一个同名数据库（本例是`WordPress`）。\n\n运行上面的命令以后，正常情况下，命令行会显示一行字符串，这是容器的 ID，表示已经新建成功了。\n\n这时，使用下面的命令查看正在运行的容器，你应该看到`wordpress`和`wordpressdb`两个容器正在运行。\n\n```Bash\ndocker container ls\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                 NAMES\nf01fe6ff7a31   mysql:5.7        \"docker-entrypoint.s…\"   15 seconds ago   Up 13 seconds   3306/tcp, 33060/tcp   wordpressdb\nb62e97351f5f   php:5.6-apache   \"docker-php-entrypoi…\"   32 seconds ago   Up 30 seconds   80/tcp                wordpress\n```\n\n其中，`wordpressdb`是后台运行的，前台看不见它的输出，必须使用下面的命令查看。\n\n```Bash\ndocker container logs wordpressdb\n```\n\n#### 2.4 定制 PHP 容器\n\n现在 WordPress 容器和 MySQL 容器都已经有了。接下来，要把 WordPress 容器连接到 MySQL 容器了。但是，PHP 的官方 image 不带有`mysql`扩展，必须自己新建 image 文件。\n\n首先，停掉 WordPress 容器。\n\n```Bash\ndocker container stop wordpress\n\n## 停掉之后\ndocker container ls              \nCONTAINER ID   IMAGE       COMMAND                  CREATED         STATUS         PORTS                 NAMES\nf01fe6ff7a31   mysql:5.7   \"docker-entrypoint.s…\"   3 minutes ago   Up 3 minutes   3306/tcp, 33060/tcp   wordpressdb\n```\n\n停掉以后，由于`--rm`参数的作用，该容器文件会被自动删除。\n\n然后，在`docker-demo`目录里面，新建一个`Dockerfile`文件，写入下面的内容。\n\n```Bash\nFROM php:5.6-apache\nRUN docker-php-ext-install mysqli\nCMD apache2-foreground\n```\n\n上面代码的意思，就是在原来 PHP 的 image 基础上，安装`mysqli`的扩展。然后，启动 Apache。\n\n基于这个 Dockerfile 文件，新建一个名为`phpwithmysql`的 image 文件。\n\n```Bash\ndocker build -t phpwithmysql .\n```\n\n#### 2.5 Wordpress 容器连接 MySQL\n\n现在基于 phpwithmysql image，重新新建一个 WordPress 容器。\n\n```Bash\ndocker container run \\\n  --rm \\\n  --name wordpress \\\n  --volume \"$PWD/\":/var/www/html \\\n  --link wordpressdb:mysql \\\n  phpwithmysql\n```\n\n跟上一次相比，上面的命令多了一个参数`--link wordpressdb:mysql`，表示 WordPress 容器要连到`wordpressdb`容器，冒号表示该容器的别名是`mysql`。\n\n这时还要改一下`wordpress`目录的权限，让容器可以将配置信息写入这个目录（容器内部写入的`/var/www/html`目录，会映射到这个目录）。\n\n```Bash\nchmod -R 777 wordpress\n```\n\n接着，回到浏览器的`http://172.17.0.2/wordpress`页面，点击\"现在就开始！\"按钮，开始安装。\n\nWordPress 提示要输入数据库参数。输入的参数如下。\n\n![7a2e3f14-a418-40ef-b5f8-8036fa3035b9](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/7a2e3f14-a418-40ef-b5f8-8036fa3035b9.jpeg)\n\n\u003e - 数据库名：`wordpress`\n\u003e\n\u003e - 用户名：`root`\n\u003e\n\u003e - 密码：`123456`\n\u003e\n\u003e - 数据库主机：`mysql`\n\u003e\n\u003e - 表前缀：`wp_`（不变）\n\n点击\"下一步\"按钮，如果 Wordpress 连接数据库成功，就会出现下面的页面，这就表示可以安装了。\n\n![IdmeQaS9iG](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/IdmeQaS9iG.jpg)\n\n至此，自建 WordPress 容器的演示完毕，可以把正在运行的两个容器关闭了（容器文件会自动删除）。\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/4MkHfisE5P.jpg\" alt=\"4MkHfisE5P\" style=\"zoom:33%;\" /\u003e\n\n安装之后\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/PXh6tI53nv.jpg\" alt=\"PXh6tI53nv\" style=\"zoom:50%;\" /\u003e\n\n```Bash\ndocker container stop wordpress wordpressdb\n\n## 之后\ndocker container ls\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n```\n\n### 三、方法 B：Wordpress 官方镜像\n\n上一部分的自建 WordPress 容器，还是挺麻烦的。其实不用这么麻烦，Docker 已经提供了官方 [WordPress](https://hub.docker.com/_/wordpress/) image，直接用那个就可以了。有了上一部分的基础，下面的操作就很容易理解了。\n\n \n\n#### 3.1 基本用法\n\n首先，新建并启动 MySQL 容器。\n\n```Bash\ndocker container run \\\n  -d \\\n  --rm \\\n  --name wordpressdb \\\n  --env MYSQL_ROOT_PASSWORD=123456 \\\n  --env MYSQL_DATABASE=wordpress \\\n  mysql:5.7\n```\n\n然后，基于官方的 WordPress image，新建并启动 WordPress 容器。\n\n```Bash\ndocker container run \\\n  -d \\\n  --rm \\\n  --name wordpress \\\n  --env WORDPRESS_DB_PASSWORD=123456 \\\n  --env WORDPRESS_DB_USER=root \\\n  --link wordpressdb:mysql \\\n  wordpress\n```\n\n上面命令中，各个参数的含义前面都解释过了，其中环境变量`WORDPRESS_DB_PASSWORD`是 MySQL 容器的根密码。\n\n```Bash\ndocker container ls   \nCONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                 NAMES\nad6ee2dd4971   wordpress   \"docker-entrypoint.s…\"   48 seconds ago   Up 35 seconds   80/tcp                wordpress\n87f2d581abda   mysql:5.7   \"docker-entrypoint.s…\"   3 minutes ago    Up 3 minutes    3306/tcp, 33060/tcp   wordpressdb\n```\n\n上面命令指定`wordpress`容器在后台运行，导致前台看不见输出，使用下面的命令查出`wordpress`容器的 IP 地址。\n\n```Bash\ndocker container inspect wordpress\n[\n    {\n        \"Id\": \"ad6ee2dd497184a397c8bd58df7b415b5081bff5b0e000a10daa0b970b38cf9c\",\n        \"Created\": \"2022-08-26T02:16:36.222826772Z\",\n        \"Path\": \"docker-entrypoint.sh\",\n        \"Args\": [\n            \"apache2-foreground\"\n        ],\n   。。。\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.3\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n   。。。\n    }\n]\n```\n\n上面命令运行以后，会输出很多内容，找到`IPAddress`字段即可。我的机器返回的 IP 地址是`172.17.0.3`。\n\n浏览器访问`172.17.0.3`，就会看到 WordPress 的安装提示。\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/fOzoBcsIhF.jpg\" alt=\"fOzoBcsIhF\" style=\"zoom:33%;\" /\u003e\n\n#### 3.2 WordPress 容器的定制\n\n到了上一步，官方 WordPress 容器的安装就已经成功了。但是，这种方法有两个很不方便的地方。\n\n\u003e - 每次新建容器，返回的 IP 地址不能保证相同，导致要更换 IP 地址访问 WordPress。\n\u003e\n\u003e - WordPress 安装在容器里面，本地无法修改文件。\n\n解决这两个问题很容易，只要新建容器的时候，加两个命令行参数就可以了。\n\n先把刚才启动的 WordPress 容器终止（容器文件会自动删除）。\n\n```Bash\ndocker container stop wordpress\n```\n\n然后，使用下面的命令新建并启动 WordPress 容器。\n\n```Bash\ndocker container run \\\n  -d \\\n  -p 127.0.0.2:8080:80 \\\n  --rm \\\n  --name wordpress \\\n  --env WORDPRESS_DB_PASSWORD=123456 \\\n  --env WORDPRESS_DB_USER=root \\\n  --link wordpressdb:mysql \\\n  --volume \"$PWD/wordpress\":/var/www/html \\\n  wordpress\n```\n\n上面的命令跟前面相比，命令行参数只多出了两个。\n\n\u003e - `-p 127.0.0.2:8080:80`：将容器的 80 端口映射到`127.0.0.2`的`8080`端口。\n\u003e\n\u003e - `--volume \"$PWD/wordpress\":/var/www/html`：将容器的`/var/www/html`目录映射到当前目录的`wordpress`子目录。\n\n浏览器访问`127.0.0.2:8080`就能看到 WordPress 的安装提示了。而且，你在`wordpress`子目录下的每次修改，都会反映到容器里面。\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/VjVIgk6FeM.jpg\" alt=\"VjVIgk6FeM\" style=\"zoom:33%;\" /\u003e\n\n最后，终止这两个容器（容器文件会自动删除）。\n\n```Bash\ndocker container stop wordpress wordpressdb\n```\n\n### 四、方法 C：Docker Compose 工具\n\n上面的方法 B 已经挺简单了，但是必须自己分别启动两个容器，启动的时候，还要在命令行提供容器之间的连接信息。因此，Docker 提供了一种更简单的方法，来管理多个容器的联动。\n\n#### 4.1 Docker Compose 简介\n\n ![KulToe4cJl](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/KulToe4cJl.jpg)\n\n[Compose](https://docs.docker.com/compose/) 是 Docker 公司推出的一个工具软件，可以管理多个 Docker 容器组成一个应用。你需要定义一个 [YAML](https://www.ruanyifeng.com/blog/2016/07/yaml.html) 格式的配置文件`docker-compose.yml`，写好多个容器之间的调用关系。然后，只要一个命令，就能同时启动/关闭这些容器。\n\n```Bash\n# 启动所有服务\ndocker compose up\n# 关闭所有服务\ndocker compose stop\n```\n\n#### 4.2 Docker Compose 的安装\n\nMac 和 Windows 在安装 docker 的时候，会一起安装 docker compose。Linux 系统下的安装参考[官方文档](https://docs.docker.com/compose/install/#install-compose)。\n\n安装完成后，运行下面的命令。\n\n```Bash\ndocker compose version\nDocker Compose version v2.6.0\n```\n\n#### 4.3 WordPress 示例\n\n在`docker-demo`目录下，新建`docker-compose.yml`文件，写入下面的内容。\n\n```Bash\nversion: \"3\"\n\nservices:\n  mysql:\n    image: mysql:5.7\n    environment:\n     - MYSQL_ROOT_PASSWORD=123456\n     - MYSQL_DATABASE=wordpress\n  web:\n    image: wordpress\n    links:\n     - mysql\n    environment:\n     - WORDPRESS_DB_PASSWORD=123456\n     - WORDPRESS_DB_USER=root\n    ports:\n     - \"127.0.0.3:8080:80\"\n    working_dir: /var/www/html\n```\n\n上面代码中，两个顶层标签表示有两个容器`mysql`和`web`。每个容器的具体设置，前面都已经讲解过了，还是挺容易理解的。\n\n启动两个容器。 \n\n```Bash\ndocker compose up\n```\n\n浏览器访问 http://127.0.0.3:8080，应该就能看到 WordPress 的安装界面。\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/rbQBhnSKGe.jpg\" alt=\"rbQBhnSKGe\" style=\"zoom:50%;\" /\u003e\n\n现在关闭两个容器。\n\n```Bash\ndocker compose stop\n```\n\n关闭以后，这两个容器文件还是存在的，写在里面的数据不会丢失。下次启动的时候，还可以复用。下面的命令可以把这两个容器文件删除（容器必须已经停止运行）。\n\n```Bash\ndocker compose rm\n```\n\n### 五、参考链接\n\n- [How to Manually Build Docker Containers for WordPress](https://www.sitepoint.com/how-to-manually-build-docker-containers-for-wordpress/), by Aleksander Koko\n\n- [How to Use the Official Docker WordPress Image](https://www.sitepoint.com/how-to-use-the-official-docker-wordpress-image/), by Aleksander Koko\n\n- [Deploying WordPress with Docker](https://www.sitepoint.com/deploying-wordpress-with-docker/), by Aleksander Koko\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-flask":{"title":"Hands on Flask","content":"\n# Hands on Flask: step by step\n\n\u003e I am a newcomer of Flask. So I follow the official tutorial to build a blog post web application step by step.\n\n\n\n## Reference\n\n[Flask Installation](https://flask.palletsprojects.com/en/2.2.x/installation/)\n\n[Flask Tutorial](https://flask.palletsprojects.com/en/2.2.x/tutorial/)\n\nFollow the tutorial to build a blog web app like:\n\n\n\n![image-20221201145000837](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221201145000837.png)\n\n\n\n## Step by Step\n\n### Step 0.Backgroud\n\nbuild a basic blog application called Flaskr using Flask.\n\n\n\n### Step 1. Project Layout\n\n#### Create a project directory\n\n```shell\nmkdir flask-tutorial\ncd flask-tutorial/\n```\n\n\n\n#### Install Flask\n\n[Flask Installation Doc ](https://flask.palletsprojects.com/en/2.2.x/installation/)\n\nCreate an python environment using [venv](https://docs.python.org/3/library/venv.html#module-venv):\n\n```shell\n$ python3 -m venv venv\n\n$ tree . -L 3\n.\n└── venv\n    ├── bin\n    │   ├── Activate.ps1\n    │   ├── activate\n    │   ├── activate.csh\n    │   ├── activate.fish\n    │   ├── pip\n    │   ├── pip3\n    │   ├── pip3.9\n    │   ├── python -\u003e python3\n    │   ├── python3 -\u003e /Users/yangls06/opt/miniconda3/bin/python3\n    │   └── python3.9 -\u003e python3\n    ├── include\n    ├── lib\n    │   └── python3.9\n    └── pyvenv.cfg\n\n5 directories, 11 files\n\n$ tree venv/lib/python3.9/ -L 3\nvenv/lib/python3.9/\n└── site-packages\n    ├── _distutils_hack\n    │   ├── __init__.py\n    │   ├── __pycache__\n    │   └── override.py\n    ├── distutils-precedence.pth\n    ├── pip\n    │   ├── __init__.py\n    │   ├── __main__.py\n        ...\n    ├── pkg_resources\n    ...\n```\n\n\n\nActivate the environment\n\n```shell\n$ . venv/bin/activate\n```\n\nThen the environment has been changed.\n\n![image-20221201153333547](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221201153333547.png)\n\nInstall Flask\n\nWithin the activated environment, install Flask using `pip`:\n\n```shell\n$ pip install Flask\n\nLooking in indexes: https://pypi.douban.com/simple\nCollecting Flask\n  Downloading https://pypi.doubanio.com/packages/0f/43/15f4f9ab225b0b25352412e8daa3d0e3d135fcf5e127070c74c3632c8b4c/Flask-2.2.2-py3-none-any.whl (101 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 KB 1.8 MB/s eta 0:00:00\n...\nCollecting MarkupSafe\u003e=2.0\n  Downloading https://pypi.doubanio.com/packages/06/7f/d5e46d7464360b6ac39c5b0b604770dba937e3d7cab485d2f3298454717b/MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\nInstalling collected packages: zipp, MarkupSafe, itsdangerous, click, Werkzeug, Jinja2, importlib-metadata, Flask\nSuccessfully installed Flask-2.2.2 Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 click-8.1.3 importlib-metadata-5.1.0 itsdangerous-2.1.2 zipp-3.11.0\n```\n\n\n\nGit init\n\n```shell\n$ git init\n```\n\n\n\nwith `.gitignore`\n\n```\nvenv/\n\n*.pyc\n__pycache__/\n\ninstance/\n\n.pytest_cache/\n.coverage\nhtmlcov/\n\ndist/\nbuild/\n*.egg-info/\n```\n\n\n\nAdd folders\n\n```shell\n$ tree -a -L 1\n.\n├── .git\n├── .gitignore\n├── flaskr\n├── tests\n└── venv\n\n4 directories, 1 file\n```\n\n\n\n### Step 3. Application Setup\n\n#### The Application Factory: \\__init__.py\n\n* create `Flask` instance \n* make `flaskr` directory a package\n\n```python\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\n\"\"\"\n@Time    :   2022/12/01 15:58:24\n@Author  :   Linsan Yang \n@Desc    :   init flaskr\n\"\"\"\n\nimport os\nfrom flask import Flask\n\ndef create_app(test_config=None):\n    # create and configure the app\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_mapping(\n        SECRET_KEY = 'dev',\n        DATABASE=os.path.join(app.instance_path, 'flaskr.sqlite'),\n    )\n\n    if test_config is None:\n        # load the instance config, if it exists, when not testing\n        app.config.from_pyfile('config.py', silent=True)\n    else:\n        # load the test config if passed in\n        app.config.from_mapping(test_config)\n    \n    # ensure the instance folder exists\n    try:\n        os.makedirs(app.instance_path)\n    except OSError as e:\n        pass\n\n    # a simple page that says hello\n    @app.route('/hello')\n    def hello():\n        return 'Hello, World!'\n\n    return app\n\n```\n\n\n\n**instance folder** \n\nThere will be a `instance/`directory, located outside the `flaskr` package and can hold local data that shouldn’t be committed to version control, such as configuration secrets and the database file.\n\n\n\n**test_config**\n\nUsing test_config for testing.\n\n\n\n**@app.route()**\n\ncreate a simple route of `/hello`\n\n\n\n#### Run The Application\n\nIn the `flask-tutorial` dir not `flaskr` package:\n\n```shell\n$ flask --app flaskr --debug run\n * Serving Flask app 'flaskr'\n * Debug mode: on\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\n * Restarting with stat\n * Debugger is active!\n * Debugger PIN: 134-914-837\n```\n\nThen open 127.0.0.1:5000/hello in browser, got\n\n![image-20221201162507048](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221201162507048.png)\n\n### Step 4. Define and Access the Database\n\nThe app will use `Sqlite` database to store users and posts. Python has a built-in module `sqlite3` module.\n\n#### Connect to Sqlite\n\nflaskr/db.py\n\n```python\nimport sqlite3\n\nimport click\nfrom flask import current_app, g\n\ndef get_db():\n    if 'db' not in g:\n        g.db = sqlite3.connect(\n            current_app.config['DATABASE'],\n            detect_types=sqlite3.PARSE_DECLTYPES\n        )\n        g.db.row_factory = sqlite3.Row\n    return g.db\n\ndef close_db(e=None):\n    db = g.pop('db', None)\n\n    if db is not None:\n        db.close()\n```\n\n\n\n`g` is a spectial object for each request to share data among different functions. `current_app` is similar.\n\n\n\n#### Create Tables: using sql\n\nDefine `user` and `post` table in `flaskr/schema.sql`:\n\n```sql\nDROP TABLE IF EXISTS user;\nDROP TABLE IF EXISTS post;\n\nCREATE TABLE user (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  username TEXT UNIQUE NOT NULL,\n  password TEXT NOT NULL\n);\n\nCREATE TABLE post (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  author_id INTEGER NOT NULL,\n  created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  title TEXT NOT NULL,\n  body TEXT NOT NULL,\n  FOREIGN KEY (author_id) REFERENCES user (id)\n);\n```\n\n\n\nAdd functions to run the SQLs to the `db.py`\n\n```python\ndef init_db():\n    db = get_db()\n    \n    with current_app.open_resource('schema.sql') as f:\n        db.executescript(f.read().decode('utf8'))\n\n@click.command('init-db')\ndef init_db_command():\n    '''Clear the existing data and create new tables.'''\n    init_db()\n    click.echo('Initialized the database.')\n```\n\n\n\n#### Register with the Applicaiton\n\nThe close_db and init_db_command functions need to be registered with the app instance for use.\n\nIn `db.py` add a new `init_app` function:\n\n```python\ndef init_app(app):\n    # tells Flask to call that function when cleaning up after returning the response\n    app.teardown_appcontext(close_db)\n    # adds a new command that can be called with the flask command\n    app.cli.add_command(init_db_command)\n```\n\n\n\nThen import and call this function from the factory in `__init__.py`.\n\n```python\ndef create_app(test_config=None):\n    # create and configure the app\n    app = ...\n    # existing code omitted\n    \n    # add db functions\n    from . import db\n    db.init_app(app)\n\n    return app\n\n```\n\n\n\n#### Initialize the Database\n\nNow use `init-db`command like this:\n\n```shell\n$ flask --app flaskr init-db\nInitialized the database.\n\n$ tree instance/\ninstance/\n└── flaskr.sqlite\n\n0 directories, 1 file\n```\n\n\n\nThe command generates a sqlite db file `flaskr.sqlite` in `instance/` dir. \n\n\n\n### Step 5. Blueprints and Views\n\nReferances:\n\n[Blueprints and Views](https://flask.palletsprojects.com/en/2.2.x/tutorial/views/)\n\n[Use a Flask Blueprint to Architect Your Applications](https://realpython.com/flask-blueprint/#:~:text=Flask%20is%20a%20very%20popular,its%20functionality%20into%20reusable%20components)\n\n\n\nConcept: view\n\nA view is Flask's respond to the outgoing request. Flask uses patterns to match the incoming request URL to the view that should handle it.\n\n\n\nConcept: blueprint\n\nA blueprint is a way to organize a group of related views and other code. Rather than registering views and other code directly with an application, they are registered with a blueprint. Then the blueprint is registered with the application when it is available in the factory function.\n\n\n\n#### Create a Blueprint\n\nFlaskr will have two blueprints:\n\n* auth functions\n* blog posts functions\n\n\n\n`Flaskr/auth.py`\n\n```python\nimport functools\n\nfrom flask import (\n    Blueprint, flash, g, redirect, render_template, request, session, url_for\n)\nfrom werkzeug.security import check_password_hash, generate_password_hash\nfrom flaskr.db import get_db\n\nbp = Blueprint('auth', __name__, url_prefix='/auth')\n\n```\n\nA new `Blueprint` is created:\n\n* with `name`: 'auth'\n* with `import_name`: '\\__name__', helping the blueprint to know where it’s defined\n* with `url_prefix`: will be prepended (added at head)to all the URLs associated with this blueprint.\n\nThen register the blueprint to the app from the factory in the `__init__.py`\n\n```python\ndef create_app(test_config=None):\n    # create and configure the app\n    app = ...\n    # existing code omitted\n    \n\n    # add auth blueprint\n    from . import auth\n    app.register_blueprint(auth.bp)\n\n    return app\n```\n\n\n\n\u003e Referances:\n\u003e\n\u003e [Python functools](https://docs.python.org/3/library/functools.html)\n\n\n\n#### Register view\n\nWhen the user visits the `/auth/register` URL, the `register` view will return [HTML](https://developer.mozilla.org/docs/Web/HTML) with a form for them to fill out. When they submit the form, it will validate their input and either show the form again with an error message or create the new user and go to the login page.\n\n\n\nThe view code is as following in `flaskr/auth.py`\n\n```python\n@bp.route('/register', methods=('GET', 'POST'))\ndef register():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        db = get_db()\n        error = None\n\n        if not username:\n            error = 'Username is required.'\n        elif not password:\n            error = 'Password is required.'\n\n        if error is None:\n            try:\n                db.execute(\n                    'INSERT INTO user (username, password) VALUES (?, ?)',\n                    (username, generate_password_hash(password))\n                )\n                db.commit()\n            except db.IntegrityError:\n                error = f\"User {username} is already registered.\"\n            else:\n                return redirect(url_for('auth.login'))\n        \n        flash(error)\n    \n    return render_template('auth/register.html')\n```\n\n\n\nThe `register` view works as following:\n\n* @bp.route associates the URL `/register` with the `register` view.\n* If the user submited the register form, `request.method == 'POST'`, then validate the input `username` and `password` and store them into the database.\n* If storing the user info succeeds, then redirect to the `auth.login` page.\n* If the user is initially landing on the `register` page, or there was a validation error, the `register.html` will be shown.\n\n\n\n#### Login view\n\nThis view follows the same pattern as `register` view.\n\n```python\n @bp.route('/login', methods=('GET', 'POST'))\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        db = get_db()\n        error = None\n\n        user = db.execute(\n            'SELECT * FROM user WHERE username = ?', (username,)\n        ).fetchone()\n\n        if user is None:\n            error = 'Incorrect username.'\n        elif not check_password_hash(user['password'], password):\n            error = 'Incorrect password.'\n        \n        if error is None:\n            session.clear()\n            session['user_id'] = user['id']\n            return redirect(url_for('index'))\n        \n        flash(error)\n    \n    return render_template('auth/login.html')\n\n```\n\nTips:\n\n* The user info is queried and stored in `user` variable using `fetch_one` function.\n* Validate the `username` and `password` (by `check_password_hash`) inputs.\n* The `session` (a dict storing data across requests) refreshes if login succeeds. (The data is stored in a *cookie* that is sent to the browser, and the browser then sends it back with subsequent requests.)\n\n\n\nWe can get user info at the beginning of each request via `session`:\n\n```python\n@bp.before_app_request\ndef load_logged_in_user():\n    user_id = session.get('user_id')\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = get_db().execute(\n            'SELECT * FROM user WHERE id = ?', (user_id,)\n        ).fetchone()\n```\n\nTips:\n\n* [`bp.before_app_request()`](https://flask.palletsprojects.com/en/2.2.x/api/#flask.Blueprint.before_app_request) registers a function that runs before the view function, no matter what URL is requested. \n* `load_logged_in_user`  gets that user info from the database via `session` and stores it on [`g.user`](https://flask.palletsprojects.com/en/2.2.x/api/#flask.g), which lasts for the length of the request. \n\n\n\n#### Logout view\n\nThe Logout view removes the user id from the [`session`](https://flask.palletsprojects.com/en/2.2.x/api/#flask.session). \n\n```python\n@bp.route('/logout')\ndef logout():\n    session.clear()\n    return redirect(url_for('index'))\n```\n\n\n\n#### Require Auth in Other views\n\nCreating, editing and deleting blog posts requires the user to be logged in. Use a **decorator** to achieve this.\n\n```python\ndef login_required(view):\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for('auth.login'))\n        \n        return view(**kwargs)\n    \n    return wrapped_view \n```\n\nThis decorator wraps the view in this way: if a user is not logged in, then redirect to login page; if logged in, return the orginal view.\n\n\n\n### Step 6. Templates\n\nThough `auth.login` view has been created, a `TemplateNotFound` error will be raised when you visit http://127.0.0.1:5000/auth/login.\n\n\n\n![image-20221202154357379](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202154357379.png)\n\nThis is because the view calls `render_template()`, but no templates are created. \n\n\n\nTips:\n\n* The template files will be stored in the `templates` directory inside the `flaskr` package.\n* Templates are files that contain static data as well as placeholders for dynamic data. \n* A template is rendered with specific data to produce a final document. Flask uses the [Jinja](https://jinja.palletsprojects.com/templates/) template library to render templates.\n* Special delimiters are used to distinguish Jinja syntax from the static data in the template. \n  * Anything between `{{` and `}}` is an expression that will be output to the final document. \n  * `{%` and `%}` denotes a control flow statement like `if` and `for`\n\n\n\n#### The Base Layout\n\nEach page in the app has the same basic layout around a different body.Instead of writing the entire HTML structure in each template, each template will extend a base template and override specific sections.\n\n\n\nFile: `flaskr/templates/base.html`\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003ctitle\u003e{% block title %}{% endblock %} - Flaskr\u003c/title\u003e\n\u003clink rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"\u003e\n\u003cnav\u003e\n   \u003ch1\u003eFlaskr\u003c/h1\u003e\n   \u003cul\u003e\n    {% if g.user %}\n        \u003cli\u003e\u003cspan\u003e{{ g.user['username'] }}\u003c/span\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"{{ url_for('auth.logout') }}\"\u003eLog Out\u003c/a\u003e\u003c/li\u003e\n    {% else %}\n        \u003cli\u003e\u003ca href=\"{{ url_for('auth.register') }}\"\u003eRegister\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"{{ url_for('auth.login') }}\"\u003eLog \u003cInput:c\u003e\u003c/Input:c\u003e\u003c/a\u003e\u003c/li\u003e\n    {% endif %}\n   \u003c/ul\u003e \n\u003c/nav\u003e\n\n\u003csection class=\"content\"\u003e\n    \u003cheader\u003e\n        {% block header %}\n        {% endblock %}\n    \u003c/header\u003e\n    {% for message in get_flashed_messages() %}\n        \u003cdiv class=\"flash\"\u003e{{ message }}\u003c/div\u003e\n    {% endfor %}\n    \n    {% block content %}\n    {% endblock %}\n\u003c/section\u003e\n\n```\n\n\n\n\u003eNotes: Using `Jinja Snippets` and `HTML CSS Support` extensions in vscode is helpful to write html code of Jinja templates.\n\n\n\nThere are three blocks defined here that will be overridden in the other templates:\n\n* `{% block title %}` will change the title displayed in the browser’s tab and window title.\n* `{% block header %}` is similar to `title` but will change the title displayed on the page.\n* `{% block content %}` is where the content of each page goes, such as the login form or a blog post.\n\n\n\nThe base template is directly in the `templates` directory. To keep the others organized, the templates for a blueprint will be placed in a directory with the same name as the blueprint.\n\n#### Register template\n\nFile: `flaskr/templates/register.html`\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Register{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"post\"\u003e\n        \u003clabel for=\"username\"\u003eUsername\u003c/label\u003e\n        \u003cinput name=\"username\" id=\"username\" required\u003e        \n        \u003clabel for=\"password\"\u003ePassword\u003c/label\u003e\n        \u003cinput type=\"password\" name=\"password\" id=\"password\" required\u003e\n        \u003cinput type=\"submit\" value=\"Register\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n#### Log In template\n\nThis is identical to the register template except for the title and submit button.\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Log In{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"post\"\u003e\n        \u003clabel for=\"username\"\u003eUsername\u003c/label\u003e\n        \u003cinput name=\"username\" id=\"username\" required\u003e        \n        \u003clabel for=\"password\"\u003ePassword\u003c/label\u003e\n        \u003cinput type=\"password\" name=\"password\" id=\"password\" required\u003e\n        \u003cinput type=\"submit\" value=\"Log In\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n\n#### Register a user\n\nVisit http://127.0.0.1:5000/auth/register\n\n![image-20221202174125671](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174125671.png)\n\nIf no password inputed, you will see:\n\n![image-20221202174349791](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174349791.png)\n\nIf register succeeds, it will redirect to login page:\n\n![image-20221202174548748](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174548748.png)\n\nIf incorrect password is inputed, you will get `Incorrect password` warning:\n\n![image-20221202174721957](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174721957.png)\n\n\n\n### Step 7. Static Files\n\nUse css file in the `flask/static` directory to give the webpages some style. In the `base.html` template there is already a link to `style.css` file.\n\n```html\n{{ url_for('static', filename='style.css') }}\n```\n\n\n\nFile: flask/static/style.css\n\n```css\nhtml { font-family: sans-serif; background: #eee; padding: 1rem; }\nbody { max-width: 960px; margin: 0 auto; background: white; }\nh1 { font-family: serif; color: #377ba8; margin: 1rem 0; }\na { color: #377ba8; }\nhr { border: none; border-top: 1px solid lightgray; }\nnav { background: lightgray; display: flex; align-items: center; padding: 0 0.5rem; }\nnav h1 { flex: auto; margin: 0; }\nnav h1 a { text-decoration: none; padding: 0.25rem 0.5rem; }\nnav ul  { display: flex; list-style: none; margin: 0; padding: 0; }\nnav ul li a, nav ul li span, header .action { display: block; padding: 0.5rem; }\n.content { padding: 0 1rem 1rem; }\n.content \u003e header { border-bottom: 1px solid lightgray; display: flex; align-items: flex-end; }\n.content \u003e header h1 { flex: auto; margin: 1rem 0 0.25rem 0; }\n.flash { margin: 1em 0; padding: 1em; background: #cae6f6; border: 1px solid #377ba8; }\n.post \u003e header { display: flex; align-items: flex-end; font-size: 0.85em; }\n.post \u003e header \u003e div:first-of-type { flex: auto; }\n.post \u003e header h1 { font-size: 1.5em; margin-bottom: 0; }\n.post .about { color: slategray; font-style: italic; }\n.post .body { white-space: pre-line; }\n.content:last-child { margin-bottom: 0; }\n.content form { margin: 1em 0; display: flex; flex-direction: column; }\n.content label { font-weight: bold; margin-bottom: 0.5em; }\n.content input, .content textarea { margin-bottom: 1em; }\n.content textarea { min-height: 12em; resize: vertical; }\ninput.danger { color: #cc2f2e; }\ninput[type=submit] { align-self: start; min-width: 10em; }\n```\n\n\n\nAfter that, reload the login page, and you can see:\n\n![image-20221202175806462](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202175806462.png)\n\n\n\n### Step 8. Blog Blueprint\n\nImplement the blog blueprint to allow a logged-in user to create posts and edit/delete the posts of his/her own.\n\n\u003e Note: As you implement each view, keep the development server running. As you save your changes, try going to the URL in your browser and testing them out.\n\n\n\n#### The Blog Blueprint\n\n\n\nDefine blog blueprint in file: `flask/blog.py`\n\n``` \nfrom flask import (\n    Blueprint, flash, g, redirect, render_template, request, session, url_for\n)\nfrom werkzeug.exceptions import abort\n\nfrom flaskr.db import get_db\nfrom flaskr.auth import login_required\n\nbp = Blueprint('auth', __name__)\n```\n\n\n\nRegister this blueprint in the app factory in file : `flask/__init__.py`\n\n```python\ndef create_app(test_config=None):\n    # create and configure the app\n    app = ...\n    # existing code omitted\n\n    # add blog blueprint\n    from . import blog\n    app.register_blueprint(blog.bp)\n    app.add_url_rule('/', endpoint='index')\n\n    return app\n```\n\n\n\nUnlike the auth blueprint, the blog blueprint does not have a `url_prefix`. So the index view will be at `/`, the create view at `/create`. The blog is the main feature of Flaskr app, so it makes sense that the blog index will be the main index.\n\n\n\nThe endpoint for index view in blog blueprint will be `blog.index`. The `app.add_url_rule('/', endpoint='index')` code associates the endpoint name 'index' with the `/` url so that `url_for('index')` or `url_for('blog.index')` will both work, generating the same `/` URL either way. \n\n\n\n#### Index: view and template\n\nThe index view shows all the posts of the logged-in user, order by created time desc. Use SQL's JOIN clause.\n\n\n\nDefine index view in file: `flaskr/blog.py`:\n\n```python\n@bp.route('/')\ndef index():\n    db = get_db()\n    posts = db.execute(\n        'SELECT p.id, title, body, created, author_id, username'\n        ' FROM post p JOIN user u ON p.author_id = u.id'\n        ' ORDER BY created DESC'\n    ).fetchall()\n    return render_template('blog/index.html', posts=posts)\n```\n\n\n\nDefine index template in file: `flaskr/templates/index.html`:\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Posts{% endblock %}\u003c/h1\u003e\n    {% if g.user %}\n        \u003ca class=\"action\" href=\"{{ url_for('blog.create') }}\" \u003eNew\u003c/a\u003e\n    {% endif %}\n{% endblock %}\n\n{% block content %}\n    {% for post in posts %}\n        \u003carticle class=\"post\"\u003e\n            \u003cheader\u003e\n                \u003cdiv\u003e\n                    \u003ch1\u003e{{ post['title'] }}\u003c/h1\u003e\n                    \u003cdiv class=\"about\"\u003eby {{ post['username'] }} on {{ post['created'].strftime('%Y-%m-%d') }}\u003c/div\u003e\n                \u003c/div\u003e\n                {% if g.user['id'] == post['author_id'] %}\n                    \u003ca class=\"action\" href=\"{{ url_for('blog.update', id=post['id']) }}\"\u003eEdit\u003c/a\u003e\n                {% endif %}\n            \u003c/header\u003e\n        \u003c/article\u003e\n        {% if not loop.last %}\n            \u003chr\u003e\n        {% endif %}\n    {% endfor %}\n{% endblock %}\n```\n\nTips:\n\n* when a user is logged in, the `header` block adds a link to the `create` view.\n* when the user is the author of a post, an `Edit` link to the `update` view will be seen.\n* `loop.last` is a special variable of Jinja's loop.\n\n\n\n#### Create: view and template\n\nThe `blog.create` view acts the similar way as `auth.register` view.\n\nDefine `blog.create` view in file: `flaskr/blog.py`:\n\n```python\n@bp.route('/create', methods=('GET', 'POST'))\n@login_required\ndef create():\n    if request.method == 'POST':\n        title = request.form['title']\n        body = request.form['body']\n        error = None\n\n        if not title:\n            error = 'Title is requested.'\n        \n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                'INSERT INTO post (title, body, author_id)'\n                ' VALUES (?, ?, ?)',\n                (title, body, g.user['id'])\n            )\n            db.commit()\n            return redirect(url_for('blog.index'))\n            \n    return render_template('blog/create.html')\n```\n\n\n\nTips:\n\n* if a new post is POSTed, add it into the database.\n* Or display the form of creating post in the `create` template.\n* Login_required decorator is used here to insure the user is logged-in before a new post is created.\n\n\n\nDefine the `create` template in file: `flaskr/templates/blog/create.html`\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}New Post{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"POST\"\u003e\n        \u003clabel for=\"title\"\u003eTitle\u003c/label\u003e\n        \u003cinput name=\"title\" id=\"title\" value=\"{{ request.form['title'] }}\" required\u003e\n        \u003clabel for=\"body\"\u003eBody\u003c/label\u003e\n        \u003ctextarea name=\"body\" id=\"body\"\u003e{{ request.form['body'] }}\u003c/textarea\u003e\n        \u003cinput type=\"submit\" value=\"Save\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n\n#### Update: view and template\n\nWrite a `get_post()` function to fetch a post by id and check if the author equals the logged in user. This function will be used in `update` and `delete` view.\n\n\n\nDefine get_post() function in file: `flaskr/blog.py`:\n\n```python\ndef get_post(id, check_author=True):\n    post = get_db().execute(\n        'SELECT p.id, title, body, created, author_id, user_name'\n        ' FROM post p JOIN user u ON p.author_id=u.id'\n        ' WHERE p.id = ?',\n        (id,)\n    ).fetchone()\n\n    if post is None:\n        abort(404, f'Post id {id} does not exist.')\n\n    if check_author and post['author_id'] != g.user['id']:\n        abort(403)\n    \n    return post\n```\n\nTips:\n\n* `abort()` will raise a exception that returns an HTTP status code like 404 (Not Found) or 403 (Forbidden).\n\n\n\nDefine `update` view in file: `flaskr/blog.py`:\n\n```python\n\n@bp.route('/\u003cint:id\u003e/update', methods=('GET', 'POST'))\n@login_required\ndef update(id):\n    post = get_post(id)\n\n    if request.method == 'POST':\n        title = request.form['title']\n        body = request.form['body']\n        error = None\n\n        if not title:\n            error = 'Title is required.'\n        \n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                'UPDATE post SET title = ?, body = ?'\n                ' WHERE id = ?',\n                (title, body, id)\n            )\n            db.commit()\n            return redirect(url_for('blog.index'))\n        \n    return render_template('blog/update.html', post=post)\n```\n\nTips:\n\n* `update` view takes an argument `id` , which corresponds to the `\u003cint:id\u003e` in the route. A real URL will look like `/1/update`. And the url_for() function also needs to be passed the `id` argument in the way of `url_for('blog.update', id=post['id'])`.\n\n\n\nDefine the update template in file: `flaskr/templates/blog/update.html`\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Edit \"{{ post['title'] }}\"{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"POST\"\u003e\n        \u003clabel for=\"title\"\u003eTitle\u003c/label\u003e\n        \u003cinput name=\"title\" id=\"title\" value=\"{{ request.form['title'] or post['title'] }}\" required\u003e\n        \u003clabel for=\"body\"\u003eBody\u003c/label\u003e\n        \u003ctextarea name=\"body\" id=\"body\"\u003e{{ request.form['body'] or post['body'] }}\u003c/textarea\u003e\n        \u003cinput type=\"submit\" value=\"Save\"\u003e\n    \u003c/form\u003e\n    \u003chr\u003e\n    \u003cform action=\"{{ url_for('blog.delete', id=post['id']) }}\" method=\"post\"\u003e\n        \u003cinput type=\"submit\" value=\"Delete\" class=\"danger\" onclick=\"return confirm('Are you sure?');\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n\nTips:\n\n* This templates has two forms:\n  * The first one to edit the current post(`/\u003cid\u003e/update`)\n  * The other one to delete the post\n* The pattern `{{ request.form['title'] or post['title'] }}` is used to choose what data appears in the form. \n  * When the form hasn’t been submitted, the original `post` data appears\n  * but if invalid form data was posted you want to display that so the user can fix the error, so `request.form` is used instead.\n\n\n\n#### Delete: view\n\nThe delete view has no template. Define it:\n\n```python\n@bp.route('/\u003cint:id\u003e/delete', methods=('POST',))\n@login_required\ndef delete(id):\n    get_post(id)\n    db = get_db()\n    db.execute('DELETE FROM post WHERE id = ?', (id,))\n    db.commit()\n    return redirect(url_for('blog.index'))\n\n```\n\n\n\nNow all code are finished. Try it!\n\n\n\nlog in \n\n![image-20221203225939183](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203225939183.png)\n\n\n\nLog out\n\n![image-20221203230028831](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230028831.png)\n\n\n\nCreate a post\n\n![image-20221203230103978](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230103978.png)\n\n\n\nWriting\n\n![image-20221203230232647](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230232647.png)\n\n\n\nsaved\n\n![image-20221203230339421](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230339421.png)\n\nedit\n\n![image-20221203230548770](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230548770.png)\n\n\n\nGreat!\n\n\n\n### Step 9. Make the Project Installable\n\n#### Describe the Project\n\nIn order to make the project Installabe, write a `setup.py` file to describe the project and its dependencies.\n\n```python\nfrom setuptools import find_packages, setup\n\nsetup(\n    name='flaskr',\n    version='1.0.0',\n    description='a simple blog post app based on Flask',\n    packages=find_packages(),\n    include_package_data=True,\n    requires=[\n        'flask'\n    ]\n)\n```\n\nTips:\n\n* `packages` tells Python what package directories to include, and `find_packages()` function finds them automatically.\n* To include other files, such as the static and templates directories, `include_package_data` is set. \n* `requires` tells what modules need to be installed as the project's dependencies.\n* Python needs another file named `MANIFEST.in` to tell what this other data is.\n\n\n\nFile `MANIFEST.in`\n\n```\ninclude flaskr/schema.sql\ngraft flaskr/static\ngraft flaskr/templates\nglobal-exclude *.pyc\n```\n\nThis tells Python to copy everything in the `static` and `templates` directories, and the `schema.sql` file, but to exclude all bytecode files.\n\n\n\n#### Install the Project\n\nUse pip to install your project in the virtual environment.\n\n```shell\n$ pip install -e .\nLooking in indexes: https://pypi.douban.com/simple\nObtaining file:///Users/yangls06/work/flask/flask-tutorial/flaskr\n  Preparing metadata (setup.py) ... done\nInstalling collected packages: flaskr\n  Running setup.py develop for flaskr\nSuccessfully installed flaskr-1.0.0\n```\n\nThis tells pip to find `setup.py` in the current directory and install it in *editable* or *development* mode. \n\nEditable mode means that as you make changes to your local code, you’ll only need to re-install if you change the metadata about the project, such as its dependencies.\n\n\n\nYou can observe that the project is now installed with `pip list`.\n\n```shell\n$ pip list\nPackage            Version Editable project location\n------------------ ------- ------------------------------------------------\nclick              8.1.3\nFlask              2.2.2\nflaskr             1.0.0   /Users/yangls06/work/flask/flask-tutorial/flaskr\nimportlib-metadata 5.1.0\nitsdangerous       2.1.2\nJinja2             3.1.2\nMarkupSafe         2.1.1\npip                22.3.1\nsetuptools         58.1.0\nWerkzeug           2.2.2\nzipp               3.11.0\n```\n\nNothing changes from how you’ve been running your project so far. `--app` is still set to `flaskr` and `flask run` still runs the application, but you can call it from **anywhere**, not just the `flask-tutorial` directory.\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-html-css":{"title":"Hands on HTML \u0026 CSS","content":"\n# Hands on HTML \u0026 CSS\n\n\n\n\u003e This is my practices on HTML\u0026CSS in order to develop the front-end of websites.\n\n## References\n\n[MDN HTML](https://developer.mozilla.org/zh-CN/docs/Web/HTML)\n\n[HTML basics](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics)\n\n[CSS basics](https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/CSS_basics)\n\n\n\n## HTML Basics\n\n### HTML element\n\nA HTML document describes the content of a web page, which contains many [HTML element](https://developer.mozilla.org/zh-CN/docs/Glossary/Element)s:\n\n![Example: in \u003cp class=\"nice\"\u003eHello world!\u003c/p\u003e, '\u003cp class=\"nice\"\u003e' is an opening tag, 'class=\"nice\"' is an attribute and its value, 'Hello world!' is enclosed text content, and '\u003c/p\u003e' is a closing tag.](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/anatomy-of-an-html-element.png)\n\n\n\n### Basic HTML\n\nA basic `index.html` is:\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eMy test html page\u003c/title\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cp\u003e\u003cstrong\u003eHTML tag: h1~h5\u003c/strong\u003e\u003c/p\u003e\n    \u003ch1\u003eHTML Basics\u003c/h1\u003e\n    \u003ch2\u003e2023年，崭新的一年\u003c/h2\u003e\n    \u003ch3\u003e2023年，崭新的一年\u003c/h3\u003e\n    \u003c!-- \u003ch4\u003e2023年，崭新的一年\u003c/h4\u003e\n    \u003ch5\u003e2023年，崭新的一年\u003c/h5\u003e\n    \u003ch6\u003e2023年，崭新的一年\u003c/h6\u003e --\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: img\u003c/strong\u003e\u003c/p\u003e\n    \u003cimg src=\"images/2023.jpg\" alt=\"my test image\" width=\"600\" height=\"400\"\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: p\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e新的转机和闪闪的星斗，正在缀满没有遮拦的天空。这是5000年的象形文字，这是未来人们凝视的眼睛👁。\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: strong\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003e新的转机和闪闪的星斗，正在缀满没有遮拦的天空。这是5000年的象形文字，\u003cstrong\u003e这是未来人们凝视的眼睛👁。\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: Unordered List\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eAt Mozilla, we're a global community of\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003etechnologists\u003c/li\u003e\n        \u003cli\u003ethinkers\u003c/li\u003e\n        \u003cli\u003ebuilders\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eworking together…\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: Ordered List\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eAt Mozilla, we're a global community of\u003c/p\u003e\n    \u003col\u003e\n        \u003cli\u003etechnologists\u003c/li\u003e\n        \u003cli\u003ethinkers\u003c/li\u003e\n        \u003cli\u003ebuilders\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eworking together…\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: a\u003c/strong\u003e\u003c/p\u003e\n    \u003ca href=\"https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/HTML_basics\"\u003emdn HTML Basics\u003c/a\u003e\n\u003c/body\u003e\n```\n\n\n\nThe page based on it:\n\n![e3ed2a84-b999-4113-b138-310a22c9ab35](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/e3ed2a84-b999-4113-b138-310a22c9ab35.jpeg) \n\n## CSS Basics\n\n[CSS basics](https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/CSS_basics)\n\nCSS is all about `style` (how it looks) of web content. It concerns the following questions:\n\n* How do I make text red? \n* How do I make content display at a certain location in the (webpage) layout? \n* How do I decorate my webpage with background images and colors?\n\n\n\n### File Layout\n\n```shell\n$ tree -L 2\n.\n├── images\n│   └── 2023.jpg\n├── index.html\n└── styles\n    └── style.css\n```\n\nWe store the CSS content in a `.css` file under the `styles` folder. The content of `style.css` is like:\n\n```css\np {\n    color: red;\n}\n```\n\nand imports it in `index.html`:\n\n```html\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eMy test html page\u003c/title\u003e\n  \n    \u003c!-- import css here --\u003e\n    \u003clink href=\"styles/style.css\" rel=\"stylesheet\"\u003e \n\u003c/head\n```\n\nwhich makes all the `\u003cp\u003e` elements red, as following:\n\n![image-20230105145429781](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105145429781.png)\n\n\n\n### CSS Rulesets\n\nThe basic element of CSS is called `ruleset`, which is explained as following:\n\n![CSS p declaration color red](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/css-declaration-small.png)\n\n* SelectorIt defines the element(s) to be styled (in this example, `\u003cp\u003e` elements). To style a different element, change the selector.\n\n* Declaration. It specifies which of the element's **properties** you want to style, like `color: red;`.\n\n\n\nYou can change the style:\n\n```css\np {\n    color: blue;\n    width: 500px;\n    border: 1px solid black;\n  }\n```\n\n\n\n![image-20230105160938495](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105160938495.png)\n\nYou can select multiple elements using `,`:\n\n```css\np,\nli,\nh1 {\n  color: red;\n}\n```\n\nwhich generates:\n\n![image-20230105161248652](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105161248652.png)\n\n### Different Types of Selectors\n\nThere are 5 basic types of selectors.\n\n| Selector name                                              | What does it select                                          | Example                                                      |\n| :--------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n| Element selector (sometimes called a tag or type selector) | All HTML elements of the specified type.                     | `p` selects `\u003cp\u003e`                                            |\n| ID selector                                                | The element on the page with the specified ID. On a given HTML page, each id value should be unique. | `#my-id` selects `\u003cp id=\"my-id\"\u003e` or `\u003ca id=\"my-id\"\u003e`        |\n| Class selector                                             | The element(s) on the page with the specified class. Multiple instances of the same class can appear on a page. | `.my-class` selects `\u003cp class=\"my-class\"\u003e` and `\u003ca class=\"my-class\"\u003e` |\n| Attribute selector                                         | The element(s) on the page with the specified attribute.     | `img[src]` selects `\u003cimg src=\"myimage.png\"\u003e` but not `\u003cimg\u003e` |\n| Pseudo-class selector                                      | The specified element(s), but only when in the specified state. (For example, when a cursor hovers over a link.) | `a:hover` selects `\u003ca\u003e`, but only when the mouse pointer is hovering over the link. |\n\n### Fonts and Text\n\nFirst, find the [output from Google Fonts](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/What_will_your_website_look_like#font) that you previously saved from [What will your website look like?](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/What_will_your_website_look_like). Add the `link` element somewhere inside your `index.html`'s head (anywhere between the `\u003chead\u003e` tags). It looks something like this:\n\n```html\n\u003clink href=\"https://fonts.googleapis.com/css?family=Open+Sans\" rel=\"stylesheet\" /\u003e\n```\n\n\n\nNext, delete the existing rule you have in your `style.css` file. It was a good test, but let's not continue with lots of red text.\n\n\n\nAdd the following lines (shown below), replacing the `font-family` assignment with your `font-family` selection. The property `font-family` refers to the font(s) you want to use for text. This rule defines a global base font and font size for the whole page. Since `\u003chtml\u003e` is the parent element of the whole page, all elements inside it inherit the same `font-size` and `font-family`.\n\n```css\nhtml {\n  font-size: 10px; /* px means \"pixels\": the base font size is now 10 pixels high */\n  font-family: \"Open Sans\", sans-serif; /* this should be the rest of the output you got from Google Fonts */\n}\n```\n\n\n\nThe page is like this now:\n\n![image-20230105164921181](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105164921181.png)\n\n\n\nNow let's set font sizes for elements that will have text inside the HTML body (`\u003ch1\u003e`, `\u003cli\u003e` and `\u003cp\u003e`). We'll also center the heading. Finally, let's expand the second ruleset (below) with settings for line height and letter spacing to make body content more readable.\n\n```css\nh1 {\n  font-size: 60px;\n  text-align: center;\n}\n\np,\nli {\n  font-size: 16px;\n  line-height: 2;\n  letter-spacing: 1px;\n}\n```\n\nwhich changes the font-size and other styles:\n\n![image-20230105165727652](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105165727652.png)\n\n\n\n### The Box Model\n\nMost HTML elements can be thought of as boxes sitting on the top of other boxes. CSS is about setting the size, color and position of boxes.\n\n![Three boxes sat inside one another. From outside to in they are labelled margin, border and padding](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/CSS_basics/box-model.png)\n\n Each box taking up space on your page has properties like:\n\n- `padding`, the space around the content. In the example below, it is the space around the paragraph text.\n- `border`, the solid line that is just outside the padding.\n- `margin`, the space around the outside of the border.\n\nIn this section we also use:\n\n- `width` (of an element).\n- `background-color`, the color behind an element's content and padding.\n- `color`, the color of an element's content (usually text).\n- `text-shadow` sets a drop shadow on the text inside an element.\n- `display` sets the display mode of an element. (keep reading to learn more)\n\n\n\n### Changing the Page Color\n\n```css\nhtml {\n  background-color: #00539f;\n}\n```\n\n### Styling the Body\n\n```css\nbody {\n  width: 600px;\n  margin: 0 auto;\n  background-color: #ff9500;\n  padding: 0 20px 20px 20px;\n  border: 5px solid black;\n}\n```\n\n- `width: 600px;` This forces the body to always be 600 pixels wide.\n- `margin: 0 auto;` When you set two values on a property like `margin` or `padding`, the first value affects the element's top *and* bottom side (setting it to `0` in this case); the second value affects the left *and* right side. (Here, `auto` is a special value that divides the available horizontal space evenly between left and right). You can also use one, two, three, or four values, as documented in [Margin Syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/margin#syntax).\n- `background-color: #FF9500;` This sets the element's background color. This project uses a reddish orange for the body background color, as opposed to dark blue for the [``](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/html) element. (Feel free to experiment.)\n- `padding: 0 20px 20px 20px;` This sets four values for padding. The goal is to put some space around the content. In this example, there is no padding on the top of the body, and 20 pixels on the right, bottom and left. The values set **top, right, bottom, left**, in that order. As with `margin`, you can use one, two, three, or four values, as documented in [Padding Syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/padding#syntax).\n- `border: 5px solid black;` This sets values for the width, style and color of the border. In this case, it's a five-pixel–wide, solid black border, on all sides of the body.\n\n### Positioning and Styling the Main Page Title\n\n```css\nh1 {\n  margin: 0;\n  padding: 20px 0;\n  color: #00539f;\n  text-shadow: 3px 3px 1px black;\n}\n```\n\n`text-shadow` applies a shadow to the text content of the element. Its four values are:\n\n- The first pixel value sets the **horizontal offset** of the shadow from the text: how far it moves across.\n- The second pixel value sets the **vertical offset** of the shadow from the text: how far it moves down.\n- The third pixel value sets the **blur radius** of the shadow. A larger value produces a more fuzzy-looking shadow.\n- The fourth value sets the base color of the shadow.\n\n### Centering the Image\n\n```css\nimg {\n  display: block;\n  margin: 0 auto;\n}\n```\n\nNext, we center the image to make it look better. We could use the `margin: 0 auto` trick again as we did for the body. But there are differences that require an additional setting to make the CSS work.\n\nThe `body` is a **block** element, meaning it takes up space on the page. The margin applied to a block element will be respected by other elements on the page. In contrast, images are **inline** elements, for the auto margin trick to work on this image, we must give it block-level behavior using `display: block;`\n\n\n\nNow, the page looks like this:\n\n![image-20230105174108098](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105174108098.png)\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-javascript":{"title":"Hands on JavaScript","content":"\n\n\n# JavaScript基础\n\n最近工作中会涉及到前端工作，因此要用到JavaScript，故而学习一波。参考：\n\n* https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_Overview\n\n## 环境准备：Jupyter Notebook + IJavaScript\n\n打算在Jupyter Notebook中使用JavaScript，便于交互式的学习和探查。于是找到了IJavascript这个Jupyter Notebook的JavaScript Kernel. \n其安装和使用参考主页：http://n-riesco.github.io/ijavascript/\n\n\u003e 注意📢：在安装的过程中（brew install pkg-config node zeromq），可能会出现“Error: No such file or directory @ rb_sysopen”的错误，可以参考：https://blog.csdn.net/weixin_43770545/article/details/127715990, 做相应的问题排查和修复。\n\n安装好IJavaScript之后，可以直接通过jupyter-lab（或者jupyter notebook）命令启动，然后选择JavaScript内核的Notebook。\n\n![image-20221222100114102](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221222100114102.png)\n\n然后就能在其中测试和探索JavaScript的各种功能。\n\n下面是跟随https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_Overview 教程一路run下来的结果。\n\n## Data types：数据类型\nJavaScript中有7中原生类型：\n* Number：除了非常非常大的整数，Number可以表达任意数字（整数、浮点数均可）。\n* Bigint：非常非常大的整数。\n* String：字符串。\n* Boolean：布尔型，true/false。\n* Symbol：类似uuid，creating unique identifiers that won't collide。\n* Undefined：没有被赋值的变量。\n* Null：non-value\n\n其他的数据都属于`Object`，包括：\n* Function：函数不是一种特殊的数据结构，它只是可以被调用的一种特殊的object。\n* Array\n* Date\n* RegExp\n* Error\n\n\u003e 疑问❓：没有dict或者map吗？\n\n### Numbers\n\nJavaScript有两种数字类型：Number和Bigint。\n\nNumber既能表示整数（-(2^53-1)~2^53-1）,又能表示浮点数（最大值：1.79\\*10^308）\n\n\n```javascript\nconsole.log(3/2); // 1.5, not 1\n```\n\n    1.5\n\n\n\n```javascript\n// 浮点数可以存在不精确的情况\nconsole.log(0.1+0.2)\n```\n\n    0.30000000000000004\n\n\n\n```javascript\n// Number literals can also have prefixes to indicate the base \n// (binary, octal, decimal, or hexadecimal), or an exponent suffix.\n\nconsole.log(0b111110111); // 503\nconsole.log(0o767); // 503\nconsole.log(0x1f7); // 503\nconsole.log(5.03e2); // 503\n\n```\n\n    503\n    503\n    503\n    503\n\n\nBigint用来指定数值是整数，它是在数字后面跟一个后缀：`n`\n\n\n```javascript\nconsole.log(-3n)\n```\n\n    -3n\n\n\n\n```javascript\n// console.log(-3.1n)\n```\n\n\n```javascript\nconsole.log(-3n/2n)\n```\n\n    -1n\n\n\n\n```javascript\n// Bigint与Number不能混合运算\n// console.log(-3n + 2); //TypeError: Cannot mix BigInt and other types, use explicit conversions\n```\n\nMath是一个提供标准数据运算的Object。\n\n\n```javascript\nMath.sin(3.5);\n```\n\n\n\n\n    -0.35078322768961984\n\n\n\n\n```javascript\nvar r0 = 2;\nconst circumference0 = 2 * Math.PI * r0;\n```\n\n\n```javascript\ncircumference0\n```\n\n\n\n\n    12.566370614359172\n\n\n\n可以使用下面的方式做数字和字符串之间的转换：\n* parseInt()：把字符串解析成整数。\n* parseFloat()：将字符串解析成一个浮点数。\n* Number()：将表示Number数值的字符串解析成Number类型的值。也可以使用`+/-`符号来代替Number()函数。\n\n\n```javascript\nparseInt('123');\n```\n\n\n\n\n    123\n\n\n\n\n```javascript\nparseInt('123n');\n```\n\n\n\n\n    123\n\n\n\n\n```javascript\nparseInt('-123');\n```\n\n\n\n\n    -123\n\n\n\n\n```javascript\nparseInt('123.4'); // 可以解析小数，只是得到其整数部分\n```\n\n\n\n\n    123\n\n\n\n\n```javascript\nparseFloat('123.4'); \n```\n\n\n\n\n    123.4\n\n\n\n\n```javascript\nparseInt('0b111110111'); // 不能正确解析其他进制的数\n```\n\n\n\n\n    0\n\n\n\n\n```javascript\nNumber('0b111110111'); \n```\n\n\n\n\n    503\n\n\n\n\n```javascript\nNumber('0o767'); \n```\n\n\n\n\n    503\n\n\n\n\n```javascript\n+'0o767'\n```\n\n\n\n\n    503\n\n\n\n\n```javascript\n-'0b111110111'\n```\n\n\n\n\n    -503\n\n\n\n`NaN`表示Not a Number，比如解析一个非数字表达；传入`NaN`做运算，会返回`NaN`。\n\n`Infinity`表示无穷大，除以0会产生这个值。它有正负之分。\n\n\n```javascript\nparseInt('Not a Number');\n```\n\n\n\n\n    NaN\n\n\n\n\n```javascript\nNaN + 1\n```\n\n\n\n\n    NaN\n\n\n\n\n```javascript\n1/0\n```\n\n\n\n\n    Infinity\n\n\n\n\n```javascript\n-1/0\n```\n\n\n\n\n    -Infinity\n\n\n\n### String\n\nJavaScript中的字符串就是Unicode（准确说是UTF-16编码）字符的序列。\n\n\n\n```javascript\nconsole.log('Hello, world');\nconsole.log('你好，世界！');\n```\n\n    Hello, world\n    你好，世界！\n\n\n\n```javascript\n// 单引号和双引号都OK\nconsole.log(\"Hello, world\");\nconsole.log(\"你好，世界！\");\n```\n\n    Hello, world\n    你好，世界！\n\n\n\n```javascript\n// 字符和字符串之间也没有差别：字符就是长度为1的字符串\n'Hello'[1] === 'e'\n```\n\n\n\n\n    true\n\n\n\n\n```javascript\n// 字符串长度：length属性\n'Hello'.length\n```\n\n\n\n\n    5\n\n\n\n\n```javascript\n// 字符串相加\nconst age = 25;\nconsole.log('I am ' + age + ' years old.') // String concatenation\n```\n\n    I am 25 years old.\n\n\n\n```javascript\n// 也可用字符串模板：使用反引号 `` + ${}\nconsole.log(`I am ${age} years old.`)\n```\n\n    I am 25 years old.\n\n\n### 其他类型\n\n在JavaScript中，`null`表示deliberate non-value（故意的空值），`undefined`表示absence of value（没有定义的值）。`null`只能通过null关键字获取，而`undefined`可以通过下面的多种方式获取：\n* return语句不带值（return;）\n* 访问object一个并不存在的属性（obj.iDontExist）\n* 申明一个变量但不赋值(let x;)\n\n\n```javascript\nfunction returnNothing() {\n    return\n}\n\nconsole.log(returnNothing())\n```\n\n    undefined\n\n\n\n```javascript\nvar arr = [1, 2, 3]\n\nconsole.log(arr.iDontExist)\n```\n\n    undefined\n\n\n\n```javascript\nlet xxx;\nconsole.log(xxx)\n```\n\n    undefined\n\n\nJavaScript的布尔值`true`和`false`，任何值都可以转换成布尔值，其规则如下：\n* `false`, `0`, 空字符串`\"\"`, `NaN`, `null`以及`undefined`被转换成`false`\n* 其他值都被转换成`true`\n\n\n```javascript\nBoolean(\"\")\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean(0)\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean(0.0)\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean(undefined)\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean('Hello')\n```\n\n\n\n\n    true\n\n\n\n## Variables: 变量\n\nJavaScript中，变量可以由下面的三个关键字申明：`let`, `const`, `var`。他们之间的差别可以参考：\n\n[JavaScript 中的 Var、Let 和 Const 有什么区别](https://www.freecodecamp.org/chinese/news/javascript-var-let-and-const/#:~:text=var%20%E5%A3%B0%E6%98%8E%E6%98%AF%E5%85%A8%E5%B1%80%E4%BD%9C%E7%94%A8,%E5%85%B6%E4%BD%9C%E7%94%A8%E5%9F%9F%E7%9A%84%E9%A1%B6%E7%AB%AF%E3%80%82)。\n\n总结其异同点：\n* `var`声明是全局作用域或函数作用域，而`let`和`const`是块作用域。\n* `var`变量可以在其范围内更新和重新声明； `let`变量可以被更新但不能重新声明； `const`变量既不能更新也不能重新声明。\n* 它们都被提升到其作用域的顶端。但是，虽然使用变量undefined初始化了`var`变量，但未初始化`let`和`const`变量。\n* 尽管可以在不初始化的情况下声明`var`和`let`，但是在声明期间必须初始化`const`。\n\n现在`let`是主流的申明方式。\n\n下面通过几个例子来说明。\n\n\n```javascript\n// var的作用域是全局或者函数范围\n\nvar greeter = 'hi';\n\nfunction newFunc() {\n    var hello = 'hello';\n}\n\nconsole.log(greeter);\n// console.log(hello); // ReferenceError: hello is not defined\n```\n\n    hi\n\n\n\n```javascript\n// var变量可以重新申明和修改\nvar greeter = 'hi';\ngreeter = 'hello';\nconsole.log(greeter);\n\nvar greeter = 'hi hi hi';\nconsole.log(greeter);\n```\n\n    hello\n    hi hi hi\n\n\n\n```javascript\n// var 的变量提升\n// 变量提升是 JavaScript 的一种机制:在执行代码之前，变量和函数声明会移至其作用域的顶部。这意味着如果我们这样做:\n\nconsole.log(greeter0);\nvar greeter0 = 'hi 0';\n```\n\n    undefined\n\n\n\n```javascript\n// 上面的代码会被解释为：\nvar greeter0;\nconsole.log(greeter0);\ngreeter0 = 'hi 0';\n```\n\n    hi 0\n\n\n\n\n\n    'hi 0'\n\n\n\n\n```javascript\n// var的问题：因为全局作用域造成的不希望发生的赋值和引用\n\nvar greeter1 = 'hi';\nvar times = 4;\n\nif(times \u003e 3) {\n    var greeter1 = 'Hello'\n}\n\nconsole.log(greeter1)\n\n```\n\n    Hello\n\n\n上例中，if block中的greeter1影响了全局greeter1的值。这有可能是我们不希望看到的。这就是使用let的原因：\n\n\n```javascript\n// let的作用域是block {} 级别的\n\nlet greeter3 = 'hi';\nlet times1 = 4;\n\nif(times1 \u003e 3) {\n    let greeter3 = 'Hello';\n    console.log(greeter3);\n}\n\nconsole.log(greeter3);\n```\n\n    Hello\n    hi\n\n\n\n```javascript\n// let变量能被修改，但不能被重新申明\nlet greeter5 = 'hi';\ngreeter5 = 'hello';\n\nconsole.log(greeter5);\n```\n\n    hello\n\n\n\n```javascript\n// let greeter5 = 'hi'; //SyntaxError: Identifier 'greeter5' has already been declared\n```\n\n`const`与`let`类似，只不过其申明的变量必须保持常量，不能被修改。\n\n\n```javascript\nconst greeter6 = 'hi';\n// greeter6 = 'hello'; //TypeError: Assignment to constant variable.\n```\n\n不过其申明的object类型的变量，其属性是可以被改变的。\n\n\n```javascript\nconst obj_greeter = {\n    message: 'hi',\n    times: 4\n}\n\nobj_greeter.message = 'hello'\n\nconsole.log(obj_greeter)\n```\n\n    { message: 'hello', times: 4 }\n\n\n\n```javascript\nobj_greeter.receiver = 'Jack'\n\nconsole.log(obj_greeter)\n```\n\n    { message: 'hello', times: 4, receiver: 'Jack' }\n\n\nJavaScript是动态类型，意味着同一个变量名可以指向不同类型的数据。\n\n\n```javascript\nlet a = 1;\na = 'foo';\n```\n\n\n\n\n    'foo'\n\n\n\n## Operators：运算符\n\nJavaScript支持的运算符包括：\n* `+`, `-`, `*`, `/`, `%`, `**`: 数学运算\n* `=`, `+=`, `-=`: 赋值\n* `++`, `--`: 自加自减\n* `+`: 字符串拼接\n* `\u003e`, `\u003c`, `\u003e=`, `\u003c=`: 不等比较\n* `==`: 双等号，不同类型会强制转换（type coercion）, 对应的不等号是`!=`\n* `===`: 三等号，不同类型不会强制转换, 对应的不等号是`!==`\n* `\u0026\u0026`, `||`, `!`: 逻辑运算：与或非\n* `\u0026`, `|`, `~`: 位运算：与或非\n\n更详尽的运算符说明，见[链接](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators)。\n\n下面是一些示例。\n\n\n```javascript\n// 求余数\n123 % (3.5)\n```\n\n\n\n\n    0.5\n\n\n\n\n```javascript\n// 注意顺序\n\"3\" + 4 + 5;\n```\n\n\n\n\n    '345'\n\n\n\n\n```javascript\n3 + 4 + '5';\n```\n\n\n\n\n    '75'\n\n\n\n\n```javascript\n// 双等号 vs 三等号\n\nconsole.log(123 == '123');\nconsole.log(1 == true);\n\nconsole.log(123 === '123');\nconsole.log(1 === true);\n```\n\n    true\n    true\n    false\n    false\n\n\n\n```javascript\n// 逻辑运算\nconst a1 = 0 \u0026\u0026 'Hello'; // 0 because 0 is \"falsy\"\nconsole.log(a1);\n```\n\n    0\n\n\n\n```javascript\nconst b1 = \"Hello\" || \"world\"; // \"Hello\" because both \"Hello\" and \"world\" are \"truthy\"\nconsole.log(b1);\n```\n\n    Hello\n\n\n## Grammar: 语法风格\n\nJavaScript的语法风格接近于C语言。有下面几点值得注意：\n* 注释：单行用`//`，多行用`/* */`\n* 表达式的结尾用';'：但这个分号也是可选的\n\n## Control Structure: 控制结构\n\n和大多数语言一样，JavaScript包括下面的控制结构：\n* `if`, `else`: 条件\n* `while`, `do...while`: 循环\n* `for`, 除了常规的for循环，还衍生出两种特殊的遍历：\n    * `for...of`: 遍历数组的各个元素\n    * `for...in`: 遍历object的各个属性\n* `switch`: 分支\n* `try...catch`, `throw`: 异常\n\n## Objects: 对象类型\n\nJavaScript的object类型用来放键值对key-value数据，相当于Python中的dict. object是非常动态的, 它的属性可以随时被添加、删除、重排、突变mutated。object的key总是string或者symbol。\n\n\n```javascript\nconst obj = {\n    name: 'Carrot',\n    for: 'Max',\n    details: {\n        color: 'orange',\n        size: 12\n    }\n}\n\nobj\n```\n\n\n\n\n    { name: 'Carrot', for: 'Max', details: { color: 'orange', size: 12 } }\n\n\n\n\n```javascript\n// dot notation: 只能是一个静态的标识符\nconsole.log(obj.name);\n\n// bracket notation：可以是一个动态的变量\nconsole.log(obj['name'])\n```\n\n    Carrot\n    Carrot\n\n\n\n```javascript\n// 用变量来标识一个key\nlet userName1 = 'nick';\nobj[userName1] = 'Catty';\nobj\n```\n\n\n\n\n    {\n      name: 'Carrot',\n      for: 'Max',\n      details: { color: 'orange', size: 12 },\n      nick: 'Catty'\n    }\n\n\n\n\n```javascript\n// 链式访问\nconsole.log(obj.details.color);\nconsole.log(obj['details']['size']);\n```\n\n    orange\n    12\n\n\n\n```javascript\n// object是按照引用传值\nconst obj1 = {}\n\nfunction doSth(o) {\n    o.x = 1;\n}\n\ndoSth(obj1);\n\nobj1\n```\n\n\n\n\n    { x: 1 }\n\n\n\n\n```javascript\n// 引用\nconst obj2 = obj1;\nobj1.y = 2;\n\nobj2\n```\n\n\n\n\n    { x: 1, y: 2 }\n\n\n\n## Arrays: 数组\n\nJavaScript中的数组是一种特殊的object，通过`[index]`来访问元素。数组长度通过`.length`来获取。\n\n\n```javascript\nconst arr1 = ['dog', 'cat', 'hen'];\na.length\n```\n\n\n\n\n    3\n\n\n\n\n```javascript\n// 可以给任意非负整数的下标赋值\narr1[10] = 'fox'\n\narr1\n```\n\n\n\n\n    [ 'dog', 'cat', 'hen', \u003c7 empty items\u003e, 'fox' ]\n\n\n\n\n```javascript\n// 数组访问越界是不会报错的，只会放回一个undefined\nconsole.log(arr1[100])\n```\n\n    undefined\n\n\n\n```javascript\n// 数组元素可以是任意类型的\narr1.push\narr1.push(false);\narr1.push(null);\narr1.push(101);\narr1.push({});\narr1.push([]);\n\narr1\n```\n\n\n\n\n    [\n      'dog',\n      'cat',\n      'hen',\n      \u003c7 empty items\u003e,\n      'fox',\n      false,\n      null,\n      101,\n      {},\n      []\n    ]\n\n\n\n\n```javascript\narr1[14].x = 1;\narr1[15].push('ok');\n\narr1\n```\n\n\n\n\n    [\n      'dog',\n      'cat',\n      'hen',\n      \u003c7 empty items\u003e,\n      'fox',\n      false,\n      null,\n      101,\n      { x: 1 },\n      [ 'ok' ]\n    ]\n\n\n\n\n```javascript\n// 遍历： 通过for\n\nfor(let i = 0; i \u003c arr1.length; i++) {\n    console.log('#' + i + ' : ' + arr1[i])\n}\n```\n\n    #0 : dog\n    #1 : cat\n    #2 : hen\n    #3 : undefined\n    #4 : undefined\n    #5 : undefined\n    #6 : undefined\n    #7 : undefined\n    #8 : undefined\n    #9 : undefined\n    #10 : fox\n    #11 : false\n    #12 : null\n    #13 : 101\n    #14 : [object Object]\n    #15 : ok\n\n\n\n```javascript\n// 遍历： for...of\nfor(const a of arr1) {\n    console.log(a);\n}\n```\n\n    dog\n    cat\n    hen\n    undefined\n    undefined\n    undefined\n    undefined\n    undefined\n    undefined\n    undefined\n    fox\n    false\n    null\n    101\n    { x: 1 }\n    [ 'ok' ]\n\n\nArray有一系列的方法，比如cancat, map, filter, slice等，详见[Array](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array)。\n\n\n```javascript\n// map\nconst babies = ['dog', 'cat', 'hen', 'fox'].map((name) =\u003e 'baby ' + name);\nbabies\n```\n\n\n\n\n    [ 'baby dog', 'baby cat', 'baby hen', 'baby fox' ]\n\n\n\n\n```javascript\n// concat: 数组拼接\nconst arr2 = ['a', 'b', 'c'];\nconst arr3 = ['d', 'e', 'f'];\nconst arr4 = arr2.concat(arr3);\n\narr4\n```\n\n\n\n\n    [ 'a', 'b', 'c', 'd', 'e', 'f' ]\n\n\n\n## Functions: 函数\n\n函数在JavaScript中非常重要，是其核心组成部分。下面是一个基础的函数声明：\n\n\n```javascript\nfunction add(x, y) {\n    const total = x + y;\n    return total;\n}\n```\n\n\n```javascript\n// 函数传参个数可以少于定义的参数个数，缺少的参数会被定义成undefined\nadd(); // Equivalent to add(undefined, undefined)\n```\n\n\n\n\n    NaN\n\n\n\n\n```javascript\n// 如果传入的参数个数多余定义的，多余的参数被直接忽略。总之，函数不会因为参数的多少而报语法错\nadd(1, 2, 3, 4);\n```\n\n\n\n\n    3\n\n\n\n函数的参数可以是rest parameter语法，通过一个数组来存储未明确指定的参数值，类似Python的`*args`（注：在语法层面没有`**kwargs`）。 \n\n\n```javascript\nfunction avg(...args) {\n    let sum = 0;\n    for (const item of args) {\n        sum += item;\n    }\n    return sum / args.length;\n}\n\navg(1, 2, 4, 5)\n```\n\n\n\n\n    3\n\n\n\n\n```javascript\n// 还可以这么调用\nconst arr5 = [5, 6, 7, 8]\navg(...arr5)\n```\n\n\n\n\n    6.5\n\n\n\n虽然JavaScript的函数不支持`**kwargs`这样的命名参数，但是可以通过传入object类型参数，通过`object destructuring`来`pack/unpack`。\n\n\n```javascript\n// Note the { } braces: this is destructuring an object\nfunction area({ width, height }) {\n  return width * height;\n}\n\n// The { } braces here create a new object\nconsole.log(area({ width: 2, height: 3 }));\n```\n\n    6\n\n\n\n```javascript\nconsole.log(area({ width1: 2, height1: 3 }));\n```\n\n    NaN\n\n\n\n```javascript\n// 与其他语言一样，JavaScript的函数支持参数的默认值\nfunction avg3(v1, v2, v3=0) {\n    return (v1 + v2 + v3)/3\n}\n\navg3(1, 2)\n```\n\n\n\n\n    1\n\n\n\n### Anonymous functions: 匿名函数\n\n匿名函数就是没有名字的函数。在实践中，匿名函数常常会作为参数传递给其他函数，或者立刻传入参数让函数立刻被调用（通常只被调用一次），抑或被另一个函数作为返回值返回。\n\n下面是一个匿名函数的定义方式：function后面不带函数名。\n\n\n```javascript\nconst avgFunc = function (...args) {\n    let sum = 0;\n    for (const item of args) {\n        sum += item;\n    }\n    return sum / args.length;  \n};\n\navgFunc(1,2,4);\n```\n\n\n\n\n    2.3333333333333335\n\n\n\n另一种定义的方式是通过arrow function expression，也就是`=\u003e`: \n\n\n```javascript\nconst avgFunc2 = (...args) =\u003e {\n    let sum = 0;\n    for (const item of args) {\n        sum += item;\n    }\n    return sum / args.length;  \n};\n\navgFunc2(1,2,4);\n```\n\n\n\n\n    2.3333333333333335\n\n\n\n\n```javascript\n// 对简单的表达式，可以不用return\nconst sumFunc = (a, b, c) =\u003e a + b + c;\nsumFunc(1, 2, 3);\n```\n\n\n\n\n    6\n\n\n\n\n```javascript\nconst sumFunc1 = (a, b, c) =\u003e {return a + b + c;};\nsumFunc1(1, 2, 3);\n```\n\n\n\n\n    6\n\n\n\n### Functions are first-class objects: 函数是头等公民\n\n函数像其他类型一样，可以被赋值给其他参数，或者传参给其他函数，以及被其他函数作为返回值返回。\n\n\n```javascript\nconst addxy = (x) =\u003e (y) =\u003e x + y;\n```\n\n\n```javascript\naddxy(1)\n```\n\n\n\n\n    [Function (anonymous)]\n\n\n\n\n```javascript\naddxy(1)(2)\n```\n\n\n\n\n    3\n\n\n\n\n```javascript\n// Function accepting function\nconst babies2 = [\"dog\", \"cat\", \"hen\"].map((name) =\u003e `baby ${name}`);\nbabies2\n```\n\n\n\n\n    [ 'baby dog', 'baby cat', 'baby hen' ]\n\n\n\n### Inner functions: 内部函数\n\n在函数内部定义的函数，内部函数可以访问上层函数作用域内的变量。这个特性可以在内部函数间共享上层函数的变量，从而避免污染全局变量。\n\n\n```javascript\nfunction parentFunc() {\n    const a = 1;\n    \n    function innerFunc() {\n        const b = 4;\n        return a + b;\n    };\n    \n    return innerFunc();\n};\n\nparentFunc();\n```\n\n\n\n\n    5\n\n\n\n## Classes: 类\n\nJavaScript的类定义类似Java：\n\n\n```javascript\nclass Person {\n    constructor(name) {\n        this.name = name;\n    };\n    \n    sayHello() {\n        return `Hello, I am ${this.name}!`;\n    };\n};\n\nconst p = new Person('Maria');\np.sayHello();\n```\n\n\n\n\n    'Hello, I am Maria!'\n\n\n\n## Asynchronous programming: 异步编程\n\nJavaScript is single-threaded by nature. There's *no paralleling; only concurrency*. Asynchronous programming is powered by an event loop, which allows a set of tasks to be queued and polled for completion.\n\nThere are three idiomatic ways to write asynchronous code in JavaScript:\n* Callback-based (such as setTimeout())\n* Promise-based\n* async/await, which is a syntactic sugar for Promises\n\nFor example, here's how a file-read operation may look like in JavaScript:\n\n\n```javascript\n// Callback-based\n\n/*\nfs.readFile(filename, (err, content) =\u003e {\n  // This callback is invoked when the file is read, which could be after a while\n  if (err) {\n    throw err;\n  }\n  console.log(content);\n});\n// Code here will be executed while the file is waiting to be read\n*/\n\n\n// Promise-based\n/*\nfs.readFile(filename)\n  .then((content) =\u003e {\n    // What to do when the file is read\n    console.log(content);\n  }).catch((err) =\u003e {\n    throw err;\n  });\n// Code here will be executed while the file is waiting to be read\n*/\n\n// Async/await\n\n/*\nasync function readFile(filename) {\n  const content = await fs.readFile(filename);\n  console.log(content);\n}\n*/\n```\n\n## Modules: 模块\n\n模块通常是一个js文件，可以被一个文件路径或者URL指定。可以通过import或者export在module之间交换数据。\n\n\n```javascript\n/*\n\nimport { foo } from \"./foo.js\";\n\n// Unexported variables are local to the module\nconst b = 2;\n\nexport const a = 1;\n\n*/\n```\n\n\n\n\n\n**这就是JavaScript的基本语法知识。Keep going!**\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-python-crud":{"title":"","content":"# Hands on Python CRUD","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-vue":{"title":"","content":"# Hands on Vue.js\n\n## Quick Start\n\nFollowing: https://vuejs.org/guide/quick-start.html\n\n\n\n```bash\n\n$ npm init vue@latest\n\nNeed to install the following packages:\n  create-vue@3.6.3\nOk to proceed? (y) y\n\nVue.js - The Progressive JavaScript Framework\n\n✔ Project name: … vue-quick-start\n✔ Add TypeScript? … No / Yes\n✔ Add JSX Support? … No / Yes\n✔ Add Vue Router for Single Page Application development? … No / Yes\n✔ Add Pinia for state management? … No / Yes\n✔ Add Vitest for Unit Testing? … No / Yes\n✔ Add an End-to-End Testing Solution? › No\n✔ Add ESLint for code quality? … No / Yes\n\nScaffolding project in /Users/yangls06/work/frontend/vue-quick-start...\n\nDone. Now run:\n\n  cd vue-quick-start\n  npm install\n  npm run dev\n\n```\n\n\n\n\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230512165933542.png\" alt=\"image-20230512165933542\" style=\"zoom:50%;\" /\u003e","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-zeppelin":{"title":"Hands on Zeppelin","content":"\n\n\n# Hands on Zeppelin: Step by Step\n\n\u003e In order to build a data playground enabling the engineers to explore data collected by autonomous cars, I am trying to deploy Apache Zeppelin as a component of data platform.\n\n## Introduction\n\n[Apache Zeppelin](https://zeppelin.apache.org/) is: \"Web-based notebook that enables data-driven,\ninteractive data analytics and collaborative documents with SQL, Scala, Python, R and more.\"\n\n\n\n## Install\n\nAccording to [installation document](https://zeppelin.apache.org/docs/0.10.0/quickstart/install.html), I choose to install Zeppelin using the offical docker on a server (http://10.10.32.4):\n\n```sh\n$ mkdir Zeppelin \u0026 cd Zeppelin\n$ docker run -u $(id -u) -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n           -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n           --name zeppelin apache/zeppelin:0.10.0\n\nUnable to find image 'apache/zeppelin:0.10.0' locally\n0.10.0: Pulling from apache/zeppelin\n16ec32c2132b: Pull complete\naafd5bdc2bb7: Pull complete\n0bb58b150809: Pull complete\n68d71ea3a296: Pull complete\n9c7277321f0c: Downloading [=============================================\u003e     ]   2.59GB/2.816GB\n6be3e4488900: Download complete\n622d30c2f649: Download complete\nd10a38bf471f: Download complete\n4006c4346d45: Download complete\n4f4fb700ef54: Download complete\n\n...\n\nERROR [2022-12-13 08:13:15,873] ({main} ZeppelinServer.java[main]:262) - Error while running jettyServer\njava.lang.Exception: A MultiException has 2 exceptions.  They are:\n1. java.io.IOException: Creating directories for /notebook/.git failed\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.zeppelin.notebook.repo.NotebookRepoSync\n\n\tat org.apache.zeppelin.server.ZeppelinServer.main(ZeppelinServer.java:256)\n\n```\n\n\n\nThese two error was raised because of [Volume mapping issue with Zeppelin](https://forums.docker.com/t/volume-mapping-issue-with-zeppelin/121917), you can fix it to add\n\n`-u 0` parameter to set the user to 0 (root) in docker run.\n\n```sh\n$ docker run -u $(id -u) -p 8080:8080 -u 0 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n           -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n           --name zeppelin apache/zeppelin:0.10.0\n```\n\n\n\nor you can change the owner of dir `logs` and `notebook` using:\n\n```sh\n$ sudo chown -R 1000:1000 notebook\n$ sudo chown -R 1000:1000 logs\n$ docker run -u $(id -u) -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n           -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n           --name zeppelin apache/zeppelin:0.10.0\n```\n\n\n\nThen visit http://10.10.32.4:8080/ to use Zeppelin.\n\n![image-20221213165658796](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221213165658796.png)\n\n\n\n## Create New Note\n\n![yshJJIkNnq](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/yshJJIkNnq.jpg)\n\nCreate a new note through these two entries by giving it a name and select a default interpreter.\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214150836370.png\" alt=\"image-20221214150836370\" style=\"zoom:50%;\" /\u003e\n\n\u003e You can use multiple interpreter in one zeppeline note to support different languages. You can get this via ChatGPT. See details in \"Q\u0026A (with ChatGPT on Zeppelin)\" part.\n\n![image-20221214151356720](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214151356720.png)\n\nI write 3 different code snippet in 3 different languages of Markdown, Python and sql. Run them and we will get:\n\n![image-20221214151758144](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214151758144.png)\n\nYou can see: \n\n* %md and %python work fine.\n* %sql gives an error: Interpreter sql not found.\n\n## Interpreters\n\nWe can see and manage Zeppelin's interpreters via the 'interpreter' menu below: \n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214151959692.png\" alt=\"image-20221214151959692\" style=\"zoom:50%;\" /\u003e\n\n\n\nAll interpreters are here:\n\n![image-20221214154255076](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214154255076.png)\n\n## Add a SQL Interpreter (for ADB)\n\nIn order to access data in mysql, add a SQL interpreter:\n\nReference: \n\n* [SQL support in Zeppelin](https://zeppelin.apache.org/docs/0.8.0/quickstart/sql_with_zeppelin.html)\n* [Generic JDBC Interpreter for Apache Zeppelin - mysql](https://zeppelin.apache.org/docs/0.8.0/interpreter/jdbc.html#mysql)\n\n\u003e If you want to connect other databases such as `Mysql`, `Redshift` and `Hive`, you need to edit the property values. You can also use [Credential](https://zeppelin.apache.org/docs/latest/setup/security/datasource_authorization.html) for JDBC authentication. If `default.user` and `default.password` properties are deleted(using X button) for database connection in the interpreter setting page, the JDBC interpreter will get the account information from [Credential](https://zeppelin.apache.org/docs/latest/setup/security/datasource_authorization.html).\n\n* [Data Source Authorization in Apache Zeppelin](https://zeppelin.apache.org/docs/0.7.0/security/datasource_authorization.html)\n* [Tutorial: Using Apache Zeppelin with MySQL](https://thedataist.com/tutorial-using-apache-zeppelin-with-mysql/)\n\n\n\nFirst, add a credential info, which stores safely the username and password of the mysql connection:\n\n![image-20221214164153294](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214164153294.png)\n\n\n\nSecond, add a new interpreter `adbsql` as \n\n![image-20221214164336089](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214164336089.png)\n\nRemember to add dependencies to locate the jar used to run the mysql driver:\n\n![image-20221214164508044](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214164508044.png)\n\n\n\n## Access Data using SQL\n\nThen I can access the data in mysql using sql like this:\n\n![image-20221214171612554](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214171612554.png)\n\n\n\n## Add MySQL Interpreter\n\n![image-20221215144609848](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215144609848.png)\n\nThe dependency should rather be `mysql:mysql-connector-java:5.1.44` than `5.1.41` used before, because \n\n[java.sql.SQLException: Unknown system variable 'query_cache_size'](https://stackoverflow.com/questions/49984267/java-sql-sqlexception-unknown-system-variable-query-cache-size)\n\n\u003e `query_cache_size` was removed in MySQL 8. Check the [docs](https://dev.mysql.com/doc/refman/5.7/en/query-cache.html).\n\u003e\n\u003e It works with JDBC driver 5.1.44.\n\n![image-20221215170718539](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215170718539.png)\n\nIf `mysql:mysql-connector-java:5.1.41` is used, `query_cache_size` error will be raised.\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215171148557.png\" alt=\"image-20221215171148557\" style=\"zoom:50%;\" /\u003e\n\n\n\nThen you can access the data in mysql db:\n\n![image-20221215171439961](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215171439961.png)\n\n## Store Zeppelin Notes to GitLab Repo\n\n[How To: Store Zeppelin Notes in GitHub repo](https://community.cloudera.com/t5/Community-Articles/How-To-Store-Zeppelin-Notes-in-GitHub-repo/ta-p/247398)\n\nYou can set the local dir for Zeppelin notes to be a git repo and use Zeppelin's version control feature to manage versions of a notes.\n\nAdd a new version and commit\n\n![image-20221215110158669](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215110158669.png)\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215110238902.png\" alt=\"image-20221215110238902\" style=\"zoom:50%;\" /\u003e\n\nThen `git log` will show the history of version:\n\n```bash\ngit log\ncommit 1f029d8073928a3a9e67e7728cd293472595d94a (HEAD -\u003e master)\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Thu Dec 15 03:03:04 2022 +0000\n\n    add dist stat\n\ncommit 836ca519affbc28ac97d998676b060f826b6073b (origin/master)\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Thu Dec 15 02:43:25 2022 +0000\n\n    test\n\ncommit a24ac32075750272a696cb0641693ff0040290ab\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Thu Dec 15 01:40:06 2022 +0000\n\n    test\n\ncommit f26c7f85d4b36a59bb7ec79152cbfc389c907a68\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Wed Dec 14 09:17:09 2022 +0000\n\n    Move note 2HPWKGXXY from test_note_of_zeppelin_2HPWKGXXY.zpln to 数据访问demo_2HPWKGXXY.zpln\n```\n\nUse `git push` to push local version to remote repo.\n\n\n\n## Q\u0026A (with ChatGPT on Zeppelin)\n\n### Can I use python and sql in one zeppelin note?\n\nChatGPT:\n\n\u003e Yes, it is possible to use Python and SQL together in a Zeppelin notebook. In fact, many data analysis tasks can benefit from combining the capabilities of both languages.\n\u003e\n\u003e \n\u003e\n\u003e To use Python and SQL together in a Zeppelin notebook, you will first need to make sure that you have the appropriate interpreters installed and configured. In Zeppelin, an interpreter is a plug-in that enables the notebook to execute code in a specific programming language. For Python and SQL, you will need to install and configure the Python and SQL interpreters, respectively.\n\u003e\n\u003e \n\u003e\n\u003e Once the interpreters are installed and configured, you can use the `%python` and `%sql` magic commands in your Zeppelin notebook to switch between Python and SQL code blocks. For example, you could use Python to load and clean your data, and then use SQL to analyze and visualize the cleaned data.\n\u003e\n\u003e Here is an example of how you might use Python and SQL together in a Zeppelin notebook:\n\n```python\n%python\n# load and clean data using Python\n\nimport pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf = df.dropna()\n\n%sql\n-- analyze and visualize data using SQL\n\nSELECT * FROM df LIMIT 10\n\n```\n\n\u003e I hope this helps! Let me know if you have any other questions.\n\n\n\n### SQL: how to add a sql interpreter to apache zeppelin\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214155408956.png\" alt=\"image-20221214155408956\" style=\"zoom:50%;\" /\u003e\n\n### Java: hello world in apache zeppelin\n\n[Java interpreter for Apache Zeppelin](https://zeppelin.apache.org/docs/0.9.0/interpreter/java.html)\n\nChatGPT:\n\n![image-20221214152806322](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214152806322.png)\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/how-to-create-a-chatgpt-clone":{"title":"","content":"# How to Create a ChatGPT Clone\n\n\u003e I want to explore the OpenAI API and ChatGPT magic by creating a Simple ChatGPT Clone. Here is all about it.\n\n\n\n## Reference\n\n[Youtube: Build ChatGPT Clone using Open AI API](https://www.youtube.com/watch?v=977mChpScU4\u0026ab_channel=ChatGPTTutorials) \n\n\n\n## Step by step\n\n### Step 1: Ask ChatGPT for coding\n\n\u003e Me：code a python flask server with '/prompt' route with post method and taking string input. use openai api and send string as prompt and return the response.\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's a sample code for a Flask server that implements the \"/prompt\" route with a POST method and uses the OpenAI API to generate a response based on a string input:\n\u003e\n\u003e ```python\n\u003e from flask import Flask, request\n\u003e import openai\n\u003e \n\u003e app = Flask(__name__)\n\u003e \n\u003e @app.route(\"/prompt\", methods=[\"POST\"])\n\u003e def prompt():\n\u003e     prompt_text = request.form.get(\"prompt\")\n\u003e     openai.api_key = \"YOUR_OPENAI_API_KEY\"\n\u003e     response = openai.Completion.create(\n\u003e         engine=\"text-davinci-002\",\n\u003e         prompt=prompt_text,\n\u003e         max_tokens=1024,\n\u003e         n=1,\n\u003e         stop=None,\n\u003e         temperature=0.5,\n\u003e     )\n\u003e     message = response[\"choices\"][0][\"text\"]\n\u003e     return message\n\u003e \n\u003e if __name__ == \"__main__\":\n\u003e     app.run()\n\u003e \n\u003e ```\n\n\u003e Me: generate the requirement.txt and setup.py for the above with 'Simple ChatGPT Clone' as the app name and yangls06 as my username.\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's the `requirements.txt` file for the above code:\n\u003e\n\u003e ```\n\u003e Flask==1.1.2\n\u003e openai==1.7.0\n\u003e \n\u003e ```\n\u003e\n\u003e And here's the `setup.py` file:\n\u003e\n\u003e ```python\n\u003e from setuptools import setup\n\u003e \n\u003e setup(\n\u003e     name='simple-chatgpt-clone',\n\u003e     version='0.1',\n\u003e     packages=[],\n\u003e     url='https://github.com/yangls06/simple-chatgpt-clone',\n\u003e     license='MIT',\n\u003e     author='yangls06',\n\u003e     author_email='your.email@example.com',\n\u003e     description='A Simple ChatGPT Clone built with Flask and OpenAI API',\n\u003e     install_requires=[\n\u003e         'Flask==1.1.2',\n\u003e         'openai==1.7.0'\n\u003e     ]\n\u003e )\n\u003e \n\u003e ```\n\u003e\n\u003e \n\u003e\n\u003e Note that you'll need to replace the email address with your own email address.\n\n\n\n\u003e Me: how to create a venv for this app and install it and run it in debug model using Flask cmd?\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's how you can create a virtual environment (venv) for this app and install it using Flask CLI in debug mode:\n\u003e\n\u003e ```\n\u003e bashCopy code\n\u003e # Create a virtual environment\n\u003e python3 -m venv venv\n\u003e \n\u003e # Activate the virtual environment\n\u003e source venv/bin/activate\n\u003e \n\u003e # Install the required packages\n\u003e pip install -r requirements.txt\n\u003e \n\u003e # Set the FLASK_APP and FLASK_ENV environment variables\n\u003e export FLASK_APP=app.py\n\u003e export FLASK_ENV=development\n\u003e \n\u003e # Run the Flask app in debug mode\n\u003e flask run\n\u003e ```\n\u003e\n\u003e Replace `app.py` with the name of the file that contains your Flask application, if it's different.\n\u003e\n\u003e With this configuration, you can now access the app in your web browser at `http://localhost:5000/`. Any changes you make to the code will be automatically reflected in the browser, without the need to restart the app. However, it's important to note that you should not use the Flask debugger in production, as it can expose sensitive information and allow arbitrary code execution.\n\n\n\n\u003e Me: create '/' default route with GET method and return index.html\n\u003e\n\u003e \n\n\n\n\u003e Me: code for 'index.html' with bootstrap, displaying input field and submit button. on click submit button check if input is not empty then send POST request to '/prompt' with text from input field as string and display the response.\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's an example of how you can code 'index.html' with Bootstrap, an input field, and a submit button. The HTML file will look for an onclick event for the submit button, and if the input field is not empty, it will send a POST request to the '/prompt' route. Finally, it will display the response from the server.\n\u003e\n\u003e ```html\n\u003e \u003c!DOCTYPE html\u003e\n\u003e \u003chtml\u003e\n\u003e   \u003chead\u003e\n\u003e     \u003cmeta charset=\"utf-8\"\u003e\n\u003e     \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n\u003e     \u003clink rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\"\u003e\n\u003e     \u003cscript src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"\u003e\u003c/script\u003e\n\u003e   \u003c/head\u003e\n\u003e   \u003cbody\u003e\n\u003e     \u003cdiv class=\"container\"\u003e\n\u003e       \u003ch2\u003eSimple ChatGPT Clone\u003c/h2\u003e\n\u003e       \u003cform\u003e\n\u003e         \u003cdiv class=\"form-group\"\u003e\n\u003e           \u003clabel for=\"prompt\"\u003eEnter a prompt:\u003c/label\u003e\n\u003e           \u003cinput type=\"text\" class=\"form-control\" id=\"prompt\"\u003e\n\u003e         \u003c/div\u003e\n\u003e         \u003cbutton type=\"button\" class=\"btn btn-default\" id=\"submit\"\u003eSubmit\u003c/button\u003e\n\u003e       \u003c/form\u003e\n\u003e       \u003cp id=\"response\"\u003e\u003c/p\u003e\n\u003e     \u003c/div\u003e\n\u003e     \u003cscript\u003e\n\u003e       $(document).ready(function() {\n\u003e         $(\"#submit\").click(function() {\n\u003e           var prompt = $(\"#prompt\").val();\n\u003e           if (prompt.length === 0) {\n\u003e             alert(\"Please enter a prompt\");\n\u003e             return;\n\u003e           }\n\u003e           $.post(\"/prompt\", {prompt: prompt}, function(data) {\n\u003e             $(\"#response\").text(data.response);\n\u003e           });\n\u003e         });\n\u003e       });\n\u003e     \u003c/script\u003e\n\u003e   \u003c/body\u003e\n\u003e \u003c/html\u003e\n\u003e ```\n\n\n\n### gitignore\n\n\u003e Me: create a gitignore file for this project\n\u003e\n\u003e ChatGPT:\n\u003e\n\u003e Here's an example of a `.gitignore` file for a Flask project:\n\u003e\n\u003e ```bash\n\u003e # .gitignore\n\u003e \n\u003e # Byte-compiled / optimized / DLL files\n\u003e __pycache__/\n\u003e *.pyc\n\u003e *.pyo\n\u003e *.egg-info/\n\u003e dist/\n\u003e build/\n\u003e \n\u003e # Flask environment\n\u003e instance/\n\u003e .env\n\u003e \n\u003e # venv\n\u003e venv/\n\u003e ```\n\u003e\n\u003e This `.gitignore` file will ignore any byte-compiled Python files (e.g., `*.pyc`), directories generated by Python eggs (e.g., `*.egg-info/`), and directories generated during the build process (e.g., `dist/`, `build/`). It also ignores the Flask `instance` directory, which is typically used to store sensitive information like database credentials. Finally, it ignores the virtual environment directory (e.g., `venv/`).\n\n### OpenAPI key\n\nhttps://platform.openai.com/docs/quickstart/build-your-application\n\nhttps://platform.openai.com/account/api-keys\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/python-data-structure-basics":{"title":"Data Structure Basics in Python","content":"\n# Data Structure Basics in Python\n\n\u003e This article records my practices on python data structures.\n\n\n\n## 0. References\n\nSee: \n\n1. https://realpython.com/python-data-structures/\n2. https://www.geeksforgeeks.org/python-collections-module/#ordereddict\n3. https://docs.python.org/3/library/collections.html#collections.OrderedDict\n\n\n\nThe following code snippets are initially run in jupyter notebook.\n\n```python\n%autosave 10\n```\n\n\n\n    Autosaving every 10 seconds\n\n\n## Part 1: Dictionaries, Maps, HashTables\n\n### dict: Your Go-To Dictionary\n\nBecause dictionaries are so important, Python features a robust dictionary implementation that’s built directly into the core language: the dict data type.\n\nPython also provides some useful syntactic sugar for working with dictionaries in your programs. For example, the curly-brace ({ }) dictionary expression syntax and dictionary comprehensions allow you to conveniently define new dictionary objects:\n\n\n\n\n```python\nphonebook = {'bob':7387, 'alice':3719, 'jack':7052}\nsqures = {x: x*x for x in range(6)}\n\nprint(phonebook['alice'])\nprint(squres)\n```\n\n    3719\n    {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\nNotes:\n\n1. Python dict's keys: hashable objects which have hash values that never change during its life time.\n2. Dictionaries are highly optimized and underlie many parts of the language. For example, class attributes and variables in a stack frame are both stored internally in dictionaries.\n3. Performance: O(1) time complexity for lookup, insert, update, and delete operations in the average case\n4. Hash table implementation\n\nBesides plain *dict* objects, Python’s standard library also includes a number of specialized dictionary implementations. These specialized dictionaries are all *based on the built-in dictionary class* (and share its performance characteristics) but also include some additional convenience features.\n\n### collections.OrderedDict: Remember the Insertion Order of Keys\n\n\n\n```python\nimport collections\n\nd = collections.OrderedDict(one=1, two=2, three=3)\nd\n```\n\n\n\n\n    OrderedDict([('one', 1), ('two', 2), ('three', 3)])\n\n\n\n\n```python\nd['four'] = 4\nd.keys()\n```\n\n\n\n\n    odict_keys(['one', 'two', 'three', 'four'])\n\n\n\n\n```python\n# remove and insert\nprint('Before Deleting')\nfor key, val in d.items():\n    print(key, val)\n\n# delete an element\nd.pop('one')\n\n# re-insert the same\nd['one'] = 1\n\n# remove and insert\nprint('\\nAfter Re-inserting')\nfor key, val in d.items():\n    print(key, val)\n```\n\n    Before Deleting\n    one 1\n    two 2\n    three 3\n    four 4\n    \n    After Re-inserting\n    two 2\n    three 3\n    four 4\n    one 1\n\n\n\n```python\n## popitem(): popitem(last=True)\n\n# pop last\np = d.popitem()\np\n```\n\n\n\n\n    ('one', 1)\n\n\n\n\n```python\n# pop first\np = d.popitem(last=False)\np\n```\n\n\n\n\n    ('two', 2)\n\n\n\n\n```python\n## move_to_end(key, last=True) \n# Move an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist:\n\nd['one'] = 1\nd['two'] = 2\nd\n\n```\n\n\n\n\n    OrderedDict([('three', 3), ('four', 4), ('one', 1), ('two', 2)])\n\n\n\n\n```python\nd.move_to_end('four')\nd\n```\n\n\n\n\n    OrderedDict([('three', 3), ('one', 1), ('two', 2), ('four', 4)])\n\n\n\n\n```python\nd.move_to_end('one', last=False)\nd\n```\n\n\n\n\n    OrderedDict([('one', 1), ('three', 3), ('two', 2), ('four', 4)])\n\n\n\n\n```python\n## reversed()\n# Python reversed() method returns an iterator that accesses the given sequence in the reverse order.\n# https://www.programiz.com/python-programming/methods/built-in/reversed\n\nfor key, val in d.items():\n    print(key, val)\n\nprint('\\nreversed')\nfor key, val in reversed(d.items()):\n    print(key, val)\n```\n\n    one 1\n    three 3\n    two 2\n    four 4\n    \n    reversed\n    four 4\n    two 2\n    three 3\n    one 1\n\n\n### collections.defaultdict: Return Default Values for Missing Keys\n\nThe defaultdict class is another dictionary subclass that accepts a callable in its constructor whose return value will be used if a requested key cannot be found.\n\nThis can save you some typing and make your intentions clearer as compared to using get() or catching a KeyError exception in regular dictionaries:\n\n\n\n```python\nfrom collections import defaultdict\n\n## use list() as default value function\ndd = defaultdict(list)\n\n# Accessing a missing key creates it and initializes it using the default factory, i.e. list() in this example.\ndd['dogs'].append('Rufus')\ndd['dogs'].append('Kathrin')\ndd\n```\n\n\n\n\n    defaultdict(list, {'dogs': ['Rufus', 'Kathrin']})\n\n\n\n\n```python\ndd['dogs']\n```\n\n\n\n\n    ['Rufus', 'Kathrin']\n\n\n\n### collections.ChainMap: Search Multiple Dictionaries as a Single Mapping\n\nThe collections.ChainMap data structure groups multiple dictionaries into a single mapping. Lookups search the underlying mappings one by one until a key is found. Insertions, updates, and deletions only affect the first mapping added to the chain:\n\n\n```python\nfrom collections import ChainMap\n\ndict1 = {'one':1, 'two':2}\ndict2 = {'three':3, 'four':4}\n\nchain = ChainMap(dict1, dict2)\nchain\n```\n\n\n\n\n    ChainMap({'one': 1, 'two': 2}, {'three': 3, 'four': 4})\n\n\n\n\n```python\nprint('\\nchain map')\nfor key, val in chain.items():\n    print(key, val)\n```\n\n\n    chain map\n    three 3\n    four 4\n    one 1\n    two 2\n\n\n\n```python\n# add new\ndict3 = {'five':5, 'six':6}\nchain.new_child(dict3)\nchain\n```\n\n\n\n\n    ChainMap({'one': 1, 'two': 2}, {'three': 3, 'four': 4})\n\n\n\n### types.MappingProxyType: A Wrapper for Making Read-Only Dictionaries\n\nMappingProxyType is a wrapper around a standard dictionary that provides a read-only view into the wrapped dictionary’s data. This class was added in Python 3.3 and can be used to create immutable proxy versions of dictionaries.\n\nMappingProxyType can be helpful if, for example, you’d like to return a dictionary carrying internal state from a class or module while discouraging write access to this object. Using MappingProxyType allows you to put these restrictions in place without first having to create a full copy of the dictionary:\n\n\n```python\nfrom types import MappingProxyType\n\nwritable = {'one':1, 'two':2}\nread_only = MappingProxyType(writable)\n\n# The proxy is read-only:\nread_only['one']\n```\n\n\n\n\n    1\n\n\n\n\n```python\nread_only['one'] = 1\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-28-11d9e8a8c63d\u003e in \u003cmodule\u003e\n    ----\u003e 1 read_only['one'] = 1\n\n\n    TypeError: 'mappingproxy' object does not support item assignment\n\n\n\n```python\n# Updates to the original are reflected in the proxy:\nwritable['one'] = 42\nread_only\n```\n\n\n\n\n    mappingproxy({'one': 42, 'two': 2})\n\n\n\n## Part 2: Array Data Structures\n\nAn array is a fundamental data structure available in most programming languages, and it has a wide range of uses across different algorithms.\n\nIn this section, you’ll take a look at array implementations in Python that use only core language features or functionality that’s included in the Python standard library. You’ll see the strengths and weaknesses of each approach so you can decide which implementation is right for your use case.\n\nBut before we jump in, let’s cover some of the basics first. How do arrays work, and what are they used for? Arrays consist of fixed-size data records that allow each element to be efficiently located based on its index:\n\nVisual representation of an array\nBecause arrays store information in adjoining blocks of memory, they’re considered contiguous data structures (as opposed to linked data structures like linked lists, for example).\n\nA real-world analogy for an array data structure is a parking lot. You can look at the parking lot as a whole and treat it as a single object, but inside the lot there are parking spots indexed by a unique number. Parking spots are containers for vehicles—each parking spot can either be empty or have a car, a motorbike, or some other vehicle parked on it.\n\nBut not all parking lots are the same. Some parking lots may be restricted to only one type of vehicle. For example, a motor home parking lot wouldn’t allow bikes to be parked on it. A restricted parking lot corresponds to a typed array data structure that allows only elements that have the same data type stored in them.\n\nPerformance-wise, it’s very fast to look up an element contained in an array given the element’s index. A proper array implementation guarantees a constant O(1) access time for this case.\n\nPython includes several array-like data structures in its standard library that each have slightly different characteristics. Let’s take a look.\n\n![Array](https://files.realpython.com/media/python-linked-list-array-visualization.5b9f4c4040cb.jpeg)\n\n### list: Mutable Dynamic Arrays\nLists are a part of the core Python language. Despite their name, Python’s lists are implemented as dynamic arrays behind the scenes.\n\nThis means a list allows elements to be added or removed, and the list will automatically adjust the backing store that holds these elements by allocating or releasing memory.\n\nPython lists can hold arbitrary elements—everything is an object in Python, including functions. Therefore, you can mix and match different kinds of data types and store them all in a single list.\n\nThis can be a powerful feature, but the downside is that supporting multiple data types at the same time means that data is generally less tightly packed. As a result, the whole structure takes up more space:\n\n\n```python\narr = ['one', 'two', 'three']\narr[0]\n```\n\n\n\n\n    'one'\n\n\n\n\n```python\n# lists have a nice repr\narr\n```\n\n\n\n\n    ['one', 'two', 'three']\n\n\n\n\n```python\n# lists are mutable\narr[1] = 'hello'\narr \n```\n\n\n\n\n    ['one', 'hello', 'three']\n\n\n\n\n```python\ndel arr[1]\narr\n```\n\n\n\n\n    ['one', 'three']\n\n\n\n\n```python\n# lists can hold arbitrary data types\narr.append(2333)\narr\n```\n\n\n\n\n    ['one', 'three', 2333]\n\n\n\n### tuple: Immutable Containers\n\nJust like lists, tuples are part of the Python core language. Unlike lists, however, Python’s tuple objects are immutable. This means elements can’t be added or removed dynamically—all elements in a tuple must be defined at creation time.\n\nTuples are another data structure that can hold elements of arbitrary data types. Having this flexibility is powerful, but again, it also means that data is less tightly packed than it would be in a typed array:\n\n\n```python\narr = ('one', 'two', 'three')\narr[0]\n```\n\n\n\n\n    'one'\n\n\n\n\n```python\narr\n```\n\n\n\n\n    ('one', 'two', 'three')\n\n\n\n\n```python\n# Tuples are immutable\narr[1] = \"hello\"\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-5-2f27284d12dc\u003e in \u003cmodule\u003e\n          1 # Tuples are immutable\n    ----\u003e 2 arr[1] = \"hello\"\n\n\n    TypeError: 'tuple' object does not support item assignment\n\n\n\n```python\ndel arr[1]\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-6-df001b61e2e0\u003e in \u003cmodule\u003e\n    ----\u003e 1 del arr[1]\n\n\n    TypeError: 'tuple' object doesn't support item deletion\n\n\n\n```python\n# Tuples can hold arbitrary data types:\n# (Adding elements creates a copy of the tuple)\narr + (23,)\n```\n\n\n\n\n    ('one', 'two', 'three', 23)\n\n\n\n### array.array: Basic Typed Arrays\n\nPython’s array module provides space-efficient storage of basic C-style data types like bytes, 32-bit integers, floating-point numbers, and so on.\n\nArrays created with the array.array class are mutable and behave similarly to lists except for one important difference: they’re typed arrays constrained to a single data type.\n\nBecause of this constraint, array.array objects with many elements are more space efficient than lists and tuples. The elements stored in them are tightly packed, and this can be useful if you need to store many elements of the same type.\n\nAlso, arrays support many of the same methods as regular lists, and you might be able to use them as a drop-in replacement without requiring other changes to your application code.\n\n\n```python\nimport array\n\narr = array.array('f', (1.0, 1.5, 2.0, 2.5))\narr[1]\n\n```\n\n\n\n\n    1.5\n\n\n\n\n```python\narr\n```\n\n\n\n\n    array('f', [1.0, 1.5, 2.0, 2.5])\n\n\n\n\n```python\n# Arrays are mutable\narr[1] = 23.0\narr\n```\n\n\n\n\n    array('f', [1.0, 23.0, 2.0, 2.5])\n\n\n\n\n```python\ndel arr[1]\n```\n\n\n```python\narr\n```\n\n\n\n\n    array('f', [1.0, 2.0, 2.5])\n\n\n\n\n```python\narr.append(42.0)\narr\n```\n\n\n\n\n    array('f', [1.0, 2.0, 2.5, 42.0])\n\n\n\n\n```python\n# Arrays are \"typed\":\narr[1] = \"hello\"\n\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-14-d4d93891e8c0\u003e in \u003cmodule\u003e\n          1 # Arrays are \"typed\":\n    ----\u003e 2 arr[1] = \"hello\"\n\n\n    TypeError: must be real number, not str\n\n\n### str: Immutable Arrays of Unicode Characters\n\nPython 3.x uses str objects to store textual data as immutable sequences of Unicode characters. Practically speaking, that means a str is an immutable array of characters. Oddly enough, it’s also a recursive data structure—each character in a string is itself a str object of length 1.\n\nString objects are space efficient because they’re tightly packed and they specialize in a single data type. If you’re storing Unicode text, then you should use a string.\n\nBecause strings are immutable in Python, modifying a string requires creating a modified copy. The closest equivalent to a mutable string is storing individual characters inside a list:\n\n\n```python\narr = \"abcd\"\narr[1]\n```\n\n\n\n\n    'b'\n\n\n\n\n```python\ntype(arr[1])\n```\n\n\n\n\n    str\n\n\n\n\n```python\n# Strings are immutable:\narr[1] = 'e'\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-3-2e0d5fa2424f\u003e in \u003cmodule\u003e\n          1 # Strings are immutable:\n    ----\u003e 2 arr[1] = 'e'\n\n\n    TypeError: 'str' object does not support item assignment\n\n\n\n```python\ndel arr[1]\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-4-df001b61e2e0\u003e in \u003cmodule\u003e\n    ----\u003e 1 del arr[1]\n\n\n    TypeError: 'str' object doesn't support item deletion\n\n\n\n```python\n# Strings can be unpacked into a list to get a mutable representation:\narr = list('abcd')\n','.join(arr)\n```\n\n\n\n\n    'a,b,c,d'\n\n\n\n\n```python\n# Strings are recursive data structures:\narr = \"abcd\"\ntype('arr')\n```\n\n\n\n\n    str\n\n\n\n\n```python\ntype(arr[0])\n```\n\n\n\n\n    str\n\n\n\n### bytes: Immutable Arrays of Single Bytes\n\nbytes objects are immutable sequences of single bytes, or integers in the range 0 ≤ x ≤ 255. Conceptually, bytes objects are similar to str objects, and you can also think of them as immutable arrays of bytes.\n\nLike strings, bytes have their own literal syntax for creating objects and are space efficient. bytes objects are immutable, but unlike strings, there’s a dedicated mutable byte array data type called bytearray that they can be unpacked into:\n\n\n```python\narr = bytes((0, 1, 2, 3))\narr[1]\n```\n\n\n\n\n    1\n\n\n\n\n```python\n# Bytes literals have their own syntax:\narr\n```\n\n\n\n\n    b'\\x00\\x01\\x02\\x03'\n\n\n\n\n```python\narr = b\"\\x00\\x01\\x02\\x03\"\narr\n```\n\n\n\n\n    b'\\x00\\x01\\x02\\x03'\n\n\n\n\n```python\n# Only valid `bytes` are allowed:\nbytes((0, 300))\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    ValueError                                Traceback (most recent call last)\n    \n    \u003cipython-input-13-f644e09a09b0\u003e in \u003cmodule\u003e\n          1 # Only valid `bytes` are allowed:\n    ----\u003e 2 bytes((0, 300))\n\n\n    ValueError: bytes must be in range(0, 256)\n\n\n\n```python\n# Bytes are immutable:\narr[1] = 23\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-15-714056f42f40\u003e in \u003cmodule\u003e\n          1 # Bytes are immutable:\n    ----\u003e 2 arr[1] = 23\n\n\n    TypeError: 'bytes' object does not support item assignment\n\n\n\n```python\ndel arr[1]\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-16-df001b61e2e0\u003e in \u003cmodule\u003e\n    ----\u003e 1 del arr[1]\n\n\n    TypeError: 'bytes' object doesn't support item deletion\n\n\n### bytearray: Mutable Arrays of Single Bytes\n\nThe bytearray type is a mutable sequence of integers in the range 0 ≤ x ≤ 255. The bytearray object is closely related to the bytes object, with the main difference being that a bytearray can be modified freely—you can overwrite elements, remove existing elements, or add new ones. The bytearray object will grow and shrink accordingly.\n\nA bytearray can be converted back into immutable bytes objects, but this involves copying the stored data in full—a slow operation taking O(n) time:\n\n\n```python\n%autosave 5\n```\n\n\n\n    Autosaving every 5 seconds\n\n\n\n```python\narr = bytearray((0,1,2,3,4))\narr[0]\n```\n\n\n\n\n    0\n\n\n\n\n```python\narr\n```\n\n\n\n\n    bytearray(b'\\x00\\x01\\x02\\x03\\x04')\n\n\n\n\n```python\n# Bytearrays are mutable:\narr[1] = 23\narr\n```\n\n\n\n\n    bytearray(b'\\x00\\x17\\x02\\x03\\x04')\n\n\n\n\n```python\ndel arr[1]\n```\n\n\n```python\narr.append(42)\narr\n```\n\n\n\n\n    bytearray(b'\\x00\\x02\\x03\\x04*')\n\n\n\n\n```python\n# Bytearrays can only hold `bytes` (integers in the range 0 \u003c= x \u003c= 255)\narr[1] = 'Hello'\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-7-42a4439943a8\u003e in \u003cmodule\u003e\n          1 # Bytearrays can only hold `bytes` (integers in the range 0 \u003c= x \u003c= 255)\n    ----\u003e 2 arr[1] = 'Hello'\n\n\n    TypeError: 'str' object cannot be interpreted as an integer\n\n\n\n```python\narr[1] = 300\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    ValueError                                Traceback (most recent call last)\n    \n    \u003cipython-input-8-6088e2f482e7\u003e in \u003cmodule\u003e\n    ----\u003e 1 arr[1] = 300\n\n\n    ValueError: byte must be in range(0, 256)\n\n\n\n```python\n# Bytearrays can be converted back into bytes objects:\n# (This will copy the data)\nbytes(arr)\n```\n\n\n\n\n    b'\\x00\\x02\\x03\\x04*'\n\n\n\n## Arrays in Python: Summary\nIf you want to restrict yourself to the array data structures included with Python, then here are a few guidelines:\n\n* If you need to store arbitrary objects, potentially with mixed data types, then use a list or a tuple, depending on whether or not you want an immutable data structure.\n\n* If you have numeric (integer or floating-point) data and tight packing and performance is important, then try out array.array.\n\n* If you have textual data represented as Unicode characters, then use Python’s built-in str. If you need a mutable string-like data structure, then use a list of characters.\n\n* If you want to store a contiguous block of bytes, then use the immutable bytes type or a bytearray if you need a mutable data structure.\n\nIn most cases, I like to start out with a simple list. I’ll only specialize later on if performance or storage space becomes an issue. Most of the time, using a general-purpose array data structure like list gives you the fastest development speed and the most programming convenience.\n\n## Part 3. Records, Structs, and Data Transfer Objects\n\nCompared to arrays, record data structures provide a fixed number of fields. Each field can have a name and may also have a different type.\n\nIn this section, you’ll see how to implement records, structs, and plain old data objects in Python using only built-in data types and classes from the standard library.\n\n\u003e Note: I’m using the definition of a record loosely here. For example, I’m also going to discuss types like Python’s built-in tuple that may or may not be considered records in a strict sense because they don’t provide named fields.\n\nPython offers several data types that you can use to implement records, structs, and data transfer objects. In this section, you’ll get a quick look at each implementation and its unique characteristics. At the end, you’ll find a summary and a decision-making guide that will help you make your own picks.\n\n\n### dict: Simple Data Objects\n\nAs mentioned previously, Python dictionaries store an arbitrary number of objects, each identified by a unique key. Dictionaries are also often called maps or associative arrays and allow for efficient lookup, insertion, and deletion of any object associated with a given key.\n\nUsing dictionaries as a record data type or data object in Python is possible. Dictionaries are easy to create in Python as they have their own syntactic sugar built into the language in the form of dictionary literals. The dictionary syntax is concise and quite convenient to type.\n\nData objects created using dictionaries are mutable, and there’s little protection against misspelled field names as fields can be added and removed freely at any time. Both of these properties can introduce surprising bugs, and there’s always a trade-off to be made between convenience and error resilience:\n\n\n```python\ncar1 = {'color':'red', 'mileage':3824, 'automatic':True}\ncar2 = {'color':'blue', 'mileage':40231, 'automatic':False}\ncar2\n```\n\n\n\n\n    {'color': 'blue', 'mileage': 40231, 'automatic': False}\n\n\n\n### tuple: Immutable Groups of Objects\n\nPython’s tuples are a straightforward data structure for grouping arbitrary objects. Tuples are immutable—they can’t be modified once they’ve been created.\n\nPerformance-wise, tuples take up slightly less memory than lists in CPython, and they’re also faster to construct.\n\nAs you can see in the bytecode disassembly below, constructing a tuple constant takes a single LOAD_CONST opcode, while constructing a list object with the same contents requires several more operations:\n\n\n```python\n# dis — Disassembler for Python bytecode\n# The dis module supports the analysis of CPython bytecode by disassembling it. The CPython bytecode which this module takes as an input is defined in the file Include/opcode.h and used by the compiler and the interpreter.\n\nimport dis\n\ndis.dis(compile(\"(23, 'a', 'b', 'c')\", \"\", \"eval\"))\n```\n\n      1           0 LOAD_CONST               0 ((23, 'a', 'b', 'c'))\n                  2 RETURN_VALUE\n\n\n\n```python\ndis.dis(compile(\"[23, 'a', 'b', 'c']\", \"\", \"eval\"))\n```\n\n      1           0 LOAD_CONST               0 (23)\n                  2 LOAD_CONST               1 ('a')\n                  4 LOAD_CONST               2 ('b')\n                  6 LOAD_CONST               3 ('c')\n                  8 BUILD_LIST               4\n                 10 RETURN_VALUE\n\n\nHowever, you shouldn’t place too much emphasis on these differences. In practice, the performance difference will often be negligible, and trying to squeeze extra performance out of a program by switching from lists to tuples will likely be the wrong approach.\n\nA potential downside of plain tuples is that the data you store in them can only be pulled out by accessing it through integer indexes. You can’t give names to individual properties stored in a tuple. This can impact code readability.\n\nAlso, a tuple is always an ad-hoc structure: it’s difficult to ensure that two tuples have the same number of fields and the same properties stored in them.\n\nThis makes it easy to introduce slip-of-the-mind bugs, such as mixing up the field order. Therefore, I would recommend that you keep the number of fields stored in a tuple as low as possible:\n\n\n```python\ncar1 = (\"red\", 3812.4, True)\ncar2 = (\"blue\", 40231.0, False)\ncar1\n```\n\n\n\n\n    ('red', 3812.4, True)\n\n\n\n### Write a Custom Class: More Work, More Control\n\nClasses allow you to define reusable blueprints for data objects to ensure each object provides the same set of fields.\n\nUsing regular Python classes as record data types is feasible, but it also takes manual work to get the convenience features of other implementations. For example, adding new fields to the __init__ constructor is verbose and takes time.\n\nAlso, the default string representation for objects instantiated from custom classes isn’t very helpful. To fix that, you may have to add your own __repr__ method, which again is usually quite verbose and must be updated each time you add a new field.\n\nFields stored on classes are mutable, and new fields can be added freely, which you may or may not like. It’s possible to provide more access control and to create read-only fields using the @property decorator, but once again, this requires writing more glue code.\n\nWriting a custom class is a great option whenever you’d like to add business logic and behavior to your record objects using methods. However, this means that these objects are technically no longer plain data objects:\n\n\n```python\nclass Car:\n    def __init__(self, color, mileage, automatic):\n        self.color = color\n        self.mileage = mileage\n        self.automatic = automatic\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\ncar2\n```\n\n\n\n\n    \u003c__main__.Car at 0x7ff72e066ca0\u003e\n\n\n\n\n```python\n# Get the mileage:\ncar2.mileage\n```\n\n\n\n\n    40231.4\n\n\n\n\n```python\n# Classes are mutable:\ncar2.mileage = 12\ncar2.windshield = 'broken'\n```\n\n### dataclasses.dataclass: Python 3.7+ Data Classes\n\nData classes are available in Python 3.7 and above. They provide an excellent alternative to defining your own data storage classes from scratch.\n\nBy writing a data class instead of a plain Python class, your object instances get a few useful features out of the box that will save you some typing and manual implementation work:\n\nThe syntax for defining instance variables is shorter, since you don’t need to implement the .__init__() method.\nInstances of your data class automatically get nice-looking string representation via an auto-generated .__repr__() method.\nInstance variables accept type annotations, making your data class self-documenting to a degree. Keep in mind that type annotations are just hints that are not enforced without a separate type-checking tool.\nData classes are typically created using the @dataclass decorator, as you’ll see in the code example below:\n\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Car:\n    color: str\n    mileage: float\n    automatic: bool\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\n# Instances have a nice repr:\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=40231.4, automatic=False)\n\n\n\n\n```python\n# Accessing fields:\ncar1.mileage\n```\n\n\n\n\n    3812.4\n\n\n\n\n```python\n# Fields are mutable:\ncar2.mileage = 12\ncar2.windshield = 'broken'\n\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=12, automatic=False)\n\n\n\n\n```python\n# Type annotations are not enforced without a separate type checking tool like mypy\nCar(\"red\", \"NOT_A_FLOAT\", 99)\n```\n\n\n\n\n    Car(color='red', mileage='NOT_A_FLOAT', automatic=99)\n\n\n\n### collections.namedtuple: Convenient Data Objects\n\nThe namedtuple class available in Python 2.6+ provides an extension of the built-in tuple data type. Similar to defining a custom class, using namedtuple allows you to define reusable blueprints for your records that ensure the correct field names are used.\n\nnamedtuple objects are immutable, just like regular tuples. This means you can’t add new fields or modify existing fields after the namedtuple instance is created.\n\nBesides that, namedtuple objects are, well . . . named tuples. Each object stored in them can be accessed through a unique identifier. This frees you from having to remember integer indexes or resort to workarounds like defining integer constants as mnemonics for your indexes.\n\nnamedtuple objects are implemented as regular Python classes internally. When it comes to memory usage, they’re also better than regular classes and just as memory efficient as regular tuples:\n\n\n```python\nfrom collections import namedtuple\nfrom sys import getsizeof\n\np1 = namedtuple('Point', 'x y z')(1, 2, 3)\np2 = (1, 2, 3)\n\np1\n```\n\n\n\n\n    Point(x=1, y=2, z=3)\n\n\n\n\n```python\np1.x, p1.y, p1.z\n```\n\n\n\n\n    (1, 2, 3)\n\n\n\n\n```python\ngetsizeof(p1)\n```\n\n\n\n\n    64\n\n\n\n\n```python\ngetsizeof(p2)\n```\n\n\n\n\n    64\n\n\n\nnamedtuple objects can be an **easy way to clean up your code and make it more readable** by enforcing a better structure for your data.\n\nI find that going from ad-hoc data types like dictionaries with a fixed format to namedtuple objects helps me to express the intent of my code more clearly. Often when I apply this refactoring, I magically come up with a better solution for the problem I’m facing.\n\nUsing namedtuple objects over regular (unstructured) tuples and dicts can also make your coworkers’ lives easier by making the data that’s being passed around self-documenting, at least to a degree:\n\n\n```python\nCar = namedtuple('Car', 'color mileage automatic')\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=40231.4, automatic=False)\n\n\n\n\n```python\ntype(car1)\n```\n\n\n\n\n    __main__.Car\n\n\n\n\n```python\n# Fields are immtuable:\ncar2.mileage = 12\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-39-f7f9c602e34d\u003e in \u003cmodule\u003e\n          1 # Fields are immtuable:\n    ----\u003e 2 car2.mileage = 12\n\n\n    AttributeError: can't set attribute\n\n\n\n```python\ncar2.windshield = \"broken\"\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-40-72bb797ed6af\u003e in \u003cmodule\u003e\n    ----\u003e 1 car2.windshield = \"broken\"\n\n\n    AttributeError: 'Car' object has no attribute 'windshield'\n\n\n### typing.NamedTuple: Improved Namedtuples\n\nAdded in Python 3.6, typing.NamedTuple is the younger sibling of the namedtuple class in the collections module. It’s very similar to namedtuple, with the main difference being an updated syntax for defining new record types and added support for type hints.\n\nPlease note that type annotations are not enforced without a separate type-checking tool like mypy. But even without tool support, they can provide useful hints for other programmers (or be terribly confusing if the type hints become out of date):\n\n\n```python\nfrom typing import NamedTuple\n\nclass Car(NamedTuple):\n    color: str\n    mileage: float\n    automatic: bool\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=40231.4, automatic=False)\n\n\n\n\n```python\n# Fields are immutable:\ncar1.mileage = 12\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-42-144528f7cc7a\u003e in \u003cmodule\u003e\n          1 # Fields are immutable:\n    ----\u003e 2 car1.mileage = 12\n\n\n    AttributeError: can't set attribute\n\n\n### struct.Struct: Serialized C Structs\n\nThe struct.Struct class converts between Python values and C structs serialized into Python bytes objects. For example, it can be used to handle binary data stored in files or coming in from network connections.\n\nStructs are defined using a mini language based on format strings that allows you to define the arrangement of various C data types like char, int, and long as well as their unsigned variants.\n\nSerialized structs are seldom used to represent data objects meant to be handled purely inside Python code. They’re intended primarily as a **data exchange format** rather than as a way of holding data in memory that’s only used by Python code.\n\nIn some cases, packing primitive data into structs may use less memory than keeping it in other data types. However, in most cases that would be quite an advanced (and probably unnecessary) optimization:\n\n\n```python\nfrom struct import Struct\n\nMyStruct = Struct('i?f')\ndata = MyStruct.pack(23, False, 42.0)\n\n# All you get is a blob of data:\ndata\n```\n\n\n\n\n    b'\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00(B'\n\n\n\n\n```python\nMyStruct.unpack(data)\n```\n\n\n\n\n    (23, False, 42.0)\n\n\n\n### types.SimpleNamespace: Fancy Attribute Access\n\nHere’s one more slightly obscure choice for implementing data objects in Python: types.SimpleNamespace. This class was added in Python 3.3 and provides attribute access to its namespace.\n\nThis means SimpleNamespace instances expose all of their keys as class attributes. You can use obj.key dotted attribute access instead of the obj['key'] square-bracket indexing syntax that’s used by regular dicts. All instances also include a meaningful __repr__ by default.\n\nAs its name proclaims, SimpleNamespace is simple! It’s basically a dictionary that allows attribute access and prints nicely. Attributes can be added, modified, and deleted freely:\n\n\n```python\nfrom types import SimpleNamespace\n\ncar1 = SimpleNamespace(color=\"red\", mileage=3812.4, automatic=True)\ncar1\n```\n\n\n\n\n    namespace(color='red', mileage=3812.4, automatic=True)\n\n\n\n\n```python\n# Instances support attribute access and are mutable:\ncar1.mileage = 12\ncar1.windshield = \"broken\"\ndel car1.automatic\n\ncar1\n```\n\n\n\n\n    namespace(color='red', mileage=12, windshield='broken')\n\n\n\n## Records, Structs, and Data Objects in Python: Summary\n\nAs you’ve seen, there’s quite a number of different options for implementing records or data objects. Which type should you use for data objects in Python? Generally your decision will depend on your use case:\n\n* If you have only a few fields, then using a plain tuple object may be okay if the field order is easy to remember or field names are superfluous. For example, think of an (x, y, z) point in three-dimensional space.\n\n* If you need immutable fields, then plain tuples, collections.namedtuple, and typing.NamedTuple are all good options.\n\n* If you need to lock down field names to avoid typos, then collections.namedtuple and typing.NamedTuple are your friends.\n\n* If you want to keep things simple, then a plain dictionary object might be a good choice due to the convenient syntax that closely resembles JSON.\n\n* If you need full control over your data structure, then it’s time to write a custom class with @property setters and getters.\n\n* If you need to add behavior (methods) to the object, then you should write a custom class, either from scratch, or using the dataclass decorator, or by extending collections.namedtuple or typing.NamedTuple.\n\n* If you need to pack data tightly to serialize it to disk or to send it over the network, then it’s time to read up on struct.Struct because this is a great use case for it!\n\nIf you’re looking for a safe default choice, then my general recommendation for implementing a plain record, struct, or data object in Python would be to use collections.namedtuple in Python 2.x and its younger sibling, typing.NamedTuple in Python 3.\n\n## Part 4. Sets and Multisets\n\nIn this section, you’ll see how to implement mutable and immutable set and multiset (bag) data structures in Python using built-in data types and classes from the standard library.\n\nA set is an unordered collection of objects that doesn’t allow duplicate elements. Typically, sets are used to quickly test a value for membership in the set, to insert or delete new values from a set, and to compute the union or intersection of two sets.\n\nIn a proper set implementation, membership tests are expected to run in fast **O(1)** time. Union, intersection, difference, and subset operations should take O(n) time on average. The set implementations included in Python’s standard library follow these performance characteristics.\n\nJust like dictionaries, sets get special treatment in Python and have some syntactic sugar that makes them easy to create. For example, the curly-brace set expression syntax and set comprehensions allow you to conveniently define new set instances:\n\n\n```python\nvowels = {'a', 'e', 'i', 'o', 'u'}\nsquares = {x * x for x in range(10)}\n\nprint(vowels)\nprint(squares)\n```\n\n    {'e', 'a', 'i', 'o', 'u'}\n    {0, 1, 64, 4, 36, 9, 16, 49, 81, 25}\n\n\nBut be careful: To create an empty set you’ll need to call the set() constructor. Using empty curly-braces ({}) is ambiguous and will create an empty dictionary instead.\n\n\n```python\n# null set using set() instead of {}\n\ndict_empty = {}\nset_empty = set()\n\nprint(type(dict_empty))\nprint(type(set_empty))\n```\n\n    \u003cclass 'dict'\u003e\n    \u003cclass 'set'\u003e\n\n\n### set: Your Go-To Set\n\nThe set type is the built-in set implementation in Python. It’s mutable and allows for the dynamic insertion and deletion of elements.\n\nPython’s sets are backed by the dict data type and share the same performance characteristics. Any hashable object can be stored in a set:\n\n\n```python\nvowels = {'a', 'e', 'i', 'o', 'u'}\n\n'e' in vowels\n```\n\n\n\n\n    True\n\n\n\n\n```python\nletters = set('alice')\nletters.intersection(vowels)\n```\n\n\n\n\n    {'a', 'e', 'i'}\n\n\n\n\n```python\nvowels.difference(letters)\n```\n\n\n\n\n    {'o', 'u'}\n\n\n\n\n```python\nletters_copy = letters.copy()\nletters_copy\n```\n\n\n\n\n    {'a', 'c', 'e', 'i', 'l'}\n\n\n\n\n```python\n# The isdisjoint() method returns True if two sets are disjoint sets. If not, it returns False\nletters_copy.isdisjoint(vowels)\n```\n\n\n\n\n    False\n\n\n\n\n```python\n![disjoint](https://cdn.programiz.com/sites/tutorial2program/files/python-disjoint-sets_0.png)\n```\n\n    zsh:1: unknown file attribute: h\n\n\n\n```python\nletters_copy.issubset(letters)\n```\n\n\n\n\n    True\n\n\n\n\n```python\nletters_copy.issuperset(letters)\n```\n\n\n\n\n    True\n\n\n\n\n```python\n# add elements\nvowels.add('x')\nvowels\n```\n\n\n\n\n    {'a', 'e', 'i', 'o', 'u', 'x'}\n\n\n\n\n```python\nvowels.union(letters)\n```\n\n\n\n\n    {'a', 'c', 'e', 'i', 'l', 'o', 'u', 'x'}\n\n\n\n\n```python\nlen(vowels)\n```\n\n\n\n\n    6\n\n\n\n\n```python\nvowels.discard('x')\nvowels\n```\n\n\n\n\n    {'a', 'e', 'i', 'o', 'u'}\n\n\n\n### frozenset: Immutable Sets\n\nThe frozenset class implements an immutable version of set that can’t be changed after it’s been constructed.\n\nfrozenset objects are static and allow only query operations on their elements, not inserts or deletions. Because frozenset objects are static and hashable, they can be used as dictionary keys or as elements of another set, something that isn’t possible with regular (mutable) set objects:\n\n\n```python\nfs = frozenset({'a', 'e', 'i', 'o', 'u'})\nfs\n```\n\n\n\n\n    frozenset({'a', 'e', 'i', 'o', 'u'})\n\n\n\n\n```python\nfs.add('x')\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-67-cb5eaf111fad\u003e in \u003cmodule\u003e\n    ----\u003e 1 fs.add('x')\n\n\n    AttributeError: 'frozenset' object has no attribute 'add'\n\n\n\n```python\n# Frozensets are hashable and can be used as dictionary keys:\nd = {frozenset({'a', 'e', 'i', 'o', 'u'}):'vowels'}\nd\n```\n\n\n\n\n    {frozenset({'a', 'e', 'i', 'o', 'u'}): 'vowels'}\n\n\n\n### collections.Counter: Multisets\n\nThe collections.Counter class in the Python standard library implements a multiset, or bag, type that allows elements in the set to have more than one occurrence.\n\nThis is useful if you need to keep track of not only if an element is part of a set, but also how many times it’s included in the set:\n\n\n\n```python\nfrom collections import Counter\n\ninventory = Counter()\nloot = {'sward': 1, 'bread': 3}\ninventory.update(loot)\ninventory\n```\n\n\n\n\n    Counter({'sward': 1, 'bread': 3})\n\n\n\n\n```python\nmore_loot = {'sward': 2, 'apple': 3}\ninventory.update(more_loot)\ninventory\n```\n\n\n\n\n    Counter({'sward': 3, 'bread': 3, 'apple': 3})\n\n\n\nOne caveat for the Counter class is that you’ll want to be careful when counting the number of elements in a Counter object. Calling len() returns the number of unique elements in the multiset, whereas the total number of elements can be retrieved using sum():\n\n\n\n```python\nlen(inventory)\n```\n\n\n\n\n    3\n\n\n\n\n```python\nsum(inventory.values())\n```\n\n\n\n\n    9\n\n\n\n### Sets and Multisets in Python: Summary\n\nSets are another useful and commonly used data structure included with Python and its standard library. Here are a few guidelines for deciding which one to use:\n\n* If you need a mutable set, then use the built-in set type.\n* If you need hashable objects that can be used as dictionary or set keys, then use a frozenset.\n* If you need a multiset, or bag, data structure, then use collections.Counter.\n\n\n## Part 5. Stacks (LIFOs)\n\nA stack is a collection of objects that supports fast **Last-In/First-Out (LIFO)** semantics for inserts and deletes. Unlike lists or arrays, stacks typically don’t allow for random access to the objects they contain. The insert and delete operations are also often called **push and pop**.\n\nA useful real-world analogy for a stack data structure is a stack of plates. New plates are added to the top of the stack, and because the plates are precious and heavy, only the topmost plate can be moved. In other words, the last plate on the stack must be the first one removed (LIFO). To reach the plates that are lower down in the stack, the topmost plates must be removed one by one.\n\nPerformance-wise, a proper stack implementation is expected to take O(1) time for insert and delete operations.\n\nStacks have a wide range of uses in algorithms. For example, they’re used in language parsing as well as runtime memory management, which relies on a call stack. A short and beautiful algorithm using a stack is depth-first search (DFS) on a tree or graph data structure.\n\nPython ships with several stack implementations that each have slightly different characteristics. Let’s take a look at them and compare their characteristics.\n\n\n### list: Simple, Built-In Stacks\n\nPython’s **built-in list** type makes a decent stack data structure as **it supports push and pop operations in amortized O(1) time**.\n\nPython’s lists are implemented as dynamic arrays internally, which means they occasionally need to resize the storage space for elements stored in them when elements are added or removed. The list over-allocates its backing storage so that not every push or pop requires resizing. As a result, you get an amortized O(1) time complexity for these operations.\n\nThe downside is that this makes their performance less consistent than the stable O(1) inserts and deletes provided by a linked list–based implementation (as you’ll see below with collections.deque). On the other hand, lists do provide fast O(1) time random access to elements on the stack, and this can be an added benefit.\n\nThere’s an important performance caveat that you should be aware of when using lists as stacks: To get the amortized O(1) performance for inserts and deletes, new items must be added to the end of the list with the append() method and removed again from the end using pop(). For optimum performance, stacks based on Python lists should grow towards higher indexes and shrink towards lower ones.\n\nAdding and removing from the front is much slower and takes O(n) time, as the existing elements must be shifted around to make room for the new element. This is a performance antipattern that you should avoid as much as possible:\n\n\n```python\ns = []\ns.append('eat')\ns.append('sleep')\ns.append('code')\n\ns\n```\n\n\n\n\n    ['eat', 'sleep', 'code']\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'code'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\ns.pop()\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    IndexError                                Traceback (most recent call last)\n    \n    \u003cipython-input-79-c88c8c48122b\u003e in \u003cmodule\u003e\n    ----\u003e 1 s.pop()\n\n\n    IndexError: pop from empty list\n\n\n### collections.deque: Fast and Robust Stacks\n\nThe deque class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks.\n\nPython’s deque objects are implemented as **doubly-linked lists**, which gives them excellent and consistent performance for inserting and deleting elements but poor O(n) performance for randomly accessing elements in the middle of a stack.\n\nOverall, collections.deque is a great choice if you’re looking for a stack data structure in Python’s standard library that has the performance characteristics of a linked-list implementation:\n\n\n```python\nfrom collections import deque\n\ns = deque()\ns.append('eat')\ns.append('sleep')\ns.append('code')\n\ns\n```\n\n\n\n\n    deque(['eat', 'sleep', 'code'])\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'code'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\ns.pop()\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    IndexError                                Traceback (most recent call last)\n    \n    \u003cipython-input-84-c88c8c48122b\u003e in \u003cmodule\u003e\n    ----\u003e 1 s.pop()\n\n\n    IndexError: pop from an empty deque\n\n\n\n```python\ns.appendleft('a')\ns.appendleft('b')\ns.appendleft('c')\n\ns\n```\n\n\n\n\n    deque(['c', 'b', 'a'])\n\n\n\n\n```python\ns.popleft()\n```\n\n\n\n\n    'c'\n\n\n\n\n```python\ns.popleft()\n```\n\n\n\n\n    'b'\n\n\n\n\n```python\ns.popleft()\n```\n\n\n\n\n    'a'\n\n\n\n\n```python\ns.appendleft('a')\ns.appendleft('b')\ns.appendleft('c')\n\ns\n```\n\n\n\n\n    deque(['c', 'b', 'a'])\n\n\n\n\n```python\ns.reverse()\n```\n\n\n```python\ns\n```\n\n\n\n\n    deque(['a', 'b', 'c'])\n\n\n\n### queue.LifoQueue: Locking Semantics for Parallel Computing\n\nThe LifoQueue stack implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers.\n\nBesides LifoQueue, the queue module contains several other classes that implement multi-producer, multi-consumer queues that are useful for parallel computing.\n\nDepending on your use case, the locking semantics might be helpful, or they might just incur unneeded overhead. In this case, you’d be better off using a list or a deque as a general-purpose stack:\n\n\n```python\nfrom queue import LifoQueue\n\ns = LifoQueue()\ns.put('eat')\ns.put('sleep')\ns.put('code')\n\ns\n```\n\n\n\n\n    \u003cqueue.LifoQueue at 0x7ff72efbcf70\u003e\n\n\n\n\n```python\ns.get()\n```\n\n\n\n\n    'code'\n\n\n\n\n```python\ns.get()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\ns.get()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\ns.get_nowait()\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    Empty                                     Traceback (most recent call last)\n    \n    \u003cipython-input-96-0bd7ad76be38\u003e in \u003cmodule\u003e\n    ----\u003e 1 s.get_nowait()\n\n\n    ~/opt/anaconda3/lib/python3.8/queue.py in get_nowait(self)\n        196         raise the Empty exception.\n        197         '''\n    --\u003e 198         return self.get(block=False)\n        199 \n        200     # Override these methods to implement other queue organizations\n\n\n    ~/opt/anaconda3/lib/python3.8/queue.py in get(self, block, timeout)\n        165             if not block:\n        166                 if not self._qsize():\n    --\u003e 167                     raise Empty\n        168             elif timeout is None:\n        169                 while not self._qsize():\n\n\n    Empty: \n\n\n## Stack Implementations in Python: Summary\nAs you’ve seen, Python ships with several implementations for a stack data structure. All of them have slightly different characteristics as well as performance and usage trade-offs.\n\nIf you’re not looking for parallel processing support (or if you don’t want to handle locking and unlocking manually), then your choice comes down to the built-in list type or collections.deque. The difference lies in the data structure used behind the scenes and overall ease of use.\n\nlist is backed by a dynamic array, which makes it great for fast random access but requires occasional resizing when elements are added or removed.\n\nThe list over-allocates its backing storage so that not every push or pop requires resizing, and you get an amortized O(1) time complexity for these operations. But you do need to be careful to only insert and remove items using append() and pop(). Otherwise, performance slows down to O(n).\n\ncollections.deque is backed by a doubly-linked list, which optimizes appends and deletes at both ends and provides consistent O(1) performance for these operations. Not only is its performance more stable, the deque class is also easier to use because you don’t have to worry about adding or removing items from the wrong end.\n\nIn summary, collections.deque is an excellent choice for implementing a stack (LIFO queue) in Python.\n\n## Part 6. Queues (FIFOs)\n\nIn this section, you’ll see how to implement a **First-In/First-Out (FIFO)** queue data structure using only built-in data types and classes from the Python standard library.\n\nA queue is a collection of objects that supports fast FIFO semantics for inserts and deletes. The insert and delete operations are sometimes called enqueue and dequeue. Unlike lists or arrays, queues typically don’t allow for random access to the objects they contain.\n\nHere’s a real-world analogy for a FIFO queue:\n\nImagine a line of Pythonistas waiting to pick up their conference badges on day one of PyCon registration. As new people enter the conference venue and queue up to receive their badges, they join the line (enqueue) at the back of the queue. Developers receive their badges and conference swag bags and then exit the line (dequeue) at the front of the queue.\n\nAnother way to memorize the characteristics of a queue data structure is to think of it as a pipe. You add ping-pong balls to one end, and they travel to the other end, where you remove them. While the balls are in the queue (a solid metal pipe) you can’t get at them. The only way to interact with the balls in the queue is to add new ones at the back of the pipe (enqueue) or to remove them at the front (dequeue).\n\nQueues are similar to stacks. The difference between them lies in how items are removed. With a queue, you remove the item least recently added (FIFO) but with a stack, you remove the item most recently added (LIFO).\n\nPerformance-wise, a proper queue implementation is expected to take O(1) time for insert and delete operations. These are the two main operations performed on a queue, and in a correct implementation, they should be fast.\n\nQueues have a wide range of applications in algorithms and often help solve scheduling and parallel programming problems. A short and beautiful algorithm using a queue is breadth-first search (BFS) on a tree or graph data structure.\n\nScheduling algorithms often use priority queues internally. These are specialized queues. Instead of retrieving the next element by insertion time, a priority queue retrieves the highest-priority element. The priority of individual elements is decided by the queue based on the ordering applied to their keys.\n\nA regular queue, however, won’t reorder the items it carries. Just like in the pipe example, you get out what you put in, and in exactly that order.\n\nPython ships with several queue implementations that each have slightly different characteristics. Let’s review them.\n\n### list: Terribly Sloooow Queues\n\nIt’s possible to use a regular list as a queue, but this is not ideal from a performance perspective. **Lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all the other elements by one, requiring O(n) time**.\n\nTherefore, I would **not recommend** using a list as a makeshift queue in Python unless you’re dealing with only a small number of elements:\n\n\n```python\nq = []\nq.append('eat')\nq.append('sleep')\nq.append('code')\nq\n```\n\n\n\n\n    ['eat', 'sleep', 'code']\n\n\n\n\n```python\nq.pop(0)\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.pop(0)\n```\n\n\n\n\n    'sleep'\n\n\n\n### collections.deque: Fast and Robust Queues\n\nThe deque class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks.\n\nPython’s deque objects are implemented as doubly-linked lists. This gives them excellent and consistent performance for inserting and deleting elements, but poor O(n) performance for randomly accessing elements in the middle of the stack.\n\nAs a result, collections.deque is a great default choice if you’re looking for a queue data structure in Python’s standard library:\n\n\n```python\nfrom collections import deque\n\nq = deque()\nq.append('eat')\nq.append('sleep')\nq.append('code')\nq\n\n```\n\n\n\n\n    deque(['eat', 'sleep', 'code'])\n\n\n\n\n```python\nq.popleft()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.popleft()\n```\n\n\n\n\n    'sleep'\n\n\n\n### queue.Queue: Locking Semantics for Parallel Computing\n\nThe queue.Queue implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers.\n\nThe queue module contains several other classes implementing multi-producer, multi-consumer queues that are useful for parallel computing.\n\nDepending on your use case, the locking semantics might be helpful or just incur unneeded overhead. In this case, you’d be better off using collections.deque as a general-purpose queue:\n\n\n```python\nfrom queue import Queue\n\nq = Queue()\nq.put('eat')\nq.put('sleep')\nq.put('code')\nq\n```\n\n\n\n\n    \u003cqueue.Queue at 0x7ff72f279130\u003e\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\nq.get_nowait()\n```\n\n\n\n\n    'code'\n\n\n\n### multiprocessing.Queue: Shared Job Queues\n\nmultiprocessing.Queue is a shared job queue implementation that allows queued items to be processed in parallel by multiple concurrent workers. Process-based parallelization is popular in CPython due to the global interpreter lock (GIL) that prevents some forms of parallel execution on a single interpreter process.\n\nAs a specialized queue implementation meant for sharing data between processes, multiprocessing.Queue makes it easy to distribute work across multiple processes in order to work around the GIL limitations. This type of queue can store and transfer any pickleable object across process boundaries:\n\n\n\n\n```python\nfrom multiprocessing import Queue\n```\n\n\n```python\nq = Queue()\nq.put('eat')\nq.put('sleep')\nq.put('code')\nq\n```\n\n\n\n\n    \u003cmultiprocessing.queues.Queue at 0x7ff72efbe8b0\u003e\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'sleep'\n\n\n\n## Queues in Python: Summary\n\nPython includes several queue implementations as part of the core language and its standard library.\n\nlist objects can be used as queues, but this is generally not recommended due to slow performance.\n\nIf you’re not looking for parallel processing support, then the implementation offered by **collections.deque** is an excellent default choice for implementing a FIFO queue data structure in Python. It provides the performance characteristics you’d expect from a good queue implementation and can also be used as a stack (LIFO queue).\n\n## Part 7. Priority Queues\n\nA priority queue is a container data structure that manages a set of records with totally-ordered keys to provide quick access to the record with the smallest or largest key in the set.\n\nYou can think of a priority queue as a modified queue. Instead of retrieving the next element by insertion time, it retrieves the highest-priority element. The priority of individual elements is decided by the order applied to their keys.\n\nPriority queues are commonly used for dealing with scheduling problems. For example, you might use them to give precedence to tasks with higher urgency.\n\nThink about the job of an operating system task scheduler:\n\nIdeally, higher-priority tasks on the system (such as playing a real-time game) should take precedence over lower-priority tasks (such as downloading updates in the background). By organizing pending tasks in a priority queue that uses task urgency as the key, the task scheduler can quickly select the highest-priority tasks and allow them to run first.\n\nIn this section, you’ll see a few options for how you can implement priority queues in Python using built-in data structures or data structures included in Python’s standard library. Each implementation will have its own upsides and downsides, but in my mind there’s a clear winner for most common scenarios. Let’s find out which one it is.\n\n### list: Manually Sorted Queues\n\nYou can use a sorted list to quickly identify and delete the smallest or largest element. The downside is that inserting new elements into a list is a slow O(n) operation.\n\nWhile the insertion point can be found in O(log n) time using bisect.insort in the standard library, this is always dominated by the slow insertion step.\n\nMaintaining the order by appending to the list and re-sorting also takes at least O(n log n) time. Another downside is that you must manually take care of re-sorting the list when new elements are inserted. It’s easy to introduce bugs by missing this step, and the burden is always on you, the developer.\n\nThis means sorted lists are only suitable as priority queues when there will be few insertions:\n\n\n```python\nq = []\nq.append((2, 'code'))\nq.append((1, 'eat'))\nq.append((3, 'sleep'))\n\n# Remember to re-sort every time a new element is inserted, or use bisect.insort()\nq.sort(reverse=True)\nq\n```\n\n\n\n\n    [(3, 'sleep'), (2, 'code'), (1, 'eat')]\n\n\n\n\n```python\nwhile q:\n    next_item = q.pop()\n    print(next_item)\n```\n\n    (1, 'eat')\n    (2, 'code')\n    (3, 'sleep')\n\n\n### heapq: List-Based Binary Heaps\n\nheapq is a binary heap implementation usually backed by a plain list, and it supports insertion and extraction of the smallest element in O(log n) time.\n\nThis module is a good choice for implementing priority queues in Python. Since heapq technically provides **only a min-heap implementation**, extra steps must be taken to ensure sort stability and other features typically expected from a practical priority queue:\n\n\n```python\nimport heapq\n\nq = []\nheapq.heappush(q, (2, \"code\"))\nheapq.heappush(q, (1, \"eat\"))\nheapq.heappush(q, (3, \"sleep\"))\nq\n```\n\n\n\n\n    [(1, 'eat'), (2, 'code'), (3, 'sleep')]\n\n\n\n\n```python\nwhile q:\n    next_item = heapq.heappop(q)\n    print(next_item)\n```\n\n    (1, 'eat')\n    (2, 'code')\n    (3, 'sleep')\n\n\n\n```python\nq = []\nheapq.heappush(q, (2, \"code\"))\nheapq.heappush(q, (1, \"eat\"))\nheapq.heappush(q, (3, \"sleep\"))\nheapq.heappush(q, (7, \"run\"))\nheapq.heappush(q, (6, \"drive\"))\nheapq.heappush(q, (4, \"swim\"))\nheapq.heappush(q, (5, \"idle\"))\nq\n\n```\n\n\n\n\n    [(1, 'eat'),\n     (2, 'code'),\n     (3, 'sleep'),\n     (7, 'run'),\n     (6, 'drive'),\n     (4, 'swim'),\n     (5, 'idle')]\n\n\n\n\n```python\nheapq.nlargest(3, q)\n```\n\n\n\n\n    [(7, 'run'), (6, 'drive'), (5, 'idle')]\n\n\n\n\n```python\nheapq.nsmallest(3, q)\n```\n\n\n\n\n    [(1, 'eat'), (2, 'code'), (3, 'sleep')]\n\n\n\n### queue.PriorityQueue: Beautiful Priority Queues\n\nqueue.PriorityQueue uses heapq internally and shares the same time and space complexities. The difference is that PriorityQueue is synchronized and provides locking semantics to support multiple concurrent producers and consumers.\n\nDepending on your use case, this might be helpful, or it might just slow your program down slightly. In any case, you might prefer the class-based interface provided by PriorityQueue over the function-based interface provided by heapq:\n\n\n```python\nfrom queue import PriorityQueue\n\nq = PriorityQueue()\nq.put((2, \"code\"))\nq.put((1, \"eat\"))\nq.put((3, \"sleep\"))\nq\n```\n\n\n\n\n    \u003cqueue.PriorityQueue at 0x7ff72e431400\u003e\n\n\n\n\n```python\nwhile not q.empty():\n    next_item = q.get()\n    print(next_item)\n```\n\n    (1, 'eat')\n    (2, 'code')\n    (3, 'sleep')\n\n\n## Priority Queues in Python: Summary\n\nPython includes several priority queue implementations ready for you to use.\n\nqueue.PriorityQueue stands out from the pack with a nice object-oriented interface and a name that clearly states its intent. It should be your preferred choice.\n\nIf you’d like to avoid the locking overhead of queue.PriorityQueue, then using the heapq module directly is also a good option.\n\n\n\n# Conclusion: Python Data Structures\n\nThat concludes your tour of common data structures in Python. With the knowledge you’ve gained here, you’re ready to implement efficient data structures that are just right for your specific algorithm or use case.\n\nIn this tutorial, you’ve learned:\n\nWhich common abstract data types are built into the Python standard library\nHow the most common abstract data types map to Python’s naming scheme\nHow to put abstract data types to practical use in various algorithms\n\n\n```python\n\n```\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/python-programming":{"title":"Python Programming","content":"\n# Python Programming\n\n## Data Structure\n\n* [Python Data Structure Basics](notes/python-data-structure-basics.md)\n\n## Flask\n\n* [Hands on Flask](notes/hands-on-flask.md)\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/web-development":{"title":"Web Development","content":"\n# Web Development\n\n## HTML \u0026 CSS\n\n* [Hands on HTML\u0026CSS](notes/hands-on-html-css.md)\n\n  \n\n## JavaScript\n\n* [Hands on JavaScript](notes/hands-on-javascript.md)\n\n  \n\n## Flask\n\n* [Hands on Flask](notes/hands-on-flask.md)\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/oss_imgs":{"title":"","content":"# OSS IMGS\n\n![GPT的能力](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230510134612561.png) \n\n\n\n![各种AI应用](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230510134941650.png)\n\n   \n\n![image-20230513170513009](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513170513009.png)\n\n\n\n![image-20230513170928664](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513170928664.png)\n\n![image-20230513171427469](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513171427469.png)\n\n![image-20230513174315520](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513174315520.png)\n\n![image-20230514103513486](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230514103513486.png)\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230515181007117.png\" alt=\"image-20230515181007117\" style=\"zoom:50%;\" /\u003e\n\n\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230515181156709.png\" alt=\"image-20230515181156709\" style=\"zoom:50%;\" /\u003e\n\n![image-20230517174249859](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230517174249859.png)","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null}}