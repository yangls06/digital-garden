{"/":{"title":"ğŸª´ æ— äººä¹‹è·¯.","content":"\n\næ¬¢è¿æ¥åˆ°â€œæ— äººä¹‹è·¯â€ï¼Œèµ°è¿›æˆ‘çš„æ•°å­—èŠ±å›­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æ±‡é›†ã€æ•´ç†è‡ªå·±åœ¨æ•°æ®ç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ã€ä¸ªäººç®¡ç†ã€äººç”Ÿä½“æ‚Ÿç­‰æ–¹é¢çš„æ¶‰çŒå’Œæ€è€ƒï¼Œæ„å»ºè‡ªå·±çš„çŸ¥è¯†ç³»ç»Ÿå’Œç²¾ç¥ä¸–ç•Œã€‚\n\n\n\nä¸ªäººçš„è§’è‰²æ˜¯å¤šé‡çš„ï¼Œå¯¹åº”çš„ç²¾ç¥ä¸–ç•Œåº”è¯¥æ˜¯ä¸°å¯Œçš„ã€‚æˆ‘æœ‰ä¸‹é¢çš„å„ç§èº«ä»½ï¼š\n\n* é•¿æœŸæ¥çœ‹ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæ•°æ®å·¥ç¨‹å¸ˆï¼Œè‡´åŠ›äºæ„å»ºæ•°æ®ç³»ç»Ÿï¼Œä»¥è¾…åŠ©æ•°æ®åŒ–å†³ç­–å’ŒAIç³»ç»Ÿã€‚\n* çŸ­æœŸæ¥çœ‹ï¼Œæˆ‘åœ¨è‡ªåŠ¨é©¾é©¶è¡Œä¸šå·¥ä½œï¼Œå¸®åŠ©è‡ªåŠ¨é©¾é©¶å›¢é˜Ÿæ„å»ºä»æ•°æ®é‡‡é›†ã€æ•°æ®ä¼ è¾“ã€æ•°æ®å­˜å‚¨ã€æ•°æ®æŒ–æ˜ã€æ•°æ®æ ‡æ³¨åˆ°æ•°æ®æ¶ˆè´¹çš„data pipelineï¼Œä»¥å®ç°æ•°æ®é—­ç¯ï¼ŒåŠ©åŠ›è‡ªåŠ¨é©¾é©¶çš„ç ”å‘ã€‚\n* ç»ˆèº«æ¥çœ‹ï¼Œæˆ‘æ˜¯ä¸€ä¸ªä¸ˆå¤«ã€çˆ¶äº²å’Œå„¿å­ï¼Œæƒ³è¦ç»è¥ä¸€ä¸ªå’Œè°ã€è¿›å–çš„å®¶åº­ï¼›å…¶åŸºçŸ³æ˜¯ï¼šæˆ‘æ˜¯ä¸€ä¸ªåœ¨æ€æƒ³ä¸Šä¸æ–­æ‹“å±•ã€è¿½æ±‚è‡ªç”±çš„ä¸ªäººã€‚\n\n\n\næ— è®ºä¸Šé¢çš„å“ªä¸€æ–¹é¢ï¼Œæƒ³è¦åšå¥½ï¼Œéƒ½è¦ä»˜å‡ºæŒç»­çš„åŠªåŠ›ï¼Œèµ°ä¸Šä¸æ–­ç§¯ç´¯ã€ä¸æ–­ç²¾è¿›çš„é“è·¯ã€‚æˆ‘æœŸå¾…åœ¨è¿™ä¸ªå°å°çš„æ•°å­—èŠ±å›­é‡Œï¼Œç¿»åœŸã€æ’­ç§ã€æµ‡æ°´ã€ä¿®å»ºã€æ–½è‚¥ã€æˆç²‰ï¼Œæ‰“ç†å‡ºå±äºè‡ªå·±çš„ä¸€ç‰‡ä¸–ç•Œã€‚\n\n\n\nMy Digital Garden include: \n\n* [Python Programming](notes/python-programming.md)\n* [Data System](notes/data-system.md)\n* [Web Development](notes/web-development.md)\n* [DevOps](notes/devops.md)\n\n","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/Modern-Data-Stack-by-ChatGPT/TOC-by-ChatGPT":{"title":"","content":"# TOC\n\n\n\n## I. Introduction\n\n- What is a modern data stack?\n- The evolution of data processing and storage\n- Why is it important?\n\n## II. Data Collection and Ingestion\n\n- Types of data sources\n- Batch and real-time data ingestion\n- Tools and techniques for data collection and ingestion\n\n## III. Data Storage\n\n- Relational databases\n- NoSQL databases\n- Cloud data storage solutions\n- Data lake concepts and implementation\n\n## IV. Data Processing\n\n- Batch processing\n- Stream processing\n- Interactive processing\n- Distributed computing frameworks\n\n## V. Data Analysis and Visualization\n\n- Big data analysis tools and techniques\n- Machine learning and artificial intelligence\n- Data visualization tools and techniques\n- Dashboarding and reporting solutions\n\n## VI. Data Governance and Security\n\n- Data privacy and security\n- Data management and organization\n- Metadata management\n- Data quality and cleansing\n\n## VII. Conclusion\n\n- The future of the modern data stack\n- Best practices and recommendations\n- Common pitfalls and challenges\n- Final thoughts and next steps.\n\n\n\n## Introduction\n\n### What is a modern data stack?\n\n\n\nA modern data stack refers to the set of tools, technologies, and processes used for collecting, storing, processing, analyzing, and visualizing data in an efficient and scalable manner. A modern data stack typically consists of multiple components, each serving a specific purpose in the overall data pipeline.\n\nThe data stack starts with data collection and ingestion, where data is collected from various sources and loaded into a data storage solution. This can be done in a batch or real-time manner, depending on the requirements. The data storage can be a relational database, a NoSQL database, a cloud data storage solution, or a data lake, among others.\n\nOnce the data is stored, it can be processed using various tools and frameworks for batch processing, stream processing, or interactive processing. The processed data can then be analyzed using big data analysis tools and techniques, machine learning and artificial intelligence algorithms, and visualized using data visualization tools and techniques, dashboarding, and reporting solutions.\n\nFinally, data governance and security play a crucial role in ensuring the quality, privacy, and security of the data. This includes data management and organization, metadata management, data privacy and security, and data quality and cleansing.\n\n```mermaid\ngraph LR\nsubgraph Data Collection and Ingestion\n    ci[Data Collection and Ingestion]\nend\nsubgraph Data Storage\n    ds[Data Storage]\nend\nsubgraph Data Processing\n    dp[Data Processing]\nend\nsubgraph Data Analysis and Visualization\n    dav[Data Analysis and Visualization]\nend\nsubgraph Data Governance and Security\n    dgs[Data Governance and Security]\nend\nci --\u003e ds\nds --\u003e dp\ndp --\u003e dav\ndav --\u003e dgs\n```\n\n### The components of a Modern Data Stack\n\nA modern data stack typically includes the following components:\n\n1. Data Ingestion: This refers to the process of collecting and importing data into the data stack. This can be done through various methods, such as batch processing, real-time streaming, or data synchronization with external systems.\n2. Data Storage: This refers to the solution or technology used to store the data. This can be a traditional relational database, a NoSQL database, a data warehouse, or a cloud-based data storage solution.\n3. Data Processing: This refers to the solution or technology used to process and transform the data. This can be a batch processing solution, a real-time streaming solution, or a data processing engine.\n4. Data Governance: This refers to the tools and technologies used to manage the quality and governance of the data. This can include data catalogs, data quality tools, data lineage solutions, and data auditing tools.\n5. Data Security: This refers to the tools and technologies used to protect the data from cyber threats and data breaches. This can include data encryption, data access controls, and data auditing tools.\n6. Data Analytics: This refers to the tools and technologies used to analyze and gain insights from the data. This can include data visualization tools, machine learning algorithms, and predictive analytics models.\n\nEach component of a modern data stack is important and plays a crucial role in enabling organizations to process and store their data effectively. By having a complete and integrated data stack, organizations can ensure that they have a flexible, scalable, and secure infrastructure for their data, enabling them to make informed decisions and drive business growth.\n\n\n\n### The evolution of data processing and storage\n\n\n\nThe evolution of data processing and storage has gone through several phases, starting from the early days of computing to the modern era of big data and cloud computing.\n\n1. The Early Days of Computing: In the early days of computing, data was processed and stored on mainframe computers. Data was entered into these computers through punch cards and was stored on magnetic tapes. Data processing was performed in batch mode, and data storage was limited by the amount of physical storage available on the computer.\n2. The Emergence of Relational Databases: In the 1970s and 1980s, relational databases emerged as a new way of processing and storing data. Relational databases were designed to store structured data, and they used the relational model to store data in tables, with relationships between the tables defined by keys. Relational databases provided a flexible and scalable way of processing and storing data, and they became widely used in organizations for business applications and data management.\n3. The Rise of Big Data: In the late 2000s and early 2010s, the amount of data being generated and collected by organizations exploded, giving rise to the concept of big data. Big data refers to the large and complex datasets that organizations generate and collect, often from multiple sources and in multiple formats. The rise of big data presented new challenges for data processing and storage, as traditional relational databases were no longer able to handle the scale and complexity of the data.\n4. The Emergence of NoSQL Databases: In response to the challenges of big data, NoSQL databases emerged as a new way of processing and storing data. NoSQL databases were designed to handle large and complex datasets, and they used different data models, such as key-value, document, and graph, to store data. NoSQL databases provided organizations with a flexible and scalable way of processing and storing big data, and they became widely used for big data applications and data management.\n5. The Rise of Cloud Computing: In the 2010s and early 2020s, cloud computing emerged as a new way of delivering computing resources, including data processing and storage, over the internet. Cloud computing provided organizations with a flexible and scalable infrastructure for data processing and storage, without the need for expensive hardware and software. The rise of cloud computing has had a profound impact on the evolution of data processing and storage, as organizations can now leverage cloud-based data processing and storage solutions to handle large and complex datasets, without the need for expensive hardware and software.\n\nIn summary, the evolution of data processing and storage has gone through several phases, from the early days of computing to the modern era of big data and cloud computing. Each phase has brought new challenges and opportunities for data processing and storage, and has enabled organizations to process and store their data more effectively and efficiently.\n\n\n\nHere's a short timeline of the evolution of data processing and storage:\n\n- 1960s-1970s: Introduction of the first relational databases\n- 1980s-1990s: Development of enterprise-level relational databases (Oracle, SQL Server)\n- 2000s: Emergence of big data and NoSQL databases (Hadoop, MongoDB, Cassandra)\n- 2010s: Adoption of cloud computing and cloud data storage solutions (AWS, Azure, Google Cloud)\n- 2020s: Advancements in AI and machine learning and their integration into data processing and storage solutions.\n\nThis timeline highlights the key milestones in the evolution of data processing and storage and shows how new technologies and solutions have emerged over the years to meet the growing demand for more efficient and scalable data processing and storage solutions.\n\n\n\n### Why is it important?\n\nThe modern data stack refers to the combination of technologies, tools, and solutions that organizations use to process and store their data. In today's digital economy, where data is the new oil, it's essential for organizations to have a modern data stack in place for several reasons:\n\n1. Improved Business Insights: A modern data stack enables organizations to collect, process, and analyze large amounts of data from various sources, including structured and unstructured data. This helps organizations gain a comprehensive understanding of their customers, operations, and market trends, which can lead to more informed decision-making.\n2. Increased Data Agility: The modern data stack provides organizations with a flexible and scalable infrastructure for processing and storing data. This enables organizations to quickly adapt to changing business needs, such as new data sources, new analytics requirements, and new business initiatives, without being limited by their data infrastructure.\n3. Better Data Governance: A modern data stack includes solutions for data governance, such as data catalogs, data quality tools, and data lineage solutions. These tools help organizations manage their data more effectively, by providing a centralized repository for data definitions, data quality rules, and data lineage information.\n4. Improved Data Security: A modern data stack includes solutions for data security, such as data encryption, data access controls, and data auditing tools. These tools help organizations protect their sensitive data from cyber threats and data breaches, ensuring that their data remains secure and compliant with regulatory requirements.\n5. Lower Total Cost of Ownership (TCO): By using a modern data stack, organizations can reduce their TCO for data processing and storage. For example, cloud-based data processing and storage solutions can provide organizations with cost-effective solutions for processing and storing large amounts of data, without the need for expensive hardware and software.\n\nIn summary, the modern data stack is essential for organizations in today's digital economy, as it provides organizations with the tools and technologies they need to manage their data more effectively, gain better insights from their data, and reduce their TCO for data processing and storage.\n\n\n\n## Data Collection and Ingestion\n\nData collection and ingestion is the process of acquiring and processing data from various sources and preparing it for storage and analysis. The data collection and ingestion process is a crucial part of the modern data stack, as it determines the quality and reliability of the data that will be used for analysis and decision-making.\n\n\n\n### Data Sources\n\n\n\nData sources refer to the places where data originates and can be collected. Data sources can come in various forms, including internal systems and databases, external APIs, text files, and images. The following are the most common data sources:\n\n1. Internal Systems and Databases: This type of data source refers to the data stored within an organization's own systems and databases. This data can include customer data, sales data, and employee data.\n2. External APIs: An API (Application Programming Interface) is a set of protocols and tools for building software applications. APIs can be used to access data from external sources, such as social media platforms, weather services, and online marketplaces.\n3. Text Files: Data can also be collected from text files such as CSV (Comma-Separated Values) or JSON (JavaScript Object Notation) files. These files can contain structured or unstructured data, depending on the format in which they are stored.\n4. Images: Data can also be collected from images, such as satellite images, drone images, and medical images. These images can be analyzed using computer vision techniques to extract valuable information.\n\nData sources can be either structured or unstructured. Structured data refers to data that is organized in a specific format, such as tables, while unstructured data refers to data that does not have a specific format, such as text and images.\n\nIn conclusion, data sources are the places where data originates and can be collected. Data sources can range from internal systems and databases to external APIs, text files, and images. The type and format of the data collected from these sources will determine the type of data collection and ingestion process that will be used.\n\n\n\n### Data Ingestion\n\n\n\n#### Types\n\nData ingestion refers to the process of importing data from various sources into a centralized storage system for further processing and analysis. The following are the most common methods of data ingestion:\n\n1. Batch Ingestion: Batch ingestion is the process of importing data in large amounts into a centralized storage system. This method is used when the data is available in a static form and needs to be processed in bulk.\n\n2. Real-time Ingestion: Real-time ingestion is the process of importing data into a centralized storage system as soon as it becomes available. This method is used when the data is generated in real-time and needs to be processed immediately.\n\n3. Stream Ingestion: Stream ingestion is a combination of batch and real-time ingestion, where data is processed in real-time but stored in batch mode. This method is used when the data is generated in real-time but needs to be processed in bulk for analysis purposes.\n\n   \n\nIn Chapter 3.1: Data Sources, we discussed the various types of data sources including databases, log files, cloud services, and social media platforms. The following are the most common methods of data ingestion for each data source:\n\n1. Databases: Data can be ingested from databases using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n2. Log Files: Log files can be ingested using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n3. Cloud Services: Cloud services can be ingested using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n4. Social Media Platforms: Social media platforms can be ingested using either batch or real-time ingestion methods. Batch ingestion is the process of importing data in large amounts into a centralized storage system, while real-time ingestion is the process of importing data as soon as it becomes available.\n\nData ingestion is a crucial step in the modern data stack as it determines the quality and reliability of the data that will be used for further analysis. To ensure data quality, data ingestion processes should include steps for data validation, data cleaning, and data normalization.\n\nIn conclusion, data ingestion refers to the process of importing data from various sources into a centralized storage system. The method used for data ingestion will depend on the type and format of the data being collected and the data source it is being collected from. It is important to have a robust data ingestion process in place to ensure data quality and reliability for further analysis.\n\n\n\n#### ETL\n\nThe ETL (Extract, Transform, Load) process is a widely used method for data ingestion in the modern data stack. The ETL process involves the following steps:\n\n1. Extract: The first step of the ETL process is to extract data from various sources such as databases, log files, cloud services, and social media platforms. Data extraction is typically performed using data extraction tools and scripts.\n2. Transform: The second step of the ETL process is to transform the extracted data into a format that is suitable for analysis and storage. Data transformation involves various activities such as data cleaning, data normalization, data enrichment, and data integration. This step is performed using data transformation tools and scripts.\n3. Load: The final step of the ETL process is to load the transformed data into a centralized storage system such as a data lake or a data warehouse. This step is performed using data loading tools and scripts.\n\nThe ETL process is important in the modern data stack as it enables organizations to collect and process large amounts of data from various sources in a consistent and reliable manner. The transformed data can then be used for further analysis and reporting purposes.\n\nIt is important to note that the ETL process can be time-consuming and resource-intensive, especially for large data sets. To minimize the impact on system performance, organizations often use parallel processing and distributed systems to perform the ETL process. Additionally, organizations may use cloud-based ETL solutions to take advantage of the scalability and cost-effectiveness of cloud computing.\n\nIn conclusion, the ETL process is a widely used method for data ingestion in the modern data stack. It involves the steps of extracting data from various sources, transforming the data into a suitable format, and loading the transformed data into a centralized storage system. The ETL process is important for collecting and processing large amounts of data in a consistent and reliable manner.\n\n\n\n### Data Quality\n\nData quality is a critical aspect of data ingestion in the modern data stack. It refers to the accuracy, completeness, consistency, and reliability of the data being ingested. Poor data quality can lead to incorrect insights and decisions, as well as wasted time and resources.\n\nIn the context of data ingestion, data quality checks are performed during the transform step of the ETL process. During this step, data quality checks are used to identify and correct issues such as missing values, duplicate records, and inconsistent data formats.\n\nThere are several data quality checks that are commonly performed during the transform step of the ETL process, including:\n\n1. Data Validation: Data validation checks are performed to ensure that the data conforms to certain rules or constraints. For example, data validation checks can be used to ensure that a date field contains a valid date format or that a string field contains only alphanumeric characters.\n2. Data Cleansing: Data cleansing is the process of removing or correcting invalid, incomplete, or inconsistent data. This step is important for ensuring that the data is accurate and consistent, and can be used for further analysis and reporting purposes.\n3. Data Normalization: Data normalization is the process of transforming data into a standardized format. This step is important for ensuring that data from different sources is consistent and can be easily integrated.\n4. Data Enrichment: Data enrichment is the process of adding additional data to the data being ingested. This step is often used to add contextual information or to enhance the data with additional information from external sources.\n\nIt is important to note that data quality checks can be time-consuming and resource-intensive, especially for large data sets. To minimize the impact on system performance, organizations often use parallel processing and distributed systems to perform the data quality checks. Additionally, organizations may use cloud-based data quality solutions to take advantage of the scalability and cost-effectiveness of cloud computing.\n\nIn conclusion, data quality is a critical aspect of data ingestion in the modern data stack. Ensuring data quality is important for ensuring that the data being ingested is accurate, complete, consistent, and reliable, and can be used for further analysis and reporting purposes. Data quality checks are performed during the transform step of the ETL process, and may include data validation, data cleansing, data normalization, and data enrichment.\n\n\n\n### Data Ingestion Tools\n\nData ingestion tools are software solutions used to collect and import data into a data repository. These tools play a crucial role in the modern data stack and are essential for processing and storing data effectively.\n\nThere are several types of data ingestion tools, each with its own set of features and capabilities. These include:\n\n1. Batch Ingestion Tools: These tools are designed for importing large amounts of data into a data storage system in a single transaction. Examples include Apache Nifi and Talend Open Studio.\n2. Real-time Ingestion Tools: These tools are designed for continuously streaming data from various sources into a data storage system in real-time. Examples include Apache Kafka and Amazon Kinesis.\n3. File-based Ingestion Tools: These tools are designed for processing and importing structured or semi-structured data in the form of files, such as CSV or JSON. Examples include Apache Flume and Logstash.\n4. API-based Ingestion Tools: These tools allow for data ingestion via APIs. They can be used to integrate data from cloud-based applications such as Salesforce and Google Analytics into a data storage system. Examples include Talend Cloud and MuleSoft.\n5. Cloud-based Data Integration Tools: These tools are specifically designed for data integration in cloud environments. They provide a set of tools and services to move and integrate data between cloud-based data sources and storage systems. Examples include FiveTran and AirByte.\n6. Custom Data Ingestion Tools: These are specialized data ingestion tools that are built for specific use cases or to address specific challenges. They can be built from scratch or using open-source tools and technologies. Examples include custom scripts and data pipelines.\n\nRegardless of the type of data ingestion tool used, it is important to ensure that data quality is maintained throughout the ingestion process. This involves validating and transforming the data as it is being imported, and ensuring that the data is properly formatted and structured.\n\n\n\n#### Apache NiFi\n\nApache NiFi is a powerful, open-source software platform for automating and managing the flow of data between systems. It is designed to be highly scalable, fault-tolerant, and flexible, making it an ideal choice for handling large amounts of data in real-time.\n\nAt its core, Apache NiFi provides a web-based interface for designing and managing data flows. Users can drag and drop a variety of data processing and transformation components onto a canvas and connect them to create complex data pipelines.\n\nOne of the key benefits of Apache NiFi is its ability to handle a wide range of data sources and formats. It can ingest data from various sources, such as databases, message queues, web services, and sensors, and can process data in real-time. Additionally, Apache NiFi provides many built-in data transformation and processing functions, allowing users to perform tasks such as data enrichment, aggregation, and filtering with ease.\n\nAnother advantage of Apache NiFi is its ability to handle large volumes of data with ease. It can scale horizontally to handle large amounts of data, and it can be easily configured to handle different levels of data volume and velocity. This makes it an ideal choice for organizations that need to process large amounts of data in real-time.\n\nOverall, Apache NiFi is a powerful and flexible tool for data ingestion and management, offering a wide range of capabilities for organizations that need to process large amounts of data in real-time.\n\n\n\nApache NiFi has several use cases:\n\n1. Data routing and transformation: NiFi can be used to route, transform and process data from various sources to its final destination.\n2. Data ingestion for big data systems: NiFi is commonly used to ingest large amounts of data into big data systems like Hadoop and Apache Spark.\n3. Data integration: NiFi can be used to integrate data from multiple systems and format it into a standardized format.\n4. Real-time streaming data: NiFi can be used to process real-time streaming data and feed it into data lakes, data warehouses or other big data systems.\n5. Internet of Things (IoT) data management: NiFi is well-suited for handling large amounts of data from IoT devices and sensors, making it a popular choice for IoT data management.\n6. Security information and event management (SIEM): NiFi can be used as a SIEM to collect, store, and analyze log data from various sources for security purposes.\n\n\n\n#### Apache Kafka\n\nApache Kafka is a distributed streaming platform that is used for building real-time data pipelines and streaming applications. Some key features and benefits of Apache Kafka include:\n\n1. Scalability: Apache Kafka is designed to handle high volumes of data in real-time and can scale horizontally by adding more brokers to the cluster.\n2. Durability: Apache Kafka stores all data on disk, providing durability in case of failures and ensuring that data is not lost.\n3. Performance: Apache Kafka is optimized for high performance and low latency, making it suitable for processing large amounts of data in real-time.\n4. Real-time streaming: Apache Kafka enables real-time data streaming and processing, making it a popular choice for use cases such as log aggregation, event sourcing, and IoT.\n5. Publish-Subscribe Model: Apache Kafka uses a publish-subscribe model, where producers publish messages to topics and consumers subscribe to topics to receive messages. This model provides decoupling between producers and consumers and enables horizontal scaling of consumers.\n6. Multi-language support: Apache Kafka provides APIs for multiple programming languages, making it easy to integrate with a wide range of systems and applications.\n\n\n\nThe internal architecture of Apache Kafka consists of four core components:\n\n1. Topics: Topics are categories or feed names to which messages are published.\n2. Partitions: A topic can be divided into multiple partitions, allowing parallel processing of the data stream. Each partition is an ordered, immutable sequence of messages.\n3. Brokers: A Kafka cluster consists of one or more servers, known as brokers, that run Kafka. They are responsible for maintaining the data in the topics and for serving client requests.\n4. Producers: Producers are the clients that publish messages to topics in the broker.\n5. Consumers: Consumers are the clients that subscribe to topics and process the published messages.\n6. Zookeeper: Apache Zookeeper is used to manage the configuration of the Kafka cluster, to coordinate the actions of the brokers and to provide failover in case of broker failures.\n\nSome use cases of Apache Kafka include:\n\n- Real-time data processing and analysis\n- Data integration between multiple systems\n- Building event-driven architectures\n- Log aggregation and analysis\n- Monitoring and alerting.\n\nSome popular examples of companies using Apache Kafka include Netflix, LinkedIn, and Uber.\n\n\n\n#### Logstash\n\nLogstash is an open source data collection and processing pipeline tool. It is part of the larger Elastic Stack, which also includes Elasticsearch, Kibana, and Beats. Logstash is designed to ingest, process, and transfer data from a variety of sources and output it to a variety of destinations.\n\nOne of the key features of Logstash is its ability to process and manipulate data in real-time. It has a wide range of input plugins that can be used to collect data from various sources, including files, databases, message brokers, and HTTP endpoints. Logstash also provides a number of filter plugins for transforming and manipulating data, including data enrichment, data normalization, and data filtering. The output plugins of Logstash allow the processed data to be sent to a variety of destinations, including Elasticsearch, Apache Kafka, files, and databases.\n\nLogstash is highly configurable and can be used to create custom data processing pipelines to fit specific use cases. It supports many programming languages, including Ruby, Java, and Groovy, and provides a rich set of APIs for data processing. It can be run on-premises, in the cloud, or in a hybrid deployment, making it a flexible tool for data collection and processing.\n\nSome examples of use cases for Logstash include log management, security event analysis, and IoT data processing. It is often used in combination with the other components of the Elastic Stack for centralized logging, real-time analytics, and visualizing data.\n\n\n\n#### Talend Cloud\n\nTalend Cloud is a cloud-based data integration tool that helps organizations to manage, process, and integrate large amounts of data from various sources into a single platform. It provides a wide range of tools and features to streamline data integration, including data ingestion, data transformation, and data delivery. The tool supports various data sources, including databases, big data platforms, cloud services, and file systems, and can be used to extract, transform, and load data into data warehouses, data lakes, or other data storage platforms.\n\nSome of the key features of Talend Cloud include:\n\n- Data Quality and Governance: Talend Cloud provides data quality and governance features to ensure the accuracy, completeness, and consistency of data.\n- Data Mapping: The tool provides an intuitive drag-and-drop interface for mapping data from source to target systems.\n- Real-time Processing: Talend Cloud supports real-time data processing, enabling organizations to make decisions based on real-time data.\n- Scalability: Talend Cloud is designed to scale with organizations as they grow, enabling them to integrate and manage large amounts of data.\n- Security: The tool provides robust security features to protect sensitive data, including encryption, access control, and audit trails.\n\nExamples of organizations that use Talend Cloud include KPMG, GE Healthcare, and Coca-Cola.\n\n\n\n#### FiveTran\n\nFiveTran is a cloud-based data integration tool that simplifies the process of connecting and collecting data from various sources into a central data warehouse. It provides a fast and reliable solution to automate the process of extracting, transforming, and loading (ETL) data into a data warehouse. FiveTran supports over 100 different data sources, including popular cloud applications like Salesforce, Hubspot, and Google Analytics, as well as databases and APIs.\n\nOne of the key features of FiveTran is its ability to support real-time data replication, which ensures that the data in the data warehouse is always up-to-date. Additionally, it provides a user-friendly interface to manage the entire data pipeline, making it easy for users to monitor and troubleshoot any issues that may arise during the data ingestion process.\n\nSome of the use cases for FiveTran include:\n\n- Centralizing data from disparate sources for business intelligence and data analysis.\n- Automating the process of moving data from multiple sources into a data warehouse for analysis and reporting.\n- Enabling real-time data analysis and reporting by ensuring that data in the data warehouse is up-to-date.\n\nOverall, FiveTran is an ideal solution for organizations that are looking to streamline and simplify their data integration process, allowing them to focus on data analysis and insights instead of dealing with complex data pipelines.\n\n\n\n#### AirByte\n\nAirbyte is a cloud-based data integration platform that helps businesses centralize, clean and transform data from various sources. It's designed for ease of use and can be set up in a matter of minutes. Airbyte allows businesses to quickly connect to popular data sources such as databases, cloud-based software, and SaaS tools, and automatically replicate data to a central location. The platform includes built-in data transformation and enrichment capabilities, which make it easy to clean and standardize data. Additionally, Airbyte allows users to track and visualize data lineage, which helps with data governance and auditability. Some common use cases for Airbyte include data warehousing, data lake ingestion, data analysis, and reporting.\n\nAirbyte and FiveTran are both cloud-based data integration tools. They are designed to help organizations easily collect, clean, and move data from various sources to a centralized data warehouse.\n\nFiveTran is a fully managed, cloud-native data integration platform that provides a simple, scalable, and automated way to move data from various sources to your data warehouse. It provides out-of-the-box integrations for popular applications and data sources, allowing you to quickly and easily set up your data pipeline.\n\nAirbyte, on the other hand, is an open-source data integration platform that provides a similar range of functionality to FiveTran, but allows for greater customization and control over your data pipeline. It supports a wide range of sources, including databases, SaaS apps, and cloud storage, and provides a flexible, modular architecture that allows you to easily add custom transformations and manipulate data as it is being processed.\n\nIn terms of differences, FiveTran may be a better choice for organizations that need a simple, turn-key solution that can be set up quickly, while Airbyte may be a better choice for organizations that require more control and customization over their data pipeline. Additionally, FiveTran is a commercial product, while Airbyte is open source, so organizations that want to save money may prefer Airbyte.\n\n\n\n#### Apache Airflow \n\nApache Airflow is an open-source workflow management platform that allows users to programmatically author, schedule, and monitor workflows. It was initially developed by Airbnb, and has since become a widely adopted tool for managing complex workflows in the data engineering and data science fields.\n\nOne of the key features of Apache Airflow is its ability to handle multi-task workflows. It allows users to define a series of tasks and the dependencies between them, and then execute those tasks in the correct order. Tasks can be executed on a schedule, triggered by events, or run on-demand.\n\nAnother important feature of Apache Airflow is its extensibility. It provides a number of plugins that allow users to extend the functionality of the platform, including integrations with popular data storage and processing technologies like Apache Hive, Apache Spark, and Apache Pig.\n\nFinally, Apache Airflow includes a number of tools for monitoring and troubleshooting workflows. It provides a web-based user interface that provides real-time updates on the status of running workflows, as well as the ability to view logs, track errors, and view performance metrics.\n\n\n\nOne of the key features of Airflow is its UI, which provides a visual representation of your workflows and their status, making it easy to understand what's happening in your pipelines. Additionally, Airflow has a large and active community, which means there is a wealth of resources and plugins available to help you solve common problems.\n\nAirflow is used by many organizations in a variety of industries, from technology companies to financial services and beyond. Some of the most common use cases for Airflow include:\n\n- ETL pipelines: Airflow is often used to automate the process of extracting, transforming, and loading data from a variety of sources into data warehouses or data lakes.\n- Machine learning workflows: Airflow can be used to manage the steps involved in training machine learning models, including data preparation, model training, and model evaluation.\n- Monitoring workflows: Airflow can be used to automate tasks related to system and application monitoring, such as pulling log data, sending notifications, and generating reports.\n\nOverall, Apache Airflow is a highly flexible and scalable platform for managing data workflows, making it a popular choice for organizations that need to process large amounts of data in a reliable and efficient manner.\n\n\n\n## Data Storage\n\n### Overview\n\nData storage refers to the process of preserving data in a persistent manner, either on physical or virtual storage mediums. With the increasing amount of data generated every day, modern data storage systems are designed to store and manage vast amounts of data efficiently, securely, and cost-effectively.\n\n\n\nTypes of Data Storage\n\nThere are several types of data storage systems available, including:\n\n- Relational databases (RDBMS): These databases store data in tables and columns, and support Structured Query Language (SQL) for querying and manipulating data. Examples include MySQL, PostgreSQL, and Microsoft SQL Server.\n- NoSQL databases: These databases store data in a non-relational manner and support flexible, semi-structured data. Examples include MongoDB, Cassandra, and Couchbase.\n- Data warehousing: Data warehousing systems are designed to store large amounts of structured data and support fast querying and analysis. Examples include Amazon Redshift, Google BigQuery, and Snowflake.\n- Distributed file systems: These systems store and manage large amounts of unstructured data across multiple servers and storage devices. Examples include Apache Hadoop HDFS, Apache Cassandra FS, and GlusterFS.\n\n\n\nData Storage Considerations\n\nWhen selecting a data storage system, there are several factors to consider, including:\n\n- Scalability: The ability to increase the capacity of the system as needed to accommodate growing amounts of data.\n- Performance: The speed and efficiency with which data can be stored, retrieved, and processed.\n- Reliability: The ability of the system to maintain data integrity and availability in the event of failures or errors.\n- Security: The measures in place to protect data from unauthorized access or manipulation.\n- Cost: The cost of purchasing, maintaining, and scaling the system.\n\n\n\nConclusion\n\nData storage is a critical component of a modern data stack, and there are many different types of data storage systems available to suit different requirements. Choosing the right data storage system requires careful consideration of factors such as scalability, performance, reliability, security, and cost.\n\n\n\n### Relational databases\n\nRelational databases, also known as RDBMS (Relational Database Management Systems), are databases that store data in tables with rows and columns, forming a structured relationship between data elements. These databases are based on the relational model proposed by E.F. Codd in 1970.\n\nIn a relational database, data is organized into tables, with each table representing a specific type of data, such as customer information, product information, or sales transactions. Each table has a set of columns that describe the properties or attributes of the data in that table. For example, a customer information table might have columns for first name, last name, email address, and postal code. Rows in the table represent individual records, with each row representing a single customer.\n\nRelational databases use a language called Structured Query Language (SQL) to query and manipulate data stored in tables. This allows for the creation of complex queries and the ability to join data from multiple tables to produce a single result set.\n\nRelational databases are widely used for a variety of applications and are a popular choice for managing structured data due to their ability to enforce relationships between data, ensuring data integrity and consistency. They also provide robust security features, scalability, and support for transactions. Examples of relational databases include MySQL, Oracle, and Microsoft SQL Server.\n\n\n\n### NoSQL databases\n\nNoSQL databases are a type of non-relational database that are designed to handle large amounts of data that don't fit neatly into a traditional relational database structure. Unlike relational databases, which use tables and rows to store data, NoSQL databases use a variety of data storage structures, such as key-value pairs, document databases, column-family databases, and graph databases. NoSQL databases are often used for large-scale, high-performance web and mobile applications, as they are designed to be flexible, scalable, and highly available.\n\nSome of the key benefits of NoSQL databases include:\n\n1. Scalability: NoSQL databases can easily scale horizontally, meaning that more servers can be added to the system as needed to handle growing amounts of data.\n2. Flexibility: NoSQL databases can handle semi-structured or unstructured data, which is common in many modern applications.\n3. Performance: NoSQL databases are designed to be fast and efficient, making them well-suited for high-performance applications.\n4. Cost-effectiveness: NoSQL databases are often open source and have lower hardware requirements compared to relational databases, making them a cost-effective option for many organizations.\n\nSome examples of popular NoSQL databases include MongoDB, Cassandra, Redis, and Couchbase.\n\n\n\nMore details about MongoDB, Cassandra, Redis, and Couchbase:\n\nMongoDB: MongoDB is a popular NoSQL database that uses a document-based data model. This means that data is stored as documents within a collection, rather than in tables with columns and rows like a relational database. MongoDB is designed for high performance, with automatic sharding that distributes data across multiple servers, and a flexible schema that makes it easy to store and query complex data.\n\nCassandra: Cassandra is a distributed NoSQL database that is optimized for scalability and high availability. It uses a column-family data model, where data is organized into columns instead of rows. Cassandra provides linear scalability and can handle high write loads, making it well-suited for applications that need to store and process large amounts of data.\n\nRedis: Redis is an in-memory data store that can be used as a database, cache, or message broker. It uses a key-value data model, where data is stored as key-value pairs in memory, making it very fast for read and write operations. Redis also supports advanced data structures such as lists, sets, and hashes, making it a good choice for applications that require complex data structures.\n\nCouchbase: Couchbase is a NoSQL database that combines the scalability and performance of a NoSQL database with the ease of use and functionality of a relational database. It uses a document-based data model and supports indexing, querying, and full-text search, making it a good choice for applications that need to store and query complex data structures. Couchbase also includes a built-in caching layer and supports automatic data replication and failover, ensuring high availability and data durability.\n\n\n\n### Cloud data storage solutions\n\nCloud data storage solutions refer to the data storage infrastructure provided by cloud computing service providers. They provide a scalable and cost-effective alternative to traditional on-premise data storage solutions. The data is stored on remote servers and can be accessed from anywhere with an internet connection.\n\nSome examples of cloud data storage solutions include:\n\n- Amazon Simple Storage Service (S3)\n- Microsoft Azure Blob Storage\n- Google Cloud Storage\n- IBM Cloud Object Storage\n\nCloud data storage solutions offer several advantages such as scalability, reliability, cost savings, and flexibility. They allow users to store unlimited amounts of data without having to worry about the physical storage space or maintaining the infrastructure. The storage capacity can be easily increased or decreased based on the user's requirements, and the cost is typically charged on a pay-as-you-go basis. Additionally, the data stored in the cloud is highly secure and can be accessed from anywhere with an internet connection, making it an ideal solution for organizations with a remote workforce.\n\n\n\nTake S3 for example. \n\n\n\nAmazon Simple Storage Service (S3) is a cloud-based object storage service offered by Amazon Web Services (AWS). S3 provides scalable and durable storage for a variety of data types, including binary files, text files, and structured data. S3 stores data as objects in a bucket, and objects are accessible via a unique identifier known as a key.\n\nS3 offers a range of storage classes, including standard, infrequent access, and archive, that allow customers to store data at different levels of availability and cost. S3 also offers built-in data management features, such as versioning and lifecycle policies, that make it easy to manage the data stored in the service. Additionally, S3 provides a robust security model that includes the ability to encrypt data at rest and in transit, control access to objects with fine-grained IAM policies, and monitor access to objects with AWS CloudTrail.\n\nS3 is widely used as a data lake for big data analytics, as well as for storing backups and archiving data. Its scalability, durability, and low cost make it an attractive option for many organizations looking to store large amounts of data in the cloud.\n\n\n\n### Data lake concepts and implementation\n\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. The main idea behind a data lake is to store data in its raw form, without any pre-processing or transformation, and then process the data on an as-needed basis. This allows organizations to store massive amounts of data in a cost-effective way, while maintaining the flexibility to process the data in the future as business needs change.\n\nData lakes are implemented using cloud-based storage solutions, such as Amazon Simple Storage Service (S3), Microsoft Azure Data Lake Storage, or Google Cloud Storage. These storage solutions are scalable and cost-effective, making it easy for organizations to store large amounts of data.\n\nOnce the data is stored in a data lake, it can be processed using big data tools such as Apache Hadoop, Apache Spark, or Apache Hive. These tools allow organizations to perform complex data processing and analysis, without the need for expensive hardware or software.\n\n\n\nA data lake application example could be a retail company using a data lake to store and process data from multiple sources such as customer purchase history, website clicks, and marketing campaign data. The company can then use this data to perform advanced analytics and generate insights, such as identifying the most popular products, tracking customer behavior, and optimizing marketing campaigns. The data lake enables the company to store structured, semi-structured, and unstructured data in its raw form, providing a centralized repository for all their data. The company can then use tools such as Apache Spark or Apache Hive to perform big data processing on the data stored in the data lake and generate insights for business decision making.\n\n\n\nIn summary, a data lake provides organizations with a centralized repository for storing all their data, while providing the flexibility to process the data in the future as business needs change. The use of cloud-based storage solutions and big data tools make data lakes a cost-effective solution for storing and processing large amounts of data.\n\n\n\n## Data Processing\n\n### Data Processing Overview\n\nThis chapter provides an overview of data processing, explaining the different types of data processing such as batch processing, real-time processing, and stream processing. It covers the purpose of data processing and why it is important in the context of big data.\n\n\n\nData Processing is a critical aspect of the data pipeline. It involves transforming raw data into a format that can be analyzed and used to derive insights. The data processing phase is crucial in ensuring that the data is clean, consistent, and in a format that can be easily utilized by data analysts and scientists.\n\nThere are several key components of data processing, including data extraction, data cleaning, data transformation, and data enrichment. Data extraction involves retrieving the raw data from various sources, including databases, APIs, and file systems. Data cleaning involves removing any irrelevant, inconsistent, or duplicate data. Data transformation involves converting the raw data into a format that can be easily analyzed, such as aggregating data, normalizing data, or pivoting data. Data enrichment involves adding additional information to the data, such as geolocation data or demographic data.\n\nIn order to achieve high-quality data processing, it is important to implement automated processes, as well as to apply best practices such as data validation and data quality checks. This can help to ensure that the data is accurate, consistent, and of high quality, which is essential for making informed decisions and achieving meaningful insights.\n\n\n\n### Batch Processing\n\n\n\nBatch processing refers to the method of processing large data sets in a batch mode, rather than processing data in real-time. This approach is typically used for offline processing and is often used to process high volumes of data in a cost-effective manner. The batch processing is performed in a sequential order, processing data in chunks or \"batches\" rather than processing one data record at a time. This method of processing is well suited for large data sets that do not require immediate processing or real-time results.\n\nIn batch processing, data is first collected, typically from various sources such as databases, file systems, or other data storage systems. The data is then processed in a predefined sequence, often using specialized software or tools, to transform the data into a format that is suitable for analysis, reporting, or further processing. The transformed data is then typically stored in a data store such as a data warehouse or a data lake for future analysis.\n\nOne of the primary benefits of batch processing is its ability to handle large volumes of data in a cost-effective manner. Batch processing can also handle complex data transformations and manipulation tasks, making it ideal for data cleansing, data integration, and other data management tasks. Additionally, batch processing can be scheduled to run at specific times, allowing organizations to plan and manage their data processing resources more effectively.\n\nExamples of batch processing include payroll processing, sales data analysis, and monthly reporting. Batch processing can be performed using a variety of tools and technologies, including Apache Hadoop, Apache Spark, and Apache Storm, among others.\n\n\n\n#### Hadoop\n\nApache Hadoop is an open-source software framework for storing and processing big data. It provides a scalable and fault-tolerant platform for distributed processing of large datasets. It was created by the Apache Software Foundation and is widely used for big data processing, storage and analysis. Hadoop is designed to work in a cluster environment, where multiple computers work together to process large amounts of data.\n\nHadoop has two main components: Hadoop Distributed File System (HDFS) and MapReduce. HDFS is a distributed file system that stores large data files across multiple nodes in a cluster. This allows for processing and analysis of very large datasets. MapReduce is a programming model for processing large datasets in parallel. It takes input data, divides it into smaller chunks, processes the chunks on different nodes in parallel, and combines the results into a final output.\n\nHadoop also includes other components such as YARN (Yet Another Resource Negotiator), which is a resource management system that allocates resources to the various processing tasks, and Pig, a high-level data processing language that can be used to write MapReduce programs.\n\nHadoop has become an important tool for big data processing and is widely used by organizations of all sizes for tasks such as log analysis, recommendation systems, fraud detection and more.\n\n\n\n#### Spark\n\nApache Spark is a fast, open-source, and general-purpose big data processing engine. It provides an in-memory computing framework that allows data scientists and engineers to perform distributed computing tasks faster and more efficiently than traditional batch processing frameworks like Hadoop MapReduce. Spark supports multiple programming languages such as Scala, Java, and Python, and provides APIs for high-level data analysis and machine learning tasks.\n\nOne of the main advantages of Apache Spark over Hadoop MapReduce is its ability to perform data processing in memory, as well as disk, which leads to faster processing speeds. Spark also provides an optimized engine for executing SQL-like queries, streaming data processing, machine learning algorithms, and graph processing.\n\nApache Spark has a large and active open-source community and has been adopted by many organizations due to its performance, scalability, and ease of use. Additionally, Spark integrates with popular big data storage systems such as Hadoop Distributed File System (HDFS), Apache Cassandra, and Apache HBase, which makes it a popular choice for big data processing.\n\n\n\n### Real-time Processing\n\nReal-time processing refers to the processing of data as soon as it is generated or received. It is a form of data processing that enables organizations to make decisions in near real-time, providing immediate business value. Real-time processing is becoming increasingly important for a variety of applications, including financial services, e-commerce, gaming, telecommunications, and more.\n\nTo achieve real-time processing, data must be processed as soon as it is received, with minimal latency. This requires the use of technologies such as in-memory data stores, distributed processing frameworks, and stream processing engines.\n\nOne of the key benefits of real-time processing is that it enables organizations to respond quickly to changing business conditions, such as market trends or customer behavior. For example, real-time processing can be used to monitor and analyze customer interactions, detect fraud, or provide recommendations based on customer behavior.\n\nSome popular real-time processing technologies include Apache Kafka, Apache Flink, and Apache Storm. These technologies provide a distributed and scalable architecture that can handle high volumes of data in real-time. They also provide the ability to process data in a stream-oriented manner, enabling organizations to process large amounts of data as soon as it is generated.\n\nOverall, real-time processing is a critical component of modern data architectures, enabling organizations to make faster and more informed decisions based on real-time data insights.\n\n\n\n### Apache Kafka\n\nApache Kafka is a distributed, high-throughput, and fault-tolerant data processing system originally developed by LinkedIn. It is designed for real-time streaming and processing of large scale data, allowing organizations to handle data in real-time with low latency.\n\nKafka uses a publish-subscribe model where data producers write data to Kafka topics, which are then consumed by data consumers. The system stores data in a partitioned and replicated manner, which provides reliability and scalability. The data stored in Kafka topics is retained for a configurable amount of time, providing a historical view of the data stream.\n\nOne of the key features of Apache Kafka is its ability to handle high volumes of data with low latency, making it suitable for use cases such as real-time analytics, fraud detection, and data integration. Additionally, Kafka provides strong durability guarantees, as data is replicated across multiple nodes and can be recovered in the event of failures.\n\nIn addition to its core functionality, Apache Kafka also provides a number of additional features such as security, data compression, and automatic data balancing. These features make it an attractive choice for organizations looking to process large amounts of real-time data, and it has been adopted by many organizations in a variety of industries, including finance, healthcare, and e-commerce.\n\n\n\n### Apache Flink\n\nApache Flink is an open-source, distributed stream processing framework designed to handle high-volume data processing in real-time. It supports batch processing and event-driven stream processing and can handle large-scale data processing across many parallel processing nodes.\n\nFlink was developed to address the limitations of existing stream processing frameworks, such as complex setup and configuration, limited scalability, and limited fault tolerance. It provides a high-level API for processing data in real-time, as well as a low-level API for implementing custom data processing logic.\n\nFlink's architecture is built around a data streaming model, with a central control system that manages the flow of data between different processing nodes. The framework supports a wide range of data sources, including Apache Kafka, Apache Cassandra, and Hadoop HDFS, and it can be integrated with other big data tools and technologies, such as Apache Hadoop and Apache Spark.\n\nOne of the key benefits of Flink is its ability to process and analyze data in real-time, without the need for batch processing. This enables organizations to make faster and more informed decisions based on the latest data, and to quickly respond to changing business conditions.\n\nIn addition, Flink provides a number of advanced features, such as windowing and state management, that enable it to handle complex data processing requirements, such as aggregating data over time windows and maintaining state information between events.\n\nOverall, Apache Flink is a powerful and flexible data processing framework that is well-suited for real-time data processing and analytics in big data environments.\n\n\n\n## Stream Processing\n\nStream Processing is a data processing paradigm in which data is processed in real-time as it is generated or received. In contrast to batch processing, where data is processed in large chunks at fixed intervals, stream processing allows for near-instant analysis of incoming data. This is especially useful for high-volume, high-velocity data, where traditional batch processing can quickly become overwhelmed.\n\nIn a stream processing system, incoming data is divided into small chunks, called events or records, which are processed one at a time in near real-time. This allows the system to detect and respond to events as soon as they occur, making it well-suited for use cases such as real-time monitoring, event-driven applications, and real-time analytics.\n\nStream processing systems typically include several key components, including data sources, stream processors, and data sinks. Data sources provide the incoming data stream, which is then processed by stream processors. Stream processors perform various operations on the incoming data, such as filtering, aggregating, and transforming, before finally sending the processed data to data sinks for storage or further processing.\n\n\n\n### Spark Streaming\n\nSpark Streaming is a real-time data processing framework for big data applications, built on top of the Apache Spark engine. It enables the processing of live data streams, making it possible to work with and analyze high-volume, high-velocity data from a variety of sources, including logs, sensors, social media, and financial data.\n\nSpark Streaming operates in near-real-time, with a batch processing interval as low as 100ms, and can handle millions of events per second. It provides a unified API for working with real-time data, making it possible to process both batch and real-time data in the same environment.\n\nSpark Streaming uses a fault-tolerant, scale-out architecture that supports high data processing performance. It also provides a wide range of built-in algorithms and libraries for machine learning, graph processing, and data analysis.\n\nSpark Streaming integrates with a variety of data sources and sinks, including Apache Kafka, Kinesis, Flume, and Cassandra, as well as Hadoop Distributed File System (HDFS). It also provides support for custom data sources and sinks, making it easy to integrate with new data sources as needed.\n\n\n\n## Interactive processing\n\nInteractive processing is a data processing technique that involves querying and analyzing data in real-time to support interactive decision-making. It is a contrast to batch processing, which processes data in large volumes after some delay. In interactive processing, data is queried and analyzed in real-time, often through OLAP (Online Analytical Processing) tools that enable users to slice and dice data in multiple dimensions.\n\nOLAP is a data processing technique that enables users to interactively analyze large datasets from multiple perspectives. OLAP tools provide a multidimensional view of data, allowing users to view data in different ways such as by time, geography, and product categories. OLAP tools allow users to drill down and aggregate data from the most granular level to the highest level, enabling users to gain insights into their data quickly.\n\nPresto is a distributed SQL query engine designed for interactive queries on large datasets. It was created by Facebook to support their massive data processing needs, including interactive analysis of petabytes of data. Presto can query data in multiple data sources, including Hadoop, Amazon S3, and traditional relational databases. It supports a wide range of SQL queries, including complex queries with multiple joins, subqueries, and aggregations.\n\nPresto is designed to work on a cluster of machines, making it easy to scale up and down as needed. It uses a cost-based optimizer to generate the best execution plan for each query, ensuring that queries are executed as efficiently as possible. Presto also supports caching of query results, which can significantly speed up subsequent queries that use the same data.\n\nInteractive processing, OLAP, and Presto are all critical components of modern data processing and analysis. They enable users to analyze large volumes of data quickly and easily, gaining insights into their data that were previously impossible. With the growing availability of massive datasets and the increasing need for real-time analysis, these technologies are becoming more important than ever.\n\n\n\n## Distributed computing frameworks\n\nDistributed computing frameworks are software frameworks used to build and run large-scale distributed applications, which are designed to process and analyze vast amounts of data across a distributed network of computers. Distributed computing frameworks have become essential for large-scale data processing, machine learning, and artificial intelligence applications that require high levels of parallelism, fault tolerance, and scalability.\n\nThe primary goal of distributed computing frameworks is to provide a transparent and consistent view of a distributed system by abstracting the complexity of distributed computing and providing a high-level programming model to developers. They provide a layer of abstraction to shield the developer from the underlying complexities of the distributed computing environment, such as data partitioning, replication, fault tolerance, and inter-node communication.\n\nSome popular examples of distributed computing frameworks include Apache Hadoop, Apache Spark, and Apache Flink. These frameworks provide a rich set of APIs for distributed data processing, including MapReduce, stream processing, batch processing, and interactive querying.\n\nDistributed computing frameworks are designed to work with commodity hardware, which means that they can scale horizontally by adding more machines to the cluster, making them highly cost-effective. This allows companies to process and analyze vast amounts of data without the need for expensive, proprietary hardware.\n\nIn summary, distributed computing frameworks are essential for large-scale data processing, machine learning, and artificial intelligence applications that require high levels of parallelism, fault tolerance, and scalability. They provide a layer of abstraction to shield developers from the complexities of distributed computing and offer a cost-effective way to process and analyze vast amounts of data.\n\n\n\n","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/Modern-Data-Stack-by-ChatGPT/What-is-a-modern-data-stack":{"title":"","content":"","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/data-system":{"title":"Data System","content":"\n\n\n# Data System\n\n## Data Exploring \u0026 Visualization\n\n* [Hands on Zeppelin](notes/hands-on-zeppelin.md)\n* [[hands-on-zeppelin]]","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/devops":{"title":"DevOps","content":"\n# DevOps\n\n## Docker\n\n* [Dockerå…¥é—¨ï¼šstep by step](notes/hands-on-docker_1.md)\n* [Dockerè¿›é˜¶ï¼šstep by step](notes/hands-on-docker_2.md)","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/hands-on-docker_1":{"title":"Dockerå…¥é—¨","content":"\n\n\n# Dockerå…¥é—¨ï¼šstep by step\n\n\u003e ä¸ºä¸Šæ‰‹å­¦ä¹ Dockerï¼Œæˆ‘å‚ç…§é˜®ä¸€å³°è€å¸ˆå†™çš„æ•™ç¨‹ï¼Œstep by stepåœ°åŠ¨æ‰‹å®è·µã€‚æ­¤æ–‡è®°å½•ã€‚\n\n## å‚è€ƒ\n\n[é˜®ä¸€å³°ï¼šDockerå…¥é—¨æ•™ç¨‹](https://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html)\n\nè¿›ä¸€æ­¥å®è·µï¼Œè¯·ç§»æ­¥ï¼š\n\n* [Dockerè¿›é˜¶ï¼šstep by step](notes/hands-on-docker_2.md)\n\n## ä¸€ã€ç¯å¢ƒé…ç½®çš„éš¾é¢˜\n\nè½¯ä»¶å¼€å‘æœ€å¤§çš„éº»çƒ¦ä¹‹ä¸€å°±æ˜¯ç¯å¢ƒé…ç½®ã€‚æ¯ä¸ªäººçš„æœºå™¨çš„ç³»ç»Ÿé…ç½®éƒ½ä¸ä¸€æ ·ï¼ŒåŒ…æ‹¬æ“ä½œç³»ç»Ÿã€å„ç§åº“ä¸ç»„ä»¶ã€ç¯å¢ƒå˜é‡ç­‰ç­‰ã€‚è½¯ä»¶+è¿è¡Œç¯å¢ƒï¼Œæ•´ä½“å‘å¸ƒæ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„æ€è·¯ã€‚Dockekæ˜¯è¿™ç§æ€è·¯çš„å®ç°ã€‚\n\n## äºŒã€è™šæ‹Ÿæœº\n\nåœ¨Dockerä¹‹å‰ï¼Œè™šæ‹Ÿæœºæ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚ä½†å…¶ç¼ºç‚¹æ˜æ˜¾ï¼ŒåŒ…æ‹¬ï¼š\n\n- **èµ„æºå ç”¨å¤š**ã€‚è™šæ‹Ÿæœºä¼šç‹¬å ä¸€éƒ¨åˆ†å†…å­˜å’Œç¡¬ç›˜ç©ºé—´ã€‚å®ƒè¿è¡Œçš„æ—¶å€™ï¼Œå…¶ä»–ç¨‹åºå°±ä¸èƒ½ä½¿ç”¨è¿™äº›èµ„æºäº†ã€‚å“ªæ€•è™šæ‹Ÿæœºé‡Œé¢çš„åº”ç”¨ç¨‹åºï¼ŒçœŸæ­£ä½¿ç”¨çš„å†…å­˜åªæœ‰ 1MBï¼Œè™šæ‹Ÿæœºä¾ç„¶éœ€è¦å‡ ç™¾ MB çš„å†…å­˜æ‰èƒ½è¿è¡Œã€‚\n\n- **å†—ä½™æ­¥éª¤å¤š**ã€‚è™šæ‹Ÿæœºæ˜¯å®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œä¸€äº›ç³»ç»Ÿçº§åˆ«çš„æ“ä½œæ­¥éª¤ï¼Œå¾€å¾€æ— æ³•è·³è¿‡ï¼Œæ¯”å¦‚ç”¨æˆ·ç™»å½•ã€‚\n\n- **å¯åŠ¨æ…¢ã€‚**å¯åŠ¨æ“ä½œç³»ç»Ÿéœ€è¦å¤šä¹…ï¼Œå¯åŠ¨è™šæ‹Ÿæœºå°±éœ€è¦å¤šä¹…ã€‚å¯èƒ½è¦ç­‰å‡ åˆ†é’Ÿï¼Œåº”ç”¨ç¨‹åºæ‰èƒ½çœŸæ­£è¿è¡Œã€‚\n\n## ä¸‰ã€Linuxå®¹å™¨\n\nLinuxå®¹å™¨ï¼ˆLinux Containerï¼Œ LXCï¼‰æ˜¯å¦ä¸€ç§è™šæ‹ŸåŒ–æŠ€æœ¯ã€‚\n\n**Linux å®¹å™¨ä¸æ˜¯æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œè€Œæ˜¯å¯¹è¿›ç¨‹è¿›è¡Œéš”ç¦»ã€‚**æˆ–è€…è¯´ï¼Œåœ¨æ­£å¸¸è¿›ç¨‹çš„å¤–é¢å¥—äº†ä¸€ä¸ª[ä¿æŠ¤å±‚](https://opensource.com/article/18/1/history-low-level-container-runtimes)ã€‚å¯¹äºå®¹å™¨é‡Œé¢çš„è¿›ç¨‹æ¥è¯´ï¼Œå®ƒæ¥è§¦åˆ°çš„å„ç§èµ„æºéƒ½æ˜¯è™šæ‹Ÿçš„ï¼Œä»è€Œå®ç°ä¸åº•å±‚ç³»ç»Ÿçš„éš”ç¦»ã€‚\n\nç”±äºå®¹å™¨æ˜¯è¿›ç¨‹çº§åˆ«çš„ï¼Œç›¸æ¯”è™šæ‹Ÿæœºæœ‰å¾ˆå¤šä¼˜åŠ¿ã€‚\n\n- **å¯åŠ¨å¿«ã€‚**å®¹å™¨é‡Œé¢çš„åº”ç”¨ï¼Œç›´æ¥å°±æ˜¯åº•å±‚ç³»ç»Ÿçš„ä¸€ä¸ªè¿›ç¨‹ï¼Œè€Œä¸æ˜¯è™šæ‹Ÿæœºå†…éƒ¨çš„è¿›ç¨‹ã€‚æ‰€ä»¥ï¼Œå¯åŠ¨å®¹å™¨ç›¸å½“äºå¯åŠ¨æœ¬æœºçš„ä¸€ä¸ªè¿›ç¨‹ï¼Œè€Œä¸æ˜¯å¯åŠ¨ä¸€ä¸ªæ“ä½œç³»ç»Ÿï¼Œé€Ÿåº¦å°±å¿«å¾ˆå¤šã€‚\n\n- **èµ„æºå ç”¨å°‘ã€‚**å®¹å™¨åªå ç”¨éœ€è¦çš„èµ„æºï¼Œä¸å ç”¨é‚£äº›æ²¡æœ‰ç”¨åˆ°çš„èµ„æºï¼›è™šæ‹Ÿæœºç”±äºæ˜¯å®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œä¸å¯é¿å…è¦å ç”¨æ‰€æœ‰èµ„æºã€‚å¦å¤–ï¼Œå¤šä¸ªå®¹å™¨å¯ä»¥å…±äº«èµ„æºï¼Œè™šæ‹Ÿæœºéƒ½æ˜¯ç‹¬äº«èµ„æºã€‚\n\n- **ä½“ç§¯å°ã€‚**å®¹å™¨åªè¦åŒ…å«ç”¨åˆ°çš„ç»„ä»¶å³å¯ï¼Œè€Œè™šæ‹Ÿæœºæ˜¯æ•´ä¸ªæ“ä½œç³»ç»Ÿçš„æ‰“åŒ…ï¼Œæ‰€ä»¥å®¹å™¨æ–‡ä»¶æ¯”è™šæ‹Ÿæœºæ–‡ä»¶è¦å°å¾ˆå¤šã€‚\n\næ€»ä¹‹ï¼Œå®¹å™¨æœ‰ç‚¹åƒè½»é‡çº§çš„è™šæ‹Ÿæœºï¼Œèƒ½å¤Ÿæä¾›è™šæ‹ŸåŒ–çš„ç¯å¢ƒï¼Œä½†æ˜¯æˆæœ¬å¼€é”€å°å¾—å¤šã€‚\n\n## å››ã€Dockeræ˜¯ä»€ä¹ˆï¼Ÿ\n\n**Docker å±äº Linux å®¹å™¨çš„ä¸€ç§å°è£…ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„å®¹å™¨ä½¿ç”¨æ¥å£ã€‚**å®ƒæ˜¯ç›®å‰æœ€æµè¡Œçš„ Linux å®¹å™¨è§£å†³æ–¹æ¡ˆã€‚\n\nDocker å°†åº”ç”¨ç¨‹åºä¸è¯¥ç¨‹åºçš„ä¾èµ–ï¼Œæ‰“åŒ…åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œé¢ã€‚è¿è¡Œè¿™ä¸ªæ–‡ä»¶ï¼Œå°±ä¼šç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿå®¹å™¨ã€‚ç¨‹åºåœ¨è¿™ä¸ªè™šæ‹Ÿå®¹å™¨é‡Œè¿è¡Œï¼Œå°±å¥½åƒåœ¨çœŸå®çš„ç‰©ç†æœºä¸Šè¿è¡Œä¸€æ ·ã€‚æœ‰äº† Dockerï¼Œå°±ä¸ç”¨æ‹…å¿ƒç¯å¢ƒé—®é¢˜ã€‚\n\næ€»ä½“æ¥è¯´ï¼ŒDocker çš„æ¥å£ç›¸å½“ç®€å•ï¼Œç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°åˆ›å»ºå’Œä½¿ç”¨å®¹å™¨ï¼ŒæŠŠè‡ªå·±çš„åº”ç”¨æ”¾å…¥å®¹å™¨ã€‚å®¹å™¨è¿˜å¯ä»¥è¿›è¡Œç‰ˆæœ¬ç®¡ç†ã€å¤åˆ¶ã€åˆ†äº«ã€ä¿®æ”¹ï¼Œå°±åƒç®¡ç†æ™®é€šçš„ä»£ç ä¸€æ ·ã€‚\n\n## äº”ã€Dockerçš„ç”¨é€”\n\nDocker çš„ä¸»è¦ç”¨é€”ï¼Œç›®å‰æœ‰ä¸‰å¤§ç±»ã€‚\n\n- **æä¾›ä¸€æ¬¡æ€§çš„ç¯å¢ƒã€‚**æ¯”å¦‚ï¼Œæœ¬åœ°æµ‹è¯•ä»–äººçš„è½¯ä»¶ã€æŒç»­é›†æˆçš„æ—¶å€™æä¾›å•å…ƒæµ‹è¯•å’Œæ„å»ºçš„ç¯å¢ƒã€‚\n\n- **æä¾›å¼¹æ€§çš„äº‘æœåŠ¡ã€‚**å› ä¸º Docker å®¹å™¨å¯ä»¥éšå¼€éšå…³ï¼Œå¾ˆé€‚åˆåŠ¨æ€æ‰©å®¹å’Œç¼©å®¹ã€‚\n\n- **ç»„å»ºå¾®æœåŠ¡æ¶æ„ã€‚**é€šè¿‡å¤šä¸ªå®¹å™¨ï¼Œä¸€å°æœºå™¨å¯ä»¥è·‘å¤šä¸ªæœåŠ¡ï¼Œå› æ­¤åœ¨æœ¬æœºå°±å¯ä»¥æ¨¡æ‹Ÿå‡ºå¾®æœåŠ¡æ¶æ„ã€‚\n\n## å…­ã€Docker çš„å®‰è£…\n\nDocker æ˜¯ä¸€ä¸ªå¼€æºçš„å•†ä¸šäº§å“ï¼Œæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šç¤¾åŒºç‰ˆï¼ˆCommunity Editionï¼Œç¼©å†™ä¸º CEï¼‰å’Œä¼ä¸šç‰ˆï¼ˆEnterprise Editionï¼Œç¼©å†™ä¸º EEï¼‰ã€‚ä¼ä¸šç‰ˆåŒ…å«äº†ä¸€äº›æ”¶è´¹æœåŠ¡ï¼Œä¸ªäººå¼€å‘è€…ä¸€èˆ¬ç”¨ä¸åˆ°ã€‚ä¸‹é¢çš„ä»‹ç»éƒ½é’ˆå¯¹ç¤¾åŒºç‰ˆã€‚\n\nDocker CE çš„å®‰è£…è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚\n\n\u003e - [Mac](https://docs.docker.com/docker-for-mac/install/)\n\u003e\n\u003e - [Windows](https://docs.docker.com/docker-for-windows/install/)\n\u003e\n\u003e - [Ubuntu](https://docs.docker.com/install/linux/docker-ce/ubuntu/)\n\u003e\n\u003e - ã€‚ã€‚ã€‚\n\nå®‰è£…å®Œæˆåï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼ŒéªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸã€‚\n\n```Bash\n$ docker version\n# æˆ–è€…\n$ docker info\n```\n\nDocker éœ€è¦ç”¨æˆ·å…·æœ‰ sudo æƒé™ï¼Œä¸ºäº†é¿å…æ¯æ¬¡å‘½ä»¤éƒ½è¾“å…¥`sudo`ï¼Œå¯ä»¥æŠŠç”¨æˆ·åŠ å…¥ Docker ç”¨æˆ·ç»„ï¼ˆ[å®˜æ–¹æ–‡æ¡£](https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user)ï¼‰ã€‚\n\n```Bash\nsudo usermod -aG docker $USER\n# éœ€è¦log out æ‰èƒ½ç”Ÿæ•ˆï¼Œä¸è®©docker infoå‘½ä»¤çš„Severéƒ¨åˆ†ä¼šæŠ¥é”™\n# è¯¦æƒ…è§ï¼šhttps://docs.docker.com/engine/install/linux-postinstall/\n```\n\nDocker æ˜¯æœåŠ¡å™¨----å®¢æˆ·ç«¯æ¶æ„ã€‚å‘½ä»¤è¡Œè¿è¡Œ`docker`å‘½ä»¤çš„æ—¶å€™ï¼Œéœ€è¦æœ¬æœºæœ‰ Docker æœåŠ¡ã€‚å¦‚æœè¿™é¡¹æœåŠ¡æ²¡æœ‰å¯åŠ¨ï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„å‘½ä»¤å¯åŠ¨ï¼ˆ[å®˜æ–¹æ–‡æ¡£](https://docs.docker.com/config/daemon/systemd/)ï¼‰ã€‚\n\n```Bash\n# service å‘½ä»¤çš„ç”¨æ³•\n$ sudo service docker start\n\n# systemctl å‘½ä»¤çš„ç”¨æ³•\n$ sudo systemctl start docker\n```\n\nå®‰è£…æˆåŠŸåï¼Œå¯ä»¥\n\n```bash\ndocker run hello-world\n```\n\n![image-20221214173436747](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214173436747.png)\n\n```\ndocker run -it ubuntu bash\n```\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/dcc8d661-5b86-4c2b-9cc3-e50e6a34719e.jpeg\" alt=\"dcc8d661-5b86-4c2b-9cc3-e50e6a34719e\" style=\"zoom:50%;\" /\u003e\n\nå…¶ä¸­ä¸‹è½½çš„imageå¤§å°æ˜¯30MB\n\n\u003e $ docker image ls\n\u003e\n\u003e REPOSITORY    TAG       IMAGE ID       CREATED         SIZE\n\u003e\n\u003e ubuntu        latest    df5de72bdb3b   2 weeks ago     77.8MB\n\u003e\n\u003e hello-world   latest    feb5d9fea6a5   11 months ago   13.3kB\n\n## ä¸ƒã€image æ–‡ä»¶\n\n**Docker æŠŠåº”ç”¨ç¨‹åºåŠå…¶ä¾èµ–ï¼Œæ‰“åŒ…åœ¨ image æ–‡ä»¶é‡Œé¢ã€‚**åªæœ‰é€šè¿‡è¿™ä¸ªæ–‡ä»¶ï¼Œæ‰èƒ½ç”Ÿæˆ Docker å®¹å™¨ã€‚image æ–‡ä»¶å¯ä»¥çœ‹ä½œæ˜¯å®¹å™¨çš„æ¨¡æ¿ã€‚Docker æ ¹æ® image æ–‡ä»¶ç”Ÿæˆå®¹å™¨çš„å®ä¾‹ã€‚åŒä¸€ä¸ª image æ–‡ä»¶ï¼Œå¯ä»¥ç”Ÿæˆå¤šä¸ªåŒæ—¶è¿è¡Œçš„å®¹å™¨å®ä¾‹ã€‚\n\nimage æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ã€‚å®é™…å¼€å‘ä¸­ï¼Œä¸€ä¸ª image æ–‡ä»¶å¾€å¾€é€šè¿‡ç»§æ‰¿å¦ä¸€ä¸ª image æ–‡ä»¶ï¼ŒåŠ ä¸Šä¸€äº›ä¸ªæ€§åŒ–è®¾ç½®è€Œç”Ÿæˆã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œä½ å¯ä»¥åœ¨ Ubuntu çš„ image åŸºç¡€ä¸Šï¼Œå¾€é‡Œé¢åŠ å…¥ Apache æœåŠ¡å™¨ï¼Œå½¢æˆä½ çš„ imageã€‚\n\nimage æ–‡ä»¶æ˜¯é€šç”¨çš„ï¼Œä¸€å°æœºå™¨çš„ image æ–‡ä»¶æ‹·è´åˆ°å¦ä¸€å°æœºå™¨ï¼Œç…§æ ·å¯ä»¥ä½¿ç”¨ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œæˆ‘ä»¬åº”è¯¥å°½é‡ä½¿ç”¨åˆ«äººåˆ¶ä½œå¥½çš„ image æ–‡ä»¶ï¼Œè€Œä¸æ˜¯è‡ªå·±åˆ¶ä½œã€‚å³ä½¿è¦å®šåˆ¶ï¼Œä¹Ÿåº”è¯¥åŸºäºåˆ«äººçš„ image æ–‡ä»¶è¿›è¡ŒåŠ å·¥ï¼Œè€Œä¸æ˜¯ä»é›¶å¼€å§‹åˆ¶ä½œã€‚\n\nä¸ºäº†æ–¹ä¾¿å…±äº«ï¼Œimage æ–‡ä»¶åˆ¶ä½œå®Œæˆåï¼Œå¯ä»¥ä¸Šä¼ åˆ°ç½‘ä¸Šçš„ä»“åº“ã€‚Docker çš„å®˜æ–¹ä»“åº“ [Docker Hub](https://hub.docker.com/) æ˜¯æœ€é‡è¦ã€æœ€å¸¸ç”¨çš„ image ä»“åº“ã€‚æ­¤å¤–ï¼Œå‡ºå”®è‡ªå·±åˆ¶ä½œçš„ image æ–‡ä»¶ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚\n\n## å…«ã€å®ä¾‹ï¼šhello world\n\nä¸‹é¢ï¼Œæˆ‘ä»¬é€šè¿‡æœ€ç®€å•çš„ image æ–‡ä»¶\"[hello world\"](https://hub.docker.com/r/library/hello-world/)ï¼Œæ„Ÿå—ä¸€ä¸‹ Dockerã€‚\n\néœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå›½å†…è¿æ¥ Docker çš„å®˜æ–¹ä»“åº“å¾ˆæ…¢ï¼Œè¿˜ä¼šæ–­çº¿ï¼Œéœ€è¦å°†é»˜è®¤ä»“åº“æ”¹æˆå›½å†…çš„é•œåƒç½‘ç«™ï¼Œå…·ä½“çš„ä¿®æ”¹æ–¹æ³•åœ¨[ä¸‹ä¸€ç¯‡æ–‡ç« ](https://www.ruanyifeng.com/blog/2018/02/docker-wordpress-tutorial.html)çš„ç¬¬ä¸€èŠ‚ã€‚æœ‰éœ€è¦çš„æœ‹å‹ï¼Œå¯ä»¥å…ˆçœ‹ä¸€ä¸‹ã€‚\n\né¦–å…ˆï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œå°† image æ–‡ä»¶ä»ä»“åº“æŠ“å–åˆ°æœ¬åœ°ã€‚\n\n```Bash\n$ docker image pull library/hello-world\n```\n\nä¸Šé¢ä»£ç ä¸­ï¼Œ`docker image pull`æ˜¯æŠ“å– image æ–‡ä»¶çš„å‘½ä»¤ã€‚`library/hello-world`æ˜¯ image æ–‡ä»¶åœ¨ä»“åº“é‡Œé¢çš„ä½ç½®ï¼Œå…¶ä¸­`library`æ˜¯ image æ–‡ä»¶æ‰€åœ¨çš„ç»„ï¼Œ`hello-world`æ˜¯ image æ–‡ä»¶çš„åå­—ã€‚\n\nç”±äº Docker å®˜æ–¹æä¾›çš„ image æ–‡ä»¶ï¼Œéƒ½æ”¾åœ¨`library`ç»„é‡Œé¢ï¼Œæ‰€ä»¥å®ƒçš„æ˜¯é»˜è®¤ç»„ï¼Œå¯ä»¥çœç•¥ã€‚å› æ­¤ï¼Œä¸Šé¢çš„å‘½ä»¤å¯ä»¥å†™æˆä¸‹é¢è¿™æ ·ã€‚\n\n```Bash\n$ docker image pull hello-world\n```\n\næŠ“å–æˆåŠŸä»¥åï¼Œå°±å¯ä»¥åœ¨æœ¬æœºçœ‹åˆ°è¿™ä¸ª image æ–‡ä»¶äº†ã€‚\n\n```Bash\n$ docker image ls\n```\n\nç°åœ¨ï¼Œè¿è¡Œè¿™ä¸ª image æ–‡ä»¶ã€‚\n\n```Bash\n$ docker container run hello-world\n```\n\n`docker container run`å‘½ä»¤ä¼šä» image æ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„å®¹å™¨å®ä¾‹ã€‚\n\næ³¨æ„ï¼Œ`docker container run`å‘½ä»¤å…·æœ‰è‡ªåŠ¨æŠ“å– image æ–‡ä»¶çš„åŠŸèƒ½ã€‚å¦‚æœå‘ç°æœ¬åœ°æ²¡æœ‰æŒ‡å®šçš„ image æ–‡ä»¶ï¼Œå°±ä¼šä»ä»“åº“è‡ªåŠ¨æŠ“å–ã€‚å› æ­¤ï¼Œå‰é¢çš„`docker image pull`å‘½ä»¤å¹¶ä¸æ˜¯å¿…éœ€çš„æ­¥éª¤ã€‚\n\nå¦‚æœè¿è¡ŒæˆåŠŸï¼Œä½ ä¼šåœ¨å±å¹•ä¸Šè¯»åˆ°ä¸‹é¢çš„è¾“å‡ºã€‚\n\n```Bash\n$ docker container run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.... ...\n```\n\nè¾“å‡ºè¿™æ®µæç¤ºä»¥åï¼Œ`hello world`å°±ä¼šåœæ­¢è¿è¡Œï¼Œå®¹å™¨è‡ªåŠ¨ç»ˆæ­¢ã€‚\n\næœ‰äº›å®¹å™¨ä¸ä¼šè‡ªåŠ¨ç»ˆæ­¢ï¼Œå› ä¸ºæä¾›çš„æ˜¯æœåŠ¡ã€‚æ¯”å¦‚ï¼Œå®‰è£…è¿è¡Œ Ubuntu çš„ imageï¼Œå°±å¯ä»¥åœ¨å‘½ä»¤è¡Œä½“éªŒ Ubuntu ç³»ç»Ÿã€‚\n\n```Bash\n$ docker container run -it ubuntu bash\n```\n\nå¯¹äºé‚£äº›ä¸ä¼šè‡ªåŠ¨ç»ˆæ­¢çš„å®¹å™¨ï¼Œå¿…é¡»ä½¿ç”¨`docker container kill` å‘½ä»¤æ‰‹åŠ¨ç»ˆæ­¢ã€‚\n\n```Bash\n$ docker container kill [containID]\n\n## å¦‚ä¸‹ä¾‹å­\n$ docker container ls\nCONTAINER ID   IMAGE     COMMAND   CREATED          STATUS          PORTS     NAMES\nf45ef5fcc683   ubuntu    \"bash\"    21 minutes ago   Up 21 minutes             charming_montalcini\n\n# happy3 @ happy3-HX in ~ [15:46:39] \n$ docker kill f45ef5fcc683\nf45ef5fcc683\n\n# happy3 @ happy3-HX in ~ [15:46:47] \n$ docker container ls     \nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n# happy3 @ happy3-HX in ~ [15:46:54] \n$ \n```\n\n## ä¹ã€å®¹å™¨æ–‡ä»¶\n\n**image æ–‡ä»¶ç”Ÿæˆçš„å®¹å™¨å®ä¾‹ï¼Œæœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œç§°ä¸ºå®¹å™¨æ–‡ä»¶ã€‚**ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ—¦å®¹å™¨ç”Ÿæˆï¼Œå°±ä¼šåŒæ—¶å­˜åœ¨ä¸¤ä¸ªæ–‡ä»¶ï¼š image æ–‡ä»¶å’Œå®¹å™¨æ–‡ä»¶ã€‚è€Œä¸”å…³é—­å®¹å™¨å¹¶ä¸ä¼šåˆ é™¤å®¹å™¨æ–‡ä»¶ï¼Œåªæ˜¯å®¹å™¨åœæ­¢è¿è¡Œè€Œå·²ã€‚\n\nä¸Šé¢å‘½ä»¤çš„è¾“å‡ºç»“æœä¹‹ä¸­ï¼ŒåŒ…æ‹¬å®¹å™¨çš„ IDã€‚å¾ˆå¤šåœ°æ–¹éƒ½éœ€è¦æä¾›è¿™ä¸ª IDï¼Œæ¯”å¦‚ä¸Šä¸€èŠ‚ç»ˆæ­¢å®¹å™¨è¿è¡Œçš„`docker container kill`å‘½ä»¤ã€‚\n\nç»ˆæ­¢è¿è¡Œçš„å®¹å™¨æ–‡ä»¶ï¼Œä¾ç„¶ä¼šå æ®ç¡¬ç›˜ç©ºé—´ï¼Œå¯ä»¥ä½¿ç”¨`docker container rm`å‘½ä»¤åˆ é™¤ã€‚\n\n```Bash\n$ docker container rm [containerID]\n```\n\nè¿è¡Œä¸Šé¢çš„å‘½ä»¤ä¹‹åï¼Œå†ä½¿ç”¨`docker container ls --all`å‘½ä»¤ï¼Œå°±ä¼šå‘ç°è¢«åˆ é™¤çš„å®¹å™¨æ–‡ä»¶å·²ç»æ¶ˆå¤±äº†ã€‚\n\n## åã€Dockerfile æ–‡ä»¶\n\nå­¦ä¼šä½¿ç”¨ image æ–‡ä»¶ä»¥åï¼Œæ¥ä¸‹æ¥çš„é—®é¢˜å°±æ˜¯ï¼Œå¦‚ä½•å¯ä»¥ç”Ÿæˆ image æ–‡ä»¶ï¼Ÿå¦‚æœä½ è¦æ¨å¹¿è‡ªå·±çš„è½¯ä»¶ï¼ŒåŠ¿å¿…è¦è‡ªå·±åˆ¶ä½œ image æ–‡ä»¶ã€‚\n\nè¿™å°±éœ€è¦ç”¨åˆ° Dockerfile æ–‡ä»¶ã€‚å®ƒæ˜¯ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œç”¨æ¥é…ç½® imageã€‚Docker æ ¹æ® è¯¥æ–‡ä»¶ç”ŸæˆäºŒè¿›åˆ¶çš„ image æ–‡ä»¶ã€‚\n\nä¸‹é¢é€šè¿‡ä¸€ä¸ªå®ä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ç¼–å†™ Dockerfile æ–‡ä»¶ã€‚\n\n## åä¸€ã€å®ä¾‹ï¼šåˆ¶ä½œè‡ªå·±çš„ Docker å®¹å™¨\n\nä¸‹é¢æˆ‘ä»¥ [koa-demos](https://www.ruanyifeng.com/blog/2017/08/koa.html) é¡¹ç›®ä¸ºä¾‹ï¼Œä»‹ç»æ€ä¹ˆå†™ Dockerfile æ–‡ä»¶ï¼Œå®ç°è®©ç”¨æˆ·åœ¨ Docker å®¹å™¨é‡Œé¢è¿è¡Œ Koa æ¡†æ¶ã€‚\n\nä½œä¸ºå‡†å¤‡å·¥ä½œï¼Œè¯·å…ˆ[ä¸‹è½½æºç ](https://github.com/ruanyf/koa-demos/archive/master.zip)ã€‚\n\n```Bash\n$ git clone https://github.com/ruanyf/koa-demos.git\n$ cd koa-demos\n```\n\n#### 11.1 ç¼–å†™ Dockerfile æ–‡ä»¶\n\né¦–å…ˆï¼Œåœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶`.dockerignore`ï¼Œå†™å…¥ä¸‹é¢çš„[å†…iå®¹](https://github.com/ruanyf/koa-demos/blob/master/.dockerignore)ã€‚\n\n```Bash\n.git\nnode_modules\nnpm-debug.log\n```\n\nä¸Šé¢ä»£ç è¡¨ç¤ºï¼Œè¿™ä¸‰ä¸ªè·¯å¾„è¦æ’é™¤ï¼Œä¸è¦æ‰“åŒ…è¿›å…¥ image æ–‡ä»¶ã€‚å¦‚æœä½ æ²¡æœ‰è·¯å¾„è¦æ’é™¤ï¼Œè¿™ä¸ªæ–‡ä»¶å¯ä»¥ä¸æ–°å»ºã€‚\n\nç„¶åï¼Œåœ¨é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ Dockerfileï¼Œå†™å…¥ä¸‹é¢çš„[å†…å®¹](https://github.com/ruanyf/koa-demos/blob/master/Dockerfile)ã€‚\n\n```Bash\nFROM node:8.4\nCOPY . /app\nWORKDIR /app\nRUN npm install --registry=https://registry.npm.taobao.org\nEXPOSE 3000\n```\n\nä¸Šé¢ä»£ç ä¸€å…±äº”è¡Œï¼Œå«ä¹‰å¦‚ä¸‹ã€‚\n\n\u003e - `FROM node:8.4`ï¼šè¯¥ image æ–‡ä»¶ç»§æ‰¿å®˜æ–¹çš„ node imageï¼Œå†’å·è¡¨ç¤ºæ ‡ç­¾ï¼Œè¿™é‡Œæ ‡ç­¾æ˜¯`8.4`ï¼Œå³8.4ç‰ˆæœ¬çš„ nodeã€‚\n\u003e\n\u003e - `COPY . /app`ï¼šå°†å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆé™¤äº†`.dockerignore`æ’é™¤çš„è·¯å¾„ï¼‰ï¼Œéƒ½æ‹·è´è¿›å…¥ image æ–‡ä»¶çš„`/app`ç›®å½•ã€‚\n\u003e\n\u003e - `WORKDIR /app`ï¼šæŒ‡å®šæ¥ä¸‹æ¥çš„å·¥ä½œè·¯å¾„ä¸º`/app`ã€‚\n\u003e\n\u003e - `RUN npm install`ï¼šåœ¨`/app`ç›®å½•ä¸‹ï¼Œè¿è¡Œ`npm install`å‘½ä»¤å®‰è£…ä¾èµ–ã€‚æ³¨æ„ï¼Œå®‰è£…åæ‰€æœ‰çš„ä¾èµ–ï¼Œéƒ½å°†æ‰“åŒ…è¿›å…¥ image æ–‡ä»¶ã€‚\n\u003e\n\u003e - `EXPOSE 3000`ï¼šå°†å®¹å™¨ 3000 ç«¯å£æš´éœ²å‡ºæ¥ï¼Œ å…è®¸å¤–éƒ¨è¿æ¥è¿™ä¸ªç«¯å£ã€‚\n\n#### 11.2 åˆ›å»º image æ–‡ä»¶\n\næœ‰äº† Dockerfile æ–‡ä»¶ä»¥åï¼Œå°±å¯ä»¥ä½¿ç”¨`docker image build`å‘½ä»¤åˆ›å»º image æ–‡ä»¶äº†ã€‚\n\n```Bash\n$ docker image build -t koa-demo .\n# æˆ–è€…\n$ docker image build -t koa-demo:0.0.1 .\n```\n\nä¸Šé¢ä»£ç ä¸­ï¼Œ`-t`å‚æ•°ç”¨æ¥æŒ‡å®š image æ–‡ä»¶çš„åå­—ï¼Œåé¢è¿˜å¯ä»¥ç”¨å†’å·æŒ‡å®šæ ‡ç­¾ã€‚å¦‚æœä¸æŒ‡å®šï¼Œé»˜è®¤çš„æ ‡ç­¾å°±æ˜¯`latest`ã€‚æœ€åçš„é‚£ä¸ªç‚¹è¡¨ç¤º Dockerfile æ–‡ä»¶æ‰€åœ¨çš„è·¯å¾„ï¼Œä¸Šä¾‹æ˜¯å½“å‰è·¯å¾„ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªç‚¹ã€‚\n\nå¦‚æœè¿è¡ŒæˆåŠŸï¼Œå°±å¯ä»¥çœ‹åˆ°æ–°ç”Ÿæˆçš„ image æ–‡ä»¶`koa-demo`äº†ã€‚\n\n```Bash\n$ docker image ls\n```\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/2c593f99-bc20-45bf-b5df-01300fa5ef4d.jpeg\" alt=\"2c593f99-bc20-45bf-b5df-01300fa5ef4d\" style=\"zoom:50%;\" /\u003e\n\n#### 11.3 ç”Ÿæˆå®¹å™¨\n\n`docker container run`å‘½ä»¤ä¼šä» image æ–‡ä»¶ç”Ÿæˆå®¹å™¨ã€‚\n\n```Bash\n$ docker container run -p 8000:3000 -it koa-demo /bin/bash\n# æˆ–è€…\n$ docker container run -p 8000:3000 -it koa-demo:0.0.1 /bin/bash\n```\n\nä¸Šé¢å‘½ä»¤çš„å„ä¸ªå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š\n\n\u003e - `-p`å‚æ•°ï¼šå®¹å™¨çš„ 3000 ç«¯å£æ˜ å°„åˆ°æœ¬æœºçš„ 8000 ç«¯å£ã€‚\n\u003e\n\u003e - `-it`å‚æ•°ï¼šå®¹å™¨çš„ Shell æ˜ å°„åˆ°å½“å‰çš„ Shellï¼Œç„¶åä½ åœ¨æœ¬æœºçª—å£è¾“å…¥çš„å‘½ä»¤ï¼Œå°±ä¼šä¼ å…¥å®¹å™¨ã€‚\n\u003e\n\u003e - `koa-demo:0.0.1`ï¼šimage æ–‡ä»¶çš„åå­—ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼Œè¿˜éœ€è¦æä¾›æ ‡ç­¾ï¼Œé»˜è®¤æ˜¯ latest æ ‡ç­¾ï¼‰ã€‚\n\u003e\n\u003e - `/bin/bash`ï¼šå®¹å™¨å¯åŠ¨ä»¥åï¼Œå†…éƒ¨ç¬¬ä¸€ä¸ªæ‰§è¡Œçš„å‘½ä»¤ã€‚è¿™é‡Œæ˜¯å¯åŠ¨ Bashï¼Œä¿è¯ç”¨æˆ·å¯ä»¥ä½¿ç”¨ Shellã€‚\n\nå¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¿è¡Œä¸Šé¢çš„å‘½ä»¤ä»¥åï¼Œå°±ä¼šè¿”å›ä¸€ä¸ªå‘½ä»¤è¡Œæç¤ºç¬¦ã€‚\n\n```Bash\nroot@66d80f4aaf1e:/app#\n```\n\nè¿™è¡¨ç¤ºä½ å·²ç»åœ¨å®¹å™¨é‡Œé¢äº†ï¼Œè¿”å›çš„æç¤ºç¬¦å°±æ˜¯å®¹å™¨å†…éƒ¨çš„ Shell æç¤ºç¬¦ã€‚æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚\n\n```Bash\nroot@66d80f4aaf1e:/app# node demos/01.js\n```\n\nè¿™æ—¶ï¼ŒKoa æ¡†æ¶å·²ç»è¿è¡Œèµ·æ¥äº†ã€‚æ‰“å¼€æœ¬æœºçš„æµè§ˆå™¨ï¼Œè®¿é—® http://127.0.0.1:8000ï¼Œç½‘é¡µæ˜¾ç¤º\"Not Found\"ï¼Œè¿™æ˜¯å› ä¸ºè¿™ä¸ª [demo](https://github.com/ruanyf/koa-demos/blob/master/demos/01.js) æ²¡æœ‰å†™è·¯ç”±ã€‚\n\nè¿™ä¸ªä¾‹å­ä¸­ï¼ŒNode è¿›ç¨‹è¿è¡Œåœ¨ Docker å®¹å™¨çš„è™šæ‹Ÿç¯å¢ƒé‡Œé¢ï¼Œè¿›ç¨‹æ¥è§¦åˆ°çš„æ–‡ä»¶ç³»ç»Ÿå’Œç½‘ç»œæ¥å£éƒ½æ˜¯è™šæ‹Ÿçš„ï¼Œä¸æœ¬æœºçš„æ–‡ä»¶ç³»ç»Ÿå’Œç½‘ç»œæ¥å£æ˜¯éš”ç¦»çš„ï¼Œå› æ­¤éœ€è¦å®šä¹‰å®¹å™¨ä¸ç‰©ç†æœºçš„ç«¯å£æ˜ å°„ï¼ˆmapï¼‰ã€‚\n\nç°åœ¨ï¼Œåœ¨å®¹å™¨çš„å‘½ä»¤è¡Œï¼ŒæŒ‰ä¸‹ Ctrl + c åœæ­¢ Node è¿›ç¨‹ï¼Œç„¶åæŒ‰ä¸‹ Ctrl + d ï¼ˆæˆ–è€…è¾“å…¥ exitï¼‰é€€å‡ºå®¹å™¨ã€‚æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥ç”¨`docker container kill`ç»ˆæ­¢å®¹å™¨è¿è¡Œã€‚\n\n```Bash\n# åœ¨æœ¬æœºçš„å¦ä¸€ä¸ªç»ˆç«¯çª—å£ï¼ŒæŸ¥å‡ºå®¹å™¨çš„ ID\n$ docker container ls\n\n# åœæ­¢æŒ‡å®šçš„å®¹å™¨è¿è¡Œ\n$ docker container kill [containerID]\n```\n\nå®¹å™¨åœæ­¢è¿è¡Œä¹‹åï¼Œå¹¶ä¸ä¼šæ¶ˆå¤±ï¼Œç”¨ä¸‹é¢çš„å‘½ä»¤åˆ é™¤å®¹å™¨æ–‡ä»¶ã€‚\n\n```Bash\n# æŸ¥å‡ºå®¹å™¨çš„ ID\n$ docker container ls --all\n\n# åˆ é™¤æŒ‡å®šçš„å®¹å™¨æ–‡ä»¶\n$ docker container rm [containerID]\n```\n\nä¹Ÿå¯ä»¥ä½¿ç”¨`docker container run`å‘½ä»¤çš„`--rm`å‚æ•°ï¼Œåœ¨å®¹å™¨ç»ˆæ­¢è¿è¡Œåè‡ªåŠ¨åˆ é™¤å®¹å™¨æ–‡ä»¶ã€‚\n\n```Bash\n$ docker container run --rm -p 8000:3000 -it koa-demo /bin/bash\n```\n\n#### 11.4 CMD å‘½ä»¤\n\nä¸Šä¸€èŠ‚çš„ä¾‹å­é‡Œé¢ï¼Œå®¹å™¨å¯åŠ¨ä»¥åï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥å‘½ä»¤`node demos/01.js`ã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªå‘½ä»¤å†™åœ¨ Dockerfile é‡Œé¢ï¼Œè¿™æ ·å®¹å™¨å¯åŠ¨ä»¥åï¼Œè¿™ä¸ªå‘½ä»¤å°±å·²ç»æ‰§è¡Œäº†ï¼Œä¸ç”¨å†æ‰‹åŠ¨è¾“å…¥äº†ã€‚\n\n```Bash\nFROM node:8.4\nCOPY . /app\nWORKDIR /app\nRUN npm install --registry=https://registry.npm.taobao.org\nEXPOSE 3000\nCMD node demos/01.js\n```\n\nä¸Šé¢çš„ Dockerfile é‡Œé¢ï¼Œå¤šäº†æœ€åä¸€è¡Œ`CMD node demos/01.js`ï¼Œå®ƒè¡¨ç¤ºå®¹å™¨å¯åŠ¨åè‡ªåŠ¨æ‰§è¡Œ`node demos/01.js`ã€‚\n\nä½ å¯èƒ½ä¼šé—®ï¼Œ`RUN`å‘½ä»¤ä¸`CMD`å‘½ä»¤çš„åŒºåˆ«åœ¨å“ªé‡Œï¼Ÿç®€å•è¯´ï¼Œ`RUN`å‘½ä»¤åœ¨ image æ–‡ä»¶çš„æ„å»ºé˜¶æ®µæ‰§è¡Œï¼Œæ‰§è¡Œç»“æœéƒ½ä¼šæ‰“åŒ…è¿›å…¥ image æ–‡ä»¶ï¼›`CMD`å‘½ä»¤åˆ™æ˜¯åœ¨å®¹å™¨å¯åŠ¨åæ‰§è¡Œã€‚å¦å¤–ï¼Œä¸€ä¸ª Dockerfile å¯ä»¥åŒ…å«å¤šä¸ª`RUN`å‘½ä»¤ï¼Œä½†æ˜¯åªèƒ½æœ‰ä¸€ä¸ª`CMD`å‘½ä»¤ã€‚\n\næ³¨æ„ï¼ŒæŒ‡å®šäº†`CMD`å‘½ä»¤ä»¥åï¼Œ`docker container run`å‘½ä»¤å°±ä¸èƒ½é™„åŠ å‘½ä»¤äº†ï¼ˆæ¯”å¦‚å‰é¢çš„`/bin/bash`ï¼‰ï¼Œå¦åˆ™å®ƒä¼šè¦†ç›–`CMD`å‘½ä»¤ã€‚ç°åœ¨ï¼Œå¯åŠ¨å®¹å™¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ã€‚\n\n```Bash\n$ docker container run --rm -p 8000:3000 -it koa-demo:0.0.1\n```\n\n#### 11.5 å‘å¸ƒ image æ–‡ä»¶\n\nå®¹å™¨è¿è¡ŒæˆåŠŸåï¼Œå°±ç¡®è®¤äº† image æ–‡ä»¶çš„æœ‰æ•ˆæ€§ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥è€ƒè™‘æŠŠ image æ–‡ä»¶åˆ†äº«åˆ°ç½‘ä¸Šï¼Œè®©å…¶ä»–äººä½¿ç”¨ã€‚\n\né¦–å…ˆï¼Œå» [hub.docker.com](https://hub.docker.com/) æˆ– [cloud.docker.com](https://cloud.docker.com/) æ³¨å†Œä¸€ä¸ªè´¦æˆ·ã€‚ç„¶åï¼Œç”¨ä¸‹é¢çš„å‘½ä»¤ç™»å½•ã€‚\n\n```Bash\n$ docker login\n```\n\næ¥ç€ï¼Œä¸ºæœ¬åœ°çš„ image æ ‡æ³¨ç”¨æˆ·åå’Œç‰ˆæœ¬ã€‚\n\n```Bash\n$ docker image tag [imageName] [username]/[repository]:[tag]\n# å®ä¾‹\n$ docker image tag koa-demos:0.0.1 ruanyf/koa-demos:0.0.1\n```\n\nä¹Ÿå¯ä»¥ä¸æ ‡æ³¨ç”¨æˆ·åï¼Œé‡æ–°æ„å»ºä¸€ä¸‹ image æ–‡ä»¶ã€‚\n\n```Bash\n$ docker image build -t [username]/[repository]:[tag] .\n```\n\næœ€åï¼Œå‘å¸ƒ image æ–‡ä»¶ã€‚\n\n```Bash\n$ docker image push [username]/[repository]:[tag]\n```\n\nå‘å¸ƒæˆåŠŸä»¥åï¼Œç™»å½• hub.docker.comï¼Œå°±å¯ä»¥çœ‹åˆ°å·²ç»å‘å¸ƒçš„ image æ–‡ä»¶ã€‚\n\n### åäºŒã€å…¶ä»–æœ‰ç”¨çš„å‘½ä»¤\n\ndocker çš„ä¸»è¦ç”¨æ³•å°±æ˜¯ä¸Šé¢è¿™äº›ï¼Œæ­¤å¤–è¿˜æœ‰å‡ ä¸ªå‘½ä»¤ï¼Œä¹Ÿéå¸¸æœ‰ç”¨ã€‚\n\n**ï¼ˆ1ï¼‰docker container start**\n\nå‰é¢çš„`docker container run`å‘½ä»¤æ˜¯æ–°å»ºå®¹å™¨ï¼Œæ¯è¿è¡Œä¸€æ¬¡ï¼Œå°±ä¼šæ–°å»ºä¸€ä¸ªå®¹å™¨ã€‚åŒæ ·çš„å‘½ä»¤è¿è¡Œä¸¤æ¬¡ï¼Œå°±ä¼šç”Ÿæˆä¸¤ä¸ªä¸€æ¨¡ä¸€æ ·çš„å®¹å™¨æ–‡ä»¶ã€‚å¦‚æœå¸Œæœ›é‡å¤ä½¿ç”¨å®¹å™¨ï¼Œå°±è¦ä½¿ç”¨`docker container start`å‘½ä»¤ï¼Œå®ƒç”¨æ¥å¯åŠ¨å·²ç»ç”Ÿæˆã€å·²ç»åœæ­¢è¿è¡Œçš„å®¹å™¨æ–‡ä»¶ã€‚\n\n```Bash\n$ docker container start [containerID]\n```\n\n**ï¼ˆ2ï¼‰docker container stop**\n\nå‰é¢çš„`docker container kill`å‘½ä»¤ç»ˆæ­¢å®¹å™¨è¿è¡Œï¼Œç›¸å½“äºå‘å®¹å™¨é‡Œé¢çš„ä¸»è¿›ç¨‹å‘å‡º SIGKILL ä¿¡å·ã€‚è€Œ`docker container stop`å‘½ä»¤ä¹Ÿæ˜¯ç”¨æ¥ç»ˆæ­¢å®¹å™¨è¿è¡Œï¼Œç›¸å½“äºå‘å®¹å™¨é‡Œé¢çš„ä¸»è¿›ç¨‹å‘å‡º SIGTERM ä¿¡å·ï¼Œç„¶åè¿‡ä¸€æ®µæ—¶é—´å†å‘å‡º SIGKILL ä¿¡å·ã€‚\n\n```Bash\n$ docker container stop [containerID]\n```\n\nè¿™ä¸¤ä¸ªä¿¡å·çš„å·®åˆ«æ˜¯ï¼Œåº”ç”¨ç¨‹åºæ”¶åˆ° SIGTERM ä¿¡å·ä»¥åï¼Œå¯ä»¥è‡ªè¡Œè¿›è¡Œæ”¶å°¾æ¸…ç†å·¥ä½œï¼Œä½†ä¹Ÿå¯ä»¥ä¸ç†ä¼šè¿™ä¸ªä¿¡å·ã€‚å¦‚æœæ”¶åˆ° SIGKILL ä¿¡å·ï¼Œå°±ä¼šå¼ºè¡Œç«‹å³ç»ˆæ­¢ï¼Œé‚£äº›æ­£åœ¨è¿›è¡Œä¸­çš„æ“ä½œä¼šå…¨éƒ¨ä¸¢å¤±ã€‚\n\n**ï¼ˆ3ï¼‰docker container logs**\n\n`docker container logs`å‘½ä»¤ç”¨æ¥æŸ¥çœ‹ docker å®¹å™¨çš„è¾“å‡ºï¼Œå³å®¹å™¨é‡Œé¢ Shell çš„æ ‡å‡†è¾“å‡ºã€‚å¦‚æœ`docker run`å‘½ä»¤è¿è¡Œå®¹å™¨çš„æ—¶å€™ï¼Œæ²¡æœ‰ä½¿ç”¨`-it`å‚æ•°ï¼Œå°±è¦ç”¨è¿™ä¸ªå‘½ä»¤æŸ¥çœ‹è¾“å‡ºã€‚\n\n```Bash\n$ docker container logs [containerID]\n```\n\n**ï¼ˆ4ï¼‰docker container exec**\n\n`docker container exec`å‘½ä»¤ç”¨äºè¿›å…¥ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ docker å®¹å™¨ã€‚å¦‚æœ`docker run`å‘½ä»¤è¿è¡Œå®¹å™¨çš„æ—¶å€™ï¼Œæ²¡æœ‰ä½¿ç”¨`-it`å‚æ•°ï¼Œå°±è¦ç”¨è¿™ä¸ªå‘½ä»¤è¿›å…¥å®¹å™¨ã€‚ä¸€æ—¦è¿›å…¥äº†å®¹å™¨ï¼Œå°±å¯ä»¥åœ¨å®¹å™¨çš„ Shell æ‰§è¡Œå‘½ä»¤äº†ã€‚\n\n```Bash\n$ docker container exec -it [containerID] /bin/bash\n```\n\n**ï¼ˆ5ï¼‰docker container cp**\n\n`docker container cp`å‘½ä»¤ç”¨äºä»æ­£åœ¨è¿è¡Œçš„ Docker å®¹å™¨é‡Œé¢ï¼Œå°†æ–‡ä»¶æ‹·è´åˆ°æœ¬æœºã€‚ä¸‹é¢æ˜¯æ‹·è´åˆ°å½“å‰ç›®å½•çš„å†™æ³•ã€‚\n\n```Bash\n$ docker container cp [containID]:[/path/to/file] .\n```\n\n\n\nDone!\n\n","lastmodified":"2023-06-17T01:48:17.624625337Z","tags":null},"/notes/hands-on-docker_2":{"title":"Dockerè¿›é˜¶","content":"\n# Dockerè¿›é˜¶ï¼šstep by step\n\n## 0. å‚è€ƒ\n\n[Docker å¾®æœåŠ¡æ•™ç¨‹](https://www.ruanyifeng.com/blog/2018/02/docker-wordpress-tutorial.html)\n\nå‰ç½®æ•™ç¨‹ï¼š\n\n* [Dockerå…¥é—¨ï¼šstep by step](notes/hands-on-docker_1.md)\n\n### ä¸€ã€é¢„å¤‡å·¥ä½œï¼šimage ä»“åº“çš„é•œåƒç½‘å€\n\næœ¬æ•™ç¨‹éœ€è¦ä»ä»“åº“ä¸‹è½½ image æ–‡ä»¶ï¼Œä½†æ˜¯å›½å†…è®¿é—® Docker çš„å®˜æ–¹ä»“åº“å¾ˆæ…¢ï¼Œè¿˜ç»å¸¸æ–­çº¿ï¼Œæ‰€ä»¥è¦æŠŠä»“åº“ç½‘å€æ”¹æˆå›½å†…çš„é•œåƒç«™ã€‚è¿™é‡Œæ¨èä½¿ç”¨å®˜æ–¹é•œåƒ registry.docker-cn.com ã€‚ä¸‹é¢æ˜¯æˆ‘çš„ Debian ç³»ç»Ÿçš„é»˜è®¤ä»“åº“ä¿®æ”¹æ–¹æ³•ï¼Œå…¶ä»–ç³»ç»Ÿçš„ä¿®æ”¹æ–¹æ³•å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://www.docker-cn.com/registry-mirror)ã€‚\n\næ‰“å¼€`/etc/default/docker`æ–‡ä»¶ï¼ˆéœ€è¦`sudo`æƒé™ï¼‰ï¼Œåœ¨æ–‡ä»¶çš„åº•éƒ¨åŠ ä¸Šä¸€è¡Œã€‚\n\n```Bash\nDOCKER_OPTS=\"--registry-mirror=https://registry.docker-cn.com\"\n```\n\nç„¶åï¼Œé‡å¯ Docker æœåŠ¡ã€‚\n\n```Bash\n$ sudo service docker restart\n```\n\nç°åœ¨å°±ä¼šè‡ªåŠ¨ä»é•œåƒä»“åº“ä¸‹è½½ image æ–‡ä»¶äº†ã€‚\n\n### äºŒã€æ–¹æ³• Aï¼šè‡ªå»º WordPress å®¹å™¨\n\nå‰é¢è¯´è¿‡ï¼Œæœ¬æ–‡ä¼šç”¨ä¸‰ç§æ–¹æ³•æ¼”ç¤º WordPress çš„å®‰è£…ã€‚ç¬¬ä¸€ç§æ–¹æ³•å°±æ˜¯è‡ªå»º WordPress å®¹å™¨ã€‚\n\n#### 2.1 å®˜æ–¹ çš„ PHP image\n\né¦–å…ˆï¼Œæ–°å»ºä¸€ä¸ªå·¥ä½œç›®å½•ï¼Œå¹¶è¿›å…¥è¯¥ç›®å½•ã€‚\n\n```Bash\n$ mkdir docker-demo \u0026\u0026 cd docker-demo\n```\n\nç„¶åï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚\n\n```Bash\ndocker container run \\\n  --rm \\\n  --name wordpress \\\n  --volume \"$PWD/\":/var/www/html \\\n  php:5.6-apache\n```\n\nä¸Šé¢çš„å‘½ä»¤åŸºäº`php`çš„ image æ–‡ä»¶æ–°å»ºä¸€ä¸ªå®¹å™¨ï¼Œå¹¶ä¸”è¿è¡Œè¯¥å®¹å™¨ã€‚`php`çš„æ ‡ç­¾æ˜¯`5.6-apache`ï¼Œè¯´æ˜è£…çš„æ˜¯ PHP 5.6ï¼Œå¹¶ä¸”è‡ªå¸¦ Apache æœåŠ¡å™¨ã€‚è¯¥å‘½ä»¤çš„ä¸‰ä¸ªå‚æ•°å«ä¹‰å¦‚ä¸‹ã€‚\n\n```Bash\n--rmï¼šåœæ­¢è¿è¡Œåï¼Œè‡ªåŠ¨åˆ é™¤å®¹å™¨æ–‡ä»¶ã€‚\n--name wordpressï¼šå®¹å™¨çš„åå­—å«åšwordpressã€‚\n--volume \"$PWD/\":/var/www/htmlï¼šå°†å½“å‰ç›®å½•ï¼ˆ$PWDï¼‰æ˜ å°„åˆ°å®¹å™¨çš„/var/www/htmlï¼ˆApache å¯¹å¤–è®¿é—®çš„é»˜è®¤ç›®å½•ï¼‰ã€‚å› æ­¤ï¼Œå½“å‰ç›®å½•çš„ä»»ä½•ä¿®æ”¹ï¼Œéƒ½ä¼šåæ˜ åˆ°å®¹å™¨é‡Œé¢ï¼Œè¿›è€Œè¢«å¤–éƒ¨è®¿é—®åˆ°ã€‚\n```\n\nè¿è¡Œä¸Šé¢çš„å‘½ä»¤ä»¥åï¼Œå¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œå‘½ä»¤è¡Œä¼šæç¤ºå®¹å™¨å¯¹å¤–çš„ IP åœ°å€ï¼Œè¯·è®°ä¸‹è¿™ä¸ªåœ°å€ï¼Œæˆ‘ä»¬è¦ç”¨å®ƒæ¥è®¿é—®å®¹å™¨ã€‚æˆ‘åˆ†é…åˆ°çš„ IP åœ°å€æ˜¯ 172.17.0.2ã€‚\n\næ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® 172.17.0.2ï¼Œä½ ä¼šçœ‹åˆ°ä¸‹é¢çš„æç¤ºã€‚\n\n```Bash\nForbidden\nYou don't have permission to access / on this server.\n```\n\næ˜¯å› ä¸ºå®¹å™¨çš„`/var/www/html`ç›®å½•ï¼ˆä¹Ÿå°±æ˜¯æœ¬æœºçš„`docker-demo`ç›®å½•ï¼‰ä¸‹é¢ä»€ä¹ˆä¹Ÿæ²¡æœ‰ï¼Œæ— æ³•æä¾›å¯ä»¥è®¿é—®çš„å†…å®¹ã€‚\n\nè¯·åœ¨æœ¬æœºçš„`docker-demo`ç›®å½•ä¸‹é¢ï¼Œæ·»åŠ ä¸€ä¸ªæœ€ç®€å•çš„ PHP æ–‡ä»¶`index.php`ã€‚\n\n```Bash\n\u003c?php \nphpinfo();\n?\u003e\n```\n\nä¿å­˜ä»¥åï¼Œæµè§ˆå™¨åˆ·æ–°`172.17.0.2`ï¼Œåº”è¯¥å°±ä¼šçœ‹åˆ°ç†Ÿæ‚‰çš„`phpinfo`é¡µé¢äº†ã€‚\n\n\n\n![135ee4ef-bf47-4239-935a-2eab8f896575](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/135ee4ef-bf47-4239-935a-2eab8f896575.jpeg)\n\n2.2 æ‹·è´ WordPress å®‰è£…åŒ…\n\næ—¢ç„¶æœ¬åœ°çš„`docker-demo`ç›®å½•å¯ä»¥æ˜ å°„åˆ°å®¹å™¨é‡Œé¢ï¼Œé‚£ä¹ˆæŠŠ WordPress å®‰è£…åŒ…æ‹·è´åˆ°`docker-demo`ç›®å½•ä¸‹ï¼Œä¸å°±å¯ä»¥é€šè¿‡å®¹å™¨è®¿é—®åˆ° WordPress çš„å®‰è£…ç•Œé¢äº†å—ï¼Ÿ\n\né¦–å…ˆï¼Œåœ¨`docker-demo`ç›®å½•ä¸‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼ŒæŠ“å–å¹¶è§£å‹ WordPress å®‰è£…åŒ…ã€‚\n\n```Bash\nwget https://cn.wordpress.org/wordpress-4.9.4-zh_CN.tar.gz\ntar -xvf wordpress-4.9.4-zh_CN.tar.gz\n```\n\nè§£å‹ä»¥åï¼ŒWordPress çš„å®‰è£…æ–‡ä»¶ä¼šåœ¨`docker-demo/wordpress`ç›®å½•ä¸‹ã€‚\n\nè¿™æ—¶æµè§ˆå™¨è®¿é—®`http://172.17.0.2/wordpress`ï¼Œå°±èƒ½çœ‹åˆ° WordPress çš„å®‰è£…æç¤ºäº†ã€‚\n\n\n\n![555d7606-f3b7-45d4-b440-a3e78ac456a5](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/555d7606-f3b7-45d4-b440-a3e78ac456a5.jpeg)\n\n#### 2.3 å®˜æ–¹çš„ MySQL å®¹å™¨\n\nWordPress å¿…é¡»æœ‰æ•°æ®åº“æ‰èƒ½å®‰è£…ï¼Œæ‰€ä»¥å¿…é¡»æ–°å»º MySQL å®¹å™¨ã€‚\n\næ‰“å¼€ä¸€ä¸ªæ–°çš„å‘½ä»¤è¡Œçª—å£ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚\n\n```Bash\ndocker container run \\\n  -d \\\n  --rm \\\n  --name wordpressdb \\\n  --env MYSQL_ROOT_PASSWORD=123456 \\\n  --env MYSQL_DATABASE=wordpress \\\n  mysql:5.7\n```\n\nä¸Šé¢çš„å‘½ä»¤ä¼šåŸºäº MySQL çš„ image æ–‡ä»¶ï¼ˆ5.7ç‰ˆæœ¬ï¼‰æ–°å»ºä¸€ä¸ªå®¹å™¨ã€‚è¯¥å‘½ä»¤çš„äº”ä¸ªå‘½ä»¤è¡Œå‚æ•°çš„å«ä¹‰å¦‚ä¸‹ã€‚\n\n\u003e - `-d`ï¼šå®¹å™¨å¯åŠ¨åï¼Œåœ¨åå°è¿è¡Œã€‚\n\u003e\n\u003e - `--rm`ï¼šå®¹å™¨ç»ˆæ­¢è¿è¡Œåï¼Œè‡ªåŠ¨åˆ é™¤å®¹å™¨æ–‡ä»¶ã€‚\n\u003e\n\u003e - `--name wordpressdb`ï¼šå®¹å™¨çš„åå­—å«åš`wordpressdb`\n\u003e\n\u003e - `--env MYSQL_ROOT_PASSWORD=123456`ï¼šå‘å®¹å™¨è¿›ç¨‹ä¼ å…¥ä¸€ä¸ªç¯å¢ƒå˜é‡`MYSQL_ROOT_PASSWORD`ï¼Œè¯¥å˜é‡ä¼šè¢«ç”¨ä½œ MySQL çš„æ ¹å¯†ç ã€‚\n\u003e\n\u003e - `--env MYSQL_DATABASE=wordpress`ï¼šå‘å®¹å™¨è¿›ç¨‹ä¼ å…¥ä¸€ä¸ªç¯å¢ƒå˜é‡`MYSQL_DATABASE`ï¼Œå®¹å™¨é‡Œé¢çš„ MySQL ä¼šæ ¹æ®è¯¥å˜é‡åˆ›å»ºä¸€ä¸ªåŒåæ•°æ®åº“ï¼ˆæœ¬ä¾‹æ˜¯`WordPress`ï¼‰ã€‚\n\nè¿è¡Œä¸Šé¢çš„å‘½ä»¤ä»¥åï¼Œæ­£å¸¸æƒ…å†µä¸‹ï¼Œå‘½ä»¤è¡Œä¼šæ˜¾ç¤ºä¸€è¡Œå­—ç¬¦ä¸²ï¼Œè¿™æ˜¯å®¹å™¨çš„ IDï¼Œè¡¨ç¤ºå·²ç»æ–°å»ºæˆåŠŸäº†ã€‚\n\nè¿™æ—¶ï¼Œä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„å®¹å™¨ï¼Œä½ åº”è¯¥çœ‹åˆ°`wordpress`å’Œ`wordpressdb`ä¸¤ä¸ªå®¹å™¨æ­£åœ¨è¿è¡Œã€‚\n\n```Bash\ndocker container ls\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                 NAMES\nf01fe6ff7a31   mysql:5.7        \"docker-entrypoint.sâ€¦\"   15 seconds ago   Up 13 seconds   3306/tcp, 33060/tcp   wordpressdb\nb62e97351f5f   php:5.6-apache   \"docker-php-entrypoiâ€¦\"   32 seconds ago   Up 30 seconds   80/tcp                wordpress\n```\n\nå…¶ä¸­ï¼Œ`wordpressdb`æ˜¯åå°è¿è¡Œçš„ï¼Œå‰å°çœ‹ä¸è§å®ƒçš„è¾“å‡ºï¼Œå¿…é¡»ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹ã€‚\n\n```Bash\ndocker container logs wordpressdb\n```\n\n#### 2.4 å®šåˆ¶ PHP å®¹å™¨\n\nç°åœ¨ WordPress å®¹å™¨å’Œ MySQL å®¹å™¨éƒ½å·²ç»æœ‰äº†ã€‚æ¥ä¸‹æ¥ï¼Œè¦æŠŠ WordPress å®¹å™¨è¿æ¥åˆ° MySQL å®¹å™¨äº†ã€‚ä½†æ˜¯ï¼ŒPHP çš„å®˜æ–¹ image ä¸å¸¦æœ‰`mysql`æ‰©å±•ï¼Œå¿…é¡»è‡ªå·±æ–°å»º image æ–‡ä»¶ã€‚\n\né¦–å…ˆï¼Œåœæ‰ WordPress å®¹å™¨ã€‚\n\n```Bash\ndocker container stop wordpress\n\n## åœæ‰ä¹‹å\ndocker container ls              \nCONTAINER ID   IMAGE       COMMAND                  CREATED         STATUS         PORTS                 NAMES\nf01fe6ff7a31   mysql:5.7   \"docker-entrypoint.sâ€¦\"   3 minutes ago   Up 3 minutes   3306/tcp, 33060/tcp   wordpressdb\n```\n\nåœæ‰ä»¥åï¼Œç”±äº`--rm`å‚æ•°çš„ä½œç”¨ï¼Œè¯¥å®¹å™¨æ–‡ä»¶ä¼šè¢«è‡ªåŠ¨åˆ é™¤ã€‚\n\nç„¶åï¼Œåœ¨`docker-demo`ç›®å½•é‡Œé¢ï¼Œæ–°å»ºä¸€ä¸ª`Dockerfile`æ–‡ä»¶ï¼Œå†™å…¥ä¸‹é¢çš„å†…å®¹ã€‚\n\n```Bash\nFROM php:5.6-apache\nRUN docker-php-ext-install mysqli\nCMD apache2-foreground\n```\n\nä¸Šé¢ä»£ç çš„æ„æ€ï¼Œå°±æ˜¯åœ¨åŸæ¥ PHP çš„ image åŸºç¡€ä¸Šï¼Œå®‰è£…`mysqli`çš„æ‰©å±•ã€‚ç„¶åï¼Œå¯åŠ¨ Apacheã€‚\n\nåŸºäºè¿™ä¸ª Dockerfile æ–‡ä»¶ï¼Œæ–°å»ºä¸€ä¸ªåä¸º`phpwithmysql`çš„ image æ–‡ä»¶ã€‚\n\n```Bash\ndocker build -t phpwithmysql .\n```\n\n#### 2.5 Wordpress å®¹å™¨è¿æ¥ MySQL\n\nç°åœ¨åŸºäº phpwithmysql imageï¼Œé‡æ–°æ–°å»ºä¸€ä¸ª WordPress å®¹å™¨ã€‚\n\n```Bash\ndocker container run \\\n  --rm \\\n  --name wordpress \\\n  --volume \"$PWD/\":/var/www/html \\\n  --link wordpressdb:mysql \\\n  phpwithmysql\n```\n\nè·Ÿä¸Šä¸€æ¬¡ç›¸æ¯”ï¼Œä¸Šé¢çš„å‘½ä»¤å¤šäº†ä¸€ä¸ªå‚æ•°`--link wordpressdb:mysql`ï¼Œè¡¨ç¤º WordPress å®¹å™¨è¦è¿åˆ°`wordpressdb`å®¹å™¨ï¼Œå†’å·è¡¨ç¤ºè¯¥å®¹å™¨çš„åˆ«åæ˜¯`mysql`ã€‚\n\nè¿™æ—¶è¿˜è¦æ”¹ä¸€ä¸‹`wordpress`ç›®å½•çš„æƒé™ï¼Œè®©å®¹å™¨å¯ä»¥å°†é…ç½®ä¿¡æ¯å†™å…¥è¿™ä¸ªç›®å½•ï¼ˆå®¹å™¨å†…éƒ¨å†™å…¥çš„`/var/www/html`ç›®å½•ï¼Œä¼šæ˜ å°„åˆ°è¿™ä¸ªç›®å½•ï¼‰ã€‚\n\n```Bash\nchmod -R 777 wordpress\n```\n\næ¥ç€ï¼Œå›åˆ°æµè§ˆå™¨çš„`http://172.17.0.2/wordpress`é¡µé¢ï¼Œç‚¹å‡»\"ç°åœ¨å°±å¼€å§‹ï¼\"æŒ‰é’®ï¼Œå¼€å§‹å®‰è£…ã€‚\n\nWordPress æç¤ºè¦è¾“å…¥æ•°æ®åº“å‚æ•°ã€‚è¾“å…¥çš„å‚æ•°å¦‚ä¸‹ã€‚\n\n![7a2e3f14-a418-40ef-b5f8-8036fa3035b9](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/7a2e3f14-a418-40ef-b5f8-8036fa3035b9.jpeg)\n\n\u003e - æ•°æ®åº“åï¼š`wordpress`\n\u003e\n\u003e - ç”¨æˆ·åï¼š`root`\n\u003e\n\u003e - å¯†ç ï¼š`123456`\n\u003e\n\u003e - æ•°æ®åº“ä¸»æœºï¼š`mysql`\n\u003e\n\u003e - è¡¨å‰ç¼€ï¼š`wp_`ï¼ˆä¸å˜ï¼‰\n\nç‚¹å‡»\"ä¸‹ä¸€æ­¥\"æŒ‰é’®ï¼Œå¦‚æœ Wordpress è¿æ¥æ•°æ®åº“æˆåŠŸï¼Œå°±ä¼šå‡ºç°ä¸‹é¢çš„é¡µé¢ï¼Œè¿™å°±è¡¨ç¤ºå¯ä»¥å®‰è£…äº†ã€‚\n\n![IdmeQaS9iG](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/IdmeQaS9iG.jpg)\n\nè‡³æ­¤ï¼Œè‡ªå»º WordPress å®¹å™¨çš„æ¼”ç¤ºå®Œæ¯•ï¼Œå¯ä»¥æŠŠæ­£åœ¨è¿è¡Œçš„ä¸¤ä¸ªå®¹å™¨å…³é—­äº†ï¼ˆå®¹å™¨æ–‡ä»¶ä¼šè‡ªåŠ¨åˆ é™¤ï¼‰ã€‚\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/4MkHfisE5P.jpg\" alt=\"4MkHfisE5P\" style=\"zoom:33%;\" /\u003e\n\nå®‰è£…ä¹‹å\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/PXh6tI53nv.jpg\" alt=\"PXh6tI53nv\" style=\"zoom:50%;\" /\u003e\n\n```Bash\ndocker container stop wordpress wordpressdb\n\n## ä¹‹å\ndocker container ls\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n```\n\n### ä¸‰ã€æ–¹æ³• Bï¼šWordpress å®˜æ–¹é•œåƒ\n\nä¸Šä¸€éƒ¨åˆ†çš„è‡ªå»º WordPress å®¹å™¨ï¼Œè¿˜æ˜¯æŒºéº»çƒ¦çš„ã€‚å…¶å®ä¸ç”¨è¿™ä¹ˆéº»çƒ¦ï¼ŒDocker å·²ç»æä¾›äº†å®˜æ–¹ [WordPress](https://hub.docker.com/_/wordpress/) imageï¼Œç›´æ¥ç”¨é‚£ä¸ªå°±å¯ä»¥äº†ã€‚æœ‰äº†ä¸Šä¸€éƒ¨åˆ†çš„åŸºç¡€ï¼Œä¸‹é¢çš„æ“ä½œå°±å¾ˆå®¹æ˜“ç†è§£äº†ã€‚\n\n \n\n#### 3.1 åŸºæœ¬ç”¨æ³•\n\né¦–å…ˆï¼Œæ–°å»ºå¹¶å¯åŠ¨ MySQL å®¹å™¨ã€‚\n\n```Bash\ndocker container run \\\n  -d \\\n  --rm \\\n  --name wordpressdb \\\n  --env MYSQL_ROOT_PASSWORD=123456 \\\n  --env MYSQL_DATABASE=wordpress \\\n  mysql:5.7\n```\n\nç„¶åï¼ŒåŸºäºå®˜æ–¹çš„ WordPress imageï¼Œæ–°å»ºå¹¶å¯åŠ¨ WordPress å®¹å™¨ã€‚\n\n```Bash\ndocker container run \\\n  -d \\\n  --rm \\\n  --name wordpress \\\n  --env WORDPRESS_DB_PASSWORD=123456 \\\n  --env WORDPRESS_DB_USER=root \\\n  --link wordpressdb:mysql \\\n  wordpress\n```\n\nä¸Šé¢å‘½ä»¤ä¸­ï¼Œå„ä¸ªå‚æ•°çš„å«ä¹‰å‰é¢éƒ½è§£é‡Šè¿‡äº†ï¼Œå…¶ä¸­ç¯å¢ƒå˜é‡`WORDPRESS_DB_PASSWORD`æ˜¯ MySQL å®¹å™¨çš„æ ¹å¯†ç ã€‚\n\n```Bash\ndocker container ls   \nCONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                 NAMES\nad6ee2dd4971   wordpress   \"docker-entrypoint.sâ€¦\"   48 seconds ago   Up 35 seconds   80/tcp                wordpress\n87f2d581abda   mysql:5.7   \"docker-entrypoint.sâ€¦\"   3 minutes ago    Up 3 minutes    3306/tcp, 33060/tcp   wordpressdb\n```\n\nä¸Šé¢å‘½ä»¤æŒ‡å®š`wordpress`å®¹å™¨åœ¨åå°è¿è¡Œï¼Œå¯¼è‡´å‰å°çœ‹ä¸è§è¾“å‡ºï¼Œä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æŸ¥å‡º`wordpress`å®¹å™¨çš„ IP åœ°å€ã€‚\n\n```Bash\ndocker container inspect wordpress\n[\n    {\n        \"Id\": \"ad6ee2dd497184a397c8bd58df7b415b5081bff5b0e000a10daa0b970b38cf9c\",\n        \"Created\": \"2022-08-26T02:16:36.222826772Z\",\n        \"Path\": \"docker-entrypoint.sh\",\n        \"Args\": [\n            \"apache2-foreground\"\n        ],\n   ã€‚ã€‚ã€‚\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.3\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n   ã€‚ã€‚ã€‚\n    }\n]\n```\n\nä¸Šé¢å‘½ä»¤è¿è¡Œä»¥åï¼Œä¼šè¾“å‡ºå¾ˆå¤šå†…å®¹ï¼Œæ‰¾åˆ°`IPAddress`å­—æ®µå³å¯ã€‚æˆ‘çš„æœºå™¨è¿”å›çš„ IP åœ°å€æ˜¯`172.17.0.3`ã€‚\n\næµè§ˆå™¨è®¿é—®`172.17.0.3`ï¼Œå°±ä¼šçœ‹åˆ° WordPress çš„å®‰è£…æç¤ºã€‚\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/fOzoBcsIhF.jpg\" alt=\"fOzoBcsIhF\" style=\"zoom:33%;\" /\u003e\n\n#### 3.2 WordPress å®¹å™¨çš„å®šåˆ¶\n\nåˆ°äº†ä¸Šä¸€æ­¥ï¼Œå®˜æ–¹ WordPress å®¹å™¨çš„å®‰è£…å°±å·²ç»æˆåŠŸäº†ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªå¾ˆä¸æ–¹ä¾¿çš„åœ°æ–¹ã€‚\n\n\u003e - æ¯æ¬¡æ–°å»ºå®¹å™¨ï¼Œè¿”å›çš„ IP åœ°å€ä¸èƒ½ä¿è¯ç›¸åŒï¼Œå¯¼è‡´è¦æ›´æ¢ IP åœ°å€è®¿é—® WordPressã€‚\n\u003e\n\u003e - WordPress å®‰è£…åœ¨å®¹å™¨é‡Œé¢ï¼Œæœ¬åœ°æ— æ³•ä¿®æ”¹æ–‡ä»¶ã€‚\n\nè§£å†³è¿™ä¸¤ä¸ªé—®é¢˜å¾ˆå®¹æ˜“ï¼Œåªè¦æ–°å»ºå®¹å™¨çš„æ—¶å€™ï¼ŒåŠ ä¸¤ä¸ªå‘½ä»¤è¡Œå‚æ•°å°±å¯ä»¥äº†ã€‚\n\nå…ˆæŠŠåˆšæ‰å¯åŠ¨çš„ WordPress å®¹å™¨ç»ˆæ­¢ï¼ˆå®¹å™¨æ–‡ä»¶ä¼šè‡ªåŠ¨åˆ é™¤ï¼‰ã€‚\n\n```Bash\ndocker container stop wordpress\n```\n\nç„¶åï¼Œä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ–°å»ºå¹¶å¯åŠ¨ WordPress å®¹å™¨ã€‚\n\n```Bash\ndocker container run \\\n  -d \\\n  -p 127.0.0.2:8080:80 \\\n  --rm \\\n  --name wordpress \\\n  --env WORDPRESS_DB_PASSWORD=123456 \\\n  --env WORDPRESS_DB_USER=root \\\n  --link wordpressdb:mysql \\\n  --volume \"$PWD/wordpress\":/var/www/html \\\n  wordpress\n```\n\nä¸Šé¢çš„å‘½ä»¤è·Ÿå‰é¢ç›¸æ¯”ï¼Œå‘½ä»¤è¡Œå‚æ•°åªå¤šå‡ºäº†ä¸¤ä¸ªã€‚\n\n\u003e - `-p 127.0.0.2:8080:80`ï¼šå°†å®¹å™¨çš„ 80 ç«¯å£æ˜ å°„åˆ°`127.0.0.2`çš„`8080`ç«¯å£ã€‚\n\u003e\n\u003e - `--volume \"$PWD/wordpress\":/var/www/html`ï¼šå°†å®¹å™¨çš„`/var/www/html`ç›®å½•æ˜ å°„åˆ°å½“å‰ç›®å½•çš„`wordpress`å­ç›®å½•ã€‚\n\næµè§ˆå™¨è®¿é—®`127.0.0.2:8080`å°±èƒ½çœ‹åˆ° WordPress çš„å®‰è£…æç¤ºäº†ã€‚è€Œä¸”ï¼Œä½ åœ¨`wordpress`å­ç›®å½•ä¸‹çš„æ¯æ¬¡ä¿®æ”¹ï¼Œéƒ½ä¼šåæ˜ åˆ°å®¹å™¨é‡Œé¢ã€‚\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/VjVIgk6FeM.jpg\" alt=\"VjVIgk6FeM\" style=\"zoom:33%;\" /\u003e\n\næœ€åï¼Œç»ˆæ­¢è¿™ä¸¤ä¸ªå®¹å™¨ï¼ˆå®¹å™¨æ–‡ä»¶ä¼šè‡ªåŠ¨åˆ é™¤ï¼‰ã€‚\n\n```Bash\ndocker container stop wordpress wordpressdb\n```\n\n### å››ã€æ–¹æ³• Cï¼šDocker Compose å·¥å…·\n\nä¸Šé¢çš„æ–¹æ³• B å·²ç»æŒºç®€å•äº†ï¼Œä½†æ˜¯å¿…é¡»è‡ªå·±åˆ†åˆ«å¯åŠ¨ä¸¤ä¸ªå®¹å™¨ï¼Œå¯åŠ¨çš„æ—¶å€™ï¼Œè¿˜è¦åœ¨å‘½ä»¤è¡Œæä¾›å®¹å™¨ä¹‹é—´çš„è¿æ¥ä¿¡æ¯ã€‚å› æ­¤ï¼ŒDocker æä¾›äº†ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œæ¥ç®¡ç†å¤šä¸ªå®¹å™¨çš„è”åŠ¨ã€‚\n\n#### 4.1 Docker Compose ç®€ä»‹\n\n ![KulToe4cJl](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/KulToe4cJl.jpg)\n\n[Compose](https://docs.docker.com/compose/) æ˜¯ Docker å…¬å¸æ¨å‡ºçš„ä¸€ä¸ªå·¥å…·è½¯ä»¶ï¼Œå¯ä»¥ç®¡ç†å¤šä¸ª Docker å®¹å™¨ç»„æˆä¸€ä¸ªåº”ç”¨ã€‚ä½ éœ€è¦å®šä¹‰ä¸€ä¸ª [YAML](https://www.ruanyifeng.com/blog/2016/07/yaml.html) æ ¼å¼çš„é…ç½®æ–‡ä»¶`docker-compose.yml`ï¼Œå†™å¥½å¤šä¸ªå®¹å™¨ä¹‹é—´çš„è°ƒç”¨å…³ç³»ã€‚ç„¶åï¼Œåªè¦ä¸€ä¸ªå‘½ä»¤ï¼Œå°±èƒ½åŒæ—¶å¯åŠ¨/å…³é—­è¿™äº›å®¹å™¨ã€‚\n\n```Bash\n# å¯åŠ¨æ‰€æœ‰æœåŠ¡\ndocker compose up\n# å…³é—­æ‰€æœ‰æœåŠ¡\ndocker compose stop\n```\n\n#### 4.2 Docker Compose çš„å®‰è£…\n\nMac å’Œ Windows åœ¨å®‰è£… docker çš„æ—¶å€™ï¼Œä¼šä¸€èµ·å®‰è£… docker composeã€‚Linux ç³»ç»Ÿä¸‹çš„å®‰è£…å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://docs.docker.com/compose/install/#install-compose)ã€‚\n\nå®‰è£…å®Œæˆåï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚\n\n```Bash\ndocker compose version\nDocker Compose version v2.6.0\n```\n\n#### 4.3 WordPress ç¤ºä¾‹\n\nåœ¨`docker-demo`ç›®å½•ä¸‹ï¼Œæ–°å»º`docker-compose.yml`æ–‡ä»¶ï¼Œå†™å…¥ä¸‹é¢çš„å†…å®¹ã€‚\n\n```Bash\nversion: \"3\"\n\nservices:\n  mysql:\n    image: mysql:5.7\n    environment:\n     - MYSQL_ROOT_PASSWORD=123456\n     - MYSQL_DATABASE=wordpress\n  web:\n    image: wordpress\n    links:\n     - mysql\n    environment:\n     - WORDPRESS_DB_PASSWORD=123456\n     - WORDPRESS_DB_USER=root\n    ports:\n     - \"127.0.0.3:8080:80\"\n    working_dir: /var/www/html\n```\n\nä¸Šé¢ä»£ç ä¸­ï¼Œä¸¤ä¸ªé¡¶å±‚æ ‡ç­¾è¡¨ç¤ºæœ‰ä¸¤ä¸ªå®¹å™¨`mysql`å’Œ`web`ã€‚æ¯ä¸ªå®¹å™¨çš„å…·ä½“è®¾ç½®ï¼Œå‰é¢éƒ½å·²ç»è®²è§£è¿‡äº†ï¼Œè¿˜æ˜¯æŒºå®¹æ˜“ç†è§£çš„ã€‚\n\nå¯åŠ¨ä¸¤ä¸ªå®¹å™¨ã€‚ \n\n```Bash\ndocker compose up\n```\n\næµè§ˆå™¨è®¿é—® http://127.0.0.3:8080ï¼Œåº”è¯¥å°±èƒ½çœ‹åˆ° WordPress çš„å®‰è£…ç•Œé¢ã€‚\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/rbQBhnSKGe.jpg\" alt=\"rbQBhnSKGe\" style=\"zoom:50%;\" /\u003e\n\nç°åœ¨å…³é—­ä¸¤ä¸ªå®¹å™¨ã€‚\n\n```Bash\ndocker compose stop\n```\n\nå…³é—­ä»¥åï¼Œè¿™ä¸¤ä¸ªå®¹å™¨æ–‡ä»¶è¿˜æ˜¯å­˜åœ¨çš„ï¼Œå†™åœ¨é‡Œé¢çš„æ•°æ®ä¸ä¼šä¸¢å¤±ã€‚ä¸‹æ¬¡å¯åŠ¨çš„æ—¶å€™ï¼Œè¿˜å¯ä»¥å¤ç”¨ã€‚ä¸‹é¢çš„å‘½ä»¤å¯ä»¥æŠŠè¿™ä¸¤ä¸ªå®¹å™¨æ–‡ä»¶åˆ é™¤ï¼ˆå®¹å™¨å¿…é¡»å·²ç»åœæ­¢è¿è¡Œï¼‰ã€‚\n\n```Bash\ndocker compose rm\n```\n\n### äº”ã€å‚è€ƒé“¾æ¥\n\n- [How to Manually Build Docker Containers for WordPress](https://www.sitepoint.com/how-to-manually-build-docker-containers-for-wordpress/), by Aleksander Koko\n\n- [How to Use the Official Docker WordPress Image](https://www.sitepoint.com/how-to-use-the-official-docker-wordpress-image/), by Aleksander Koko\n\n- [Deploying WordPress with Docker](https://www.sitepoint.com/deploying-wordpress-with-docker/), by Aleksander Koko\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-flask":{"title":"Hands on Flask","content":"\n# Hands on Flask: step by step\n\n\u003e I am a newcomer of Flask. So I follow the official tutorial to build a blog post web application step by step.\n\n\n\n## Reference\n\n[Flask Installation](https://flask.palletsprojects.com/en/2.2.x/installation/)\n\n[Flask Tutorial](https://flask.palletsprojects.com/en/2.2.x/tutorial/)\n\nFollow the tutorial to build a blog web app like:\n\n\n\n![image-20221201145000837](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221201145000837.png)\n\n\n\n## Step by Step\n\n### Step 0.Backgroud\n\nbuild a basic blog application called Flaskr using Flask.\n\n\n\n### Step 1. Project Layout\n\n#### Create a project directory\n\n```shell\nmkdir flask-tutorial\ncd flask-tutorial/\n```\n\n\n\n#### Install Flask\n\n[Flask Installation Doc ](https://flask.palletsprojects.com/en/2.2.x/installation/)\n\nCreate an python environment using [venv](https://docs.python.org/3/library/venv.html#module-venv):\n\n```shell\n$ python3 -m venv venv\n\n$ tree . -L 3\n.\nâ””â”€â”€ venv\n    â”œâ”€â”€ bin\n    â”‚Â Â  â”œâ”€â”€ Activate.ps1\n    â”‚Â Â  â”œâ”€â”€ activate\n    â”‚Â Â  â”œâ”€â”€ activate.csh\n    â”‚Â Â  â”œâ”€â”€ activate.fish\n    â”‚Â Â  â”œâ”€â”€ pip\n    â”‚Â Â  â”œâ”€â”€ pip3\n    â”‚Â Â  â”œâ”€â”€ pip3.9\n    â”‚Â Â  â”œâ”€â”€ python -\u003e python3\n    â”‚Â Â  â”œâ”€â”€ python3 -\u003e /Users/yangls06/opt/miniconda3/bin/python3\n    â”‚Â Â  â””â”€â”€ python3.9 -\u003e python3\n    â”œâ”€â”€ include\n    â”œâ”€â”€ lib\n    â”‚Â Â  â””â”€â”€ python3.9\n    â””â”€â”€ pyvenv.cfg\n\n5 directories, 11 files\n\n$ tree venv/lib/python3.9/ -L 3\nvenv/lib/python3.9/\nâ””â”€â”€ site-packages\n    â”œâ”€â”€ _distutils_hack\n    â”‚Â Â  â”œâ”€â”€ __init__.py\n    â”‚Â Â  â”œâ”€â”€ __pycache__\n    â”‚Â Â  â””â”€â”€ override.py\n    â”œâ”€â”€ distutils-precedence.pth\n    â”œâ”€â”€ pip\n    â”‚Â Â  â”œâ”€â”€ __init__.py\n    â”‚Â Â  â”œâ”€â”€ __main__.py\n        ...\n    â”œâ”€â”€ pkg_resources\n    ...\n```\n\n\n\nActivate the environment\n\n```shell\n$ . venv/bin/activate\n```\n\nThen the environment has been changed.\n\n![image-20221201153333547](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221201153333547.png)\n\nInstall Flask\n\nWithin the activated environment, install Flask using `pip`:\n\n```shell\n$ pip install Flask\n\nLooking in indexes: https://pypi.douban.com/simple\nCollecting Flask\n  Downloading https://pypi.doubanio.com/packages/0f/43/15f4f9ab225b0b25352412e8daa3d0e3d135fcf5e127070c74c3632c8b4c/Flask-2.2.2-py3-none-any.whl (101 kB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 101.5/101.5 KB 1.8 MB/s eta 0:00:00\n...\nCollecting MarkupSafe\u003e=2.0\n  Downloading https://pypi.doubanio.com/packages/06/7f/d5e46d7464360b6ac39c5b0b604770dba937e3d7cab485d2f3298454717b/MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\nInstalling collected packages: zipp, MarkupSafe, itsdangerous, click, Werkzeug, Jinja2, importlib-metadata, Flask\nSuccessfully installed Flask-2.2.2 Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 click-8.1.3 importlib-metadata-5.1.0 itsdangerous-2.1.2 zipp-3.11.0\n```\n\n\n\nGit init\n\n```shell\n$ git init\n```\n\n\n\nwith `.gitignore`\n\n```\nvenv/\n\n*.pyc\n__pycache__/\n\ninstance/\n\n.pytest_cache/\n.coverage\nhtmlcov/\n\ndist/\nbuild/\n*.egg-info/\n```\n\n\n\nAdd folders\n\n```shell\n$ tree -a -L 1\n.\nâ”œâ”€â”€ .git\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ flaskr\nâ”œâ”€â”€ tests\nâ””â”€â”€ venv\n\n4 directories, 1 file\n```\n\n\n\n### Step 3. Application Setup\n\n#### The Application Factory: \\__init__.py\n\n* create `Flask` instance \n* make `flaskr` directory a package\n\n```python\n#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\n\"\"\"\n@Time    :   2022/12/01 15:58:24\n@Author  :   Linsan Yang \n@Desc    :   init flaskr\n\"\"\"\n\nimport os\nfrom flask import Flask\n\ndef create_app(test_config=None):\n    # create and configure the app\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_mapping(\n        SECRET_KEY = 'dev',\n        DATABASE=os.path.join(app.instance_path, 'flaskr.sqlite'),\n    )\n\n    if test_config is None:\n        # load the instance config, if it exists, when not testing\n        app.config.from_pyfile('config.py', silent=True)\n    else:\n        # load the test config if passed in\n        app.config.from_mapping(test_config)\n    \n    # ensure the instance folder exists\n    try:\n        os.makedirs(app.instance_path)\n    except OSError as e:\n        pass\n\n    # a simple page that says hello\n    @app.route('/hello')\n    def hello():\n        return 'Hello, World!'\n\n    return app\n\n```\n\n\n\n**instance folder** \n\nThere will be a `instance/`directory, located outside the `flaskr` package and can hold local data that shouldnâ€™t be committed to version control, such as configuration secrets and the database file.\n\n\n\n**test_config**\n\nUsing test_config for testing.\n\n\n\n**@app.route()**\n\ncreate a simple route of `/hello`\n\n\n\n#### Run The Application\n\nIn the `flask-tutorial` dir not `flaskr` package:\n\n```shell\n$ flask --app flaskr --debug run\n * Serving Flask app 'flaskr'\n * Debug mode: on\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nPress CTRL+C to quit\n * Restarting with stat\n * Debugger is active!\n * Debugger PIN: 134-914-837\n```\n\nThen open 127.0.0.1:5000/hello in browser, got\n\n![image-20221201162507048](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221201162507048.png)\n\n### Step 4. Define and Access the Database\n\nThe app will use `Sqlite` database to store users and posts. Python has a built-in module `sqlite3` module.\n\n#### Connect to Sqlite\n\nflaskr/db.py\n\n```python\nimport sqlite3\n\nimport click\nfrom flask import current_app, g\n\ndef get_db():\n    if 'db' not in g:\n        g.db = sqlite3.connect(\n            current_app.config['DATABASE'],\n            detect_types=sqlite3.PARSE_DECLTYPES\n        )\n        g.db.row_factory = sqlite3.Row\n    return g.db\n\ndef close_db(e=None):\n    db = g.pop('db', None)\n\n    if db is not None:\n        db.close()\n```\n\n\n\n`g` is a spectial object for each request to share data among different functions. `current_app` is similar.\n\n\n\n#### Create Tables: using sql\n\nDefine `user` and `post` table in `flaskr/schema.sql`:\n\n```sql\nDROP TABLE IF EXISTS user;\nDROP TABLE IF EXISTS post;\n\nCREATE TABLE user (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  username TEXT UNIQUE NOT NULL,\n  password TEXT NOT NULL\n);\n\nCREATE TABLE post (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  author_id INTEGER NOT NULL,\n  created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  title TEXT NOT NULL,\n  body TEXT NOT NULL,\n  FOREIGN KEY (author_id) REFERENCES user (id)\n);\n```\n\n\n\nAdd functions to run the SQLs to the `db.py`\n\n```python\ndef init_db():\n    db = get_db()\n    \n    with current_app.open_resource('schema.sql') as f:\n        db.executescript(f.read().decode('utf8'))\n\n@click.command('init-db')\ndef init_db_command():\n    '''Clear the existing data and create new tables.'''\n    init_db()\n    click.echo('Initialized the database.')\n```\n\n\n\n#### Register with the Applicaiton\n\nThe close_db and init_db_command functions need to be registered with the app instance for use.\n\nIn `db.py` add a new `init_app` function:\n\n```python\ndef init_app(app):\n    # tells Flask to call that function when cleaning up after returning the response\n    app.teardown_appcontext(close_db)\n    # adds a new command that can be called with the flask command\n    app.cli.add_command(init_db_command)\n```\n\n\n\nThen import and call this function from the factory in `__init__.py`.\n\n```python\ndef create_app(test_config=None):\n    # create and configure the app\n    app = ...\n    # existing code omitted\n    \n    # add db functions\n    from . import db\n    db.init_app(app)\n\n    return app\n\n```\n\n\n\n#### Initialize the Database\n\nNow use `init-db`command like this:\n\n```shell\n$ flask --app flaskr init-db\nInitialized the database.\n\n$ tree instance/\ninstance/\nâ””â”€â”€ flaskr.sqlite\n\n0 directories, 1 file\n```\n\n\n\nThe command generates a sqlite db file `flaskr.sqlite` in `instance/` dir. \n\n\n\n### Step 5. Blueprints and Views\n\nReferances:\n\n[Blueprints and Views](https://flask.palletsprojects.com/en/2.2.x/tutorial/views/)\n\n[Use a Flask Blueprint to Architect Your Applications](https://realpython.com/flask-blueprint/#:~:text=Flask%20is%20a%20very%20popular,its%20functionality%20into%20reusable%20components)\n\n\n\nConcept: view\n\nA view is Flask's respond to the outgoing request. Flask uses patterns to match the incoming request URL to the view that should handle it.\n\n\n\nConcept: blueprint\n\nA blueprint is a way to organize a group of related views and other code. Rather than registering views and other code directly with an application, they are registered with a blueprint. Then the blueprint is registered with the application when it is available in the factory function.\n\n\n\n#### Create a Blueprint\n\nFlaskr will have two blueprints:\n\n* auth functions\n* blog posts functions\n\n\n\n`Flaskr/auth.py`\n\n```python\nimport functools\n\nfrom flask import (\n    Blueprint, flash, g, redirect, render_template, request, session, url_for\n)\nfrom werkzeug.security import check_password_hash, generate_password_hash\nfrom flaskr.db import get_db\n\nbp = Blueprint('auth', __name__, url_prefix='/auth')\n\n```\n\nA new `Blueprint` is created:\n\n* with `name`: 'auth'\n* with `import_name`: '\\__name__', helping the blueprint to know where itâ€™s defined\n* with `url_prefix`: will be prepended (added at head)to all the URLs associated with this blueprint.\n\nThen register the blueprint to the app from the factory in the `__init__.py`\n\n```python\ndef create_app(test_config=None):\n    # create and configure the app\n    app = ...\n    # existing code omitted\n    \n\n    # add auth blueprint\n    from . import auth\n    app.register_blueprint(auth.bp)\n\n    return app\n```\n\n\n\n\u003e Referances:\n\u003e\n\u003e [Python functools](https://docs.python.org/3/library/functools.html)\n\n\n\n#### Register view\n\nWhen the user visits the `/auth/register` URL, the `register` view will return [HTML](https://developer.mozilla.org/docs/Web/HTML) with a form for them to fill out. When they submit the form, it will validate their input and either show the form again with an error message or create the new user and go to the login page.\n\n\n\nThe view code is as following in `flaskr/auth.py`\n\n```python\n@bp.route('/register', methods=('GET', 'POST'))\ndef register():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        db = get_db()\n        error = None\n\n        if not username:\n            error = 'Username is required.'\n        elif not password:\n            error = 'Password is required.'\n\n        if error is None:\n            try:\n                db.execute(\n                    'INSERT INTO user (username, password) VALUES (?, ?)',\n                    (username, generate_password_hash(password))\n                )\n                db.commit()\n            except db.IntegrityError:\n                error = f\"User {username} is already registered.\"\n            else:\n                return redirect(url_for('auth.login'))\n        \n        flash(error)\n    \n    return render_template('auth/register.html')\n```\n\n\n\nThe `register` view works as following:\n\n* @bp.route associates the URL `/register` with the `register` view.\n* If the user submited the register form, `request.method == 'POST'`, then validate the input `username` and `password` and store them into the database.\n* If storing the user info succeeds, then redirect to the `auth.login` page.\n* If the user is initially landing on the `register` page, or there was a validation error, the `register.html` will be shown.\n\n\n\n#### Login view\n\nThis view follows the same pattern as `register` view.\n\n```python\n @bp.route('/login', methods=('GET', 'POST'))\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        db = get_db()\n        error = None\n\n        user = db.execute(\n            'SELECT * FROM user WHERE username = ?', (username,)\n        ).fetchone()\n\n        if user is None:\n            error = 'Incorrect username.'\n        elif not check_password_hash(user['password'], password):\n            error = 'Incorrect password.'\n        \n        if error is None:\n            session.clear()\n            session['user_id'] = user['id']\n            return redirect(url_for('index'))\n        \n        flash(error)\n    \n    return render_template('auth/login.html')\n\n```\n\nTips:\n\n* The user info is queried and stored in `user` variable using `fetch_one` function.\n* Validate the `username` and `password` (by `check_password_hash`) inputs.\n* The `session` (a dict storing data across requests) refreshes if login succeeds. (The data is stored in a *cookie* that is sent to the browser, and the browser then sends it back with subsequent requests.)\n\n\n\nWe can get user info at the beginning of each request via `session`:\n\n```python\n@bp.before_app_request\ndef load_logged_in_user():\n    user_id = session.get('user_id')\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = get_db().execute(\n            'SELECT * FROM user WHERE id = ?', (user_id,)\n        ).fetchone()\n```\n\nTips:\n\n* [`bp.before_app_request()`](https://flask.palletsprojects.com/en/2.2.x/api/#flask.Blueprint.before_app_request) registers a function that runs before the view function, no matter what URL is requested. \n* `load_logged_in_user`  gets that user info from the database via `session` and stores it on [`g.user`](https://flask.palletsprojects.com/en/2.2.x/api/#flask.g), which lasts for the length of the request. \n\n\n\n#### Logout view\n\nThe Logout view removes the user id from the [`session`](https://flask.palletsprojects.com/en/2.2.x/api/#flask.session). \n\n```python\n@bp.route('/logout')\ndef logout():\n    session.clear()\n    return redirect(url_for('index'))\n```\n\n\n\n#### Require Auth in Other views\n\nCreating, editing and deleting blog posts requires the user to be logged in. Use a **decorator** to achieve this.\n\n```python\ndef login_required(view):\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for('auth.login'))\n        \n        return view(**kwargs)\n    \n    return wrapped_view \n```\n\nThis decorator wraps the view in this way: if a user is not logged in, then redirect to login page; if logged in, return the orginal view.\n\n\n\n### Step 6. Templates\n\nThough `auth.login` view has been created, a `TemplateNotFound` error will be raised when you visit http://127.0.0.1:5000/auth/login.\n\n\n\n![image-20221202154357379](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202154357379.png)\n\nThis is because the view calls `render_template()`, but no templates are created. \n\n\n\nTips:\n\n* The template files will be stored in the `templates` directory inside the `flaskr` package.\n* Templates are files that contain static data as well as placeholders for dynamic data. \n* A template is rendered with specific data to produce a final document. Flask uses the [Jinja](https://jinja.palletsprojects.com/templates/) template library to render templates.\n* Special delimiters are used to distinguish Jinja syntax from the static data in the template. \n  * Anything between `{{` and `}}` is an expression that will be output to the final document. \n  * `{%` and `%}` denotes a control flow statement like `if` and `for`\n\n\n\n#### The Base Layout\n\nEach page in the app has the same basic layout around a different body.Instead of writing the entire HTML structure in each template, each template will extend a base template and override specific sections.\n\n\n\nFile: `flaskr/templates/base.html`\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003ctitle\u003e{% block title %}{% endblock %} - Flaskr\u003c/title\u003e\n\u003clink rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"\u003e\n\u003cnav\u003e\n   \u003ch1\u003eFlaskr\u003c/h1\u003e\n   \u003cul\u003e\n    {% if g.user %}\n        \u003cli\u003e\u003cspan\u003e{{ g.user['username'] }}\u003c/span\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"{{ url_for('auth.logout') }}\"\u003eLog Out\u003c/a\u003e\u003c/li\u003e\n    {% else %}\n        \u003cli\u003e\u003ca href=\"{{ url_for('auth.register') }}\"\u003eRegister\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"{{ url_for('auth.login') }}\"\u003eLog \u003cInput:c\u003e\u003c/Input:c\u003e\u003c/a\u003e\u003c/li\u003e\n    {% endif %}\n   \u003c/ul\u003e \n\u003c/nav\u003e\n\n\u003csection class=\"content\"\u003e\n    \u003cheader\u003e\n        {% block header %}\n        {% endblock %}\n    \u003c/header\u003e\n    {% for message in get_flashed_messages() %}\n        \u003cdiv class=\"flash\"\u003e{{ message }}\u003c/div\u003e\n    {% endfor %}\n    \n    {% block content %}\n    {% endblock %}\n\u003c/section\u003e\n\n```\n\n\n\n\u003eNotes: Using `Jinja Snippets` and `HTML CSS Support` extensions in vscode is helpful to write html code of Jinja templates.\n\n\n\nThere are three blocks defined here that will be overridden in the other templates:\n\n* `{% block title %}` will change the title displayed in the browserâ€™s tab and window title.\n* `{% block header %}` is similar to `title` but will change the title displayed on the page.\n* `{% block content %}` is where the content of each page goes, such as the login form or a blog post.\n\n\n\nThe base template is directly in the `templates` directory. To keep the others organized, the templates for a blueprint will be placed in a directory with the same name as the blueprint.\n\n#### Register template\n\nFile: `flaskr/templates/register.html`\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Register{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"post\"\u003e\n        \u003clabel for=\"username\"\u003eUsername\u003c/label\u003e\n        \u003cinput name=\"username\" id=\"username\" required\u003e        \n        \u003clabel for=\"password\"\u003ePassword\u003c/label\u003e\n        \u003cinput type=\"password\" name=\"password\" id=\"password\" required\u003e\n        \u003cinput type=\"submit\" value=\"Register\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n#### Log In template\n\nThis is identical to the register template except for the title and submit button.\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Log In{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"post\"\u003e\n        \u003clabel for=\"username\"\u003eUsername\u003c/label\u003e\n        \u003cinput name=\"username\" id=\"username\" required\u003e        \n        \u003clabel for=\"password\"\u003ePassword\u003c/label\u003e\n        \u003cinput type=\"password\" name=\"password\" id=\"password\" required\u003e\n        \u003cinput type=\"submit\" value=\"Log In\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n\n#### Register a user\n\nVisit http://127.0.0.1:5000/auth/register\n\n![image-20221202174125671](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174125671.png)\n\nIf no password inputed, you will see:\n\n![image-20221202174349791](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174349791.png)\n\nIf register succeeds, it will redirect to login page:\n\n![image-20221202174548748](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174548748.png)\n\nIf incorrect password is inputed, you will get `Incorrect password` warning:\n\n![image-20221202174721957](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202174721957.png)\n\n\n\n### Step 7. Static Files\n\nUse css file in the `flask/static` directory to give the webpages some style. In the `base.html` template there is already a link to `style.css` file.\n\n```html\n{{ url_for('static', filename='style.css') }}\n```\n\n\n\nFile: flask/static/style.css\n\n```css\nhtml { font-family: sans-serif; background: #eee; padding: 1rem; }\nbody { max-width: 960px; margin: 0 auto; background: white; }\nh1 { font-family: serif; color: #377ba8; margin: 1rem 0; }\na { color: #377ba8; }\nhr { border: none; border-top: 1px solid lightgray; }\nnav { background: lightgray; display: flex; align-items: center; padding: 0 0.5rem; }\nnav h1 { flex: auto; margin: 0; }\nnav h1 a { text-decoration: none; padding: 0.25rem 0.5rem; }\nnav ul  { display: flex; list-style: none; margin: 0; padding: 0; }\nnav ul li a, nav ul li span, header .action { display: block; padding: 0.5rem; }\n.content { padding: 0 1rem 1rem; }\n.content \u003e header { border-bottom: 1px solid lightgray; display: flex; align-items: flex-end; }\n.content \u003e header h1 { flex: auto; margin: 1rem 0 0.25rem 0; }\n.flash { margin: 1em 0; padding: 1em; background: #cae6f6; border: 1px solid #377ba8; }\n.post \u003e header { display: flex; align-items: flex-end; font-size: 0.85em; }\n.post \u003e header \u003e div:first-of-type { flex: auto; }\n.post \u003e header h1 { font-size: 1.5em; margin-bottom: 0; }\n.post .about { color: slategray; font-style: italic; }\n.post .body { white-space: pre-line; }\n.content:last-child { margin-bottom: 0; }\n.content form { margin: 1em 0; display: flex; flex-direction: column; }\n.content label { font-weight: bold; margin-bottom: 0.5em; }\n.content input, .content textarea { margin-bottom: 1em; }\n.content textarea { min-height: 12em; resize: vertical; }\ninput.danger { color: #cc2f2e; }\ninput[type=submit] { align-self: start; min-width: 10em; }\n```\n\n\n\nAfter that, reload the login page, and you can see:\n\n![image-20221202175806462](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221202175806462.png)\n\n\n\n### Step 8. Blog Blueprint\n\nImplement the blog blueprint to allow a logged-in user to create posts and edit/delete the posts of his/her own.\n\n\u003e Note: As you implement each view, keep the development server running. As you save your changes, try going to the URL in your browser and testing them out.\n\n\n\n#### The Blog Blueprint\n\n\n\nDefine blog blueprint in file: `flask/blog.py`\n\n```Â \nfrom flask import (\n    Blueprint, flash, g, redirect, render_template, request, session, url_for\n)\nfrom werkzeug.exceptions import abort\n\nfrom flaskr.db import get_db\nfrom flaskr.auth import login_required\n\nbp = Blueprint('auth', __name__)\n```\n\n\n\nRegister this blueprint in the app factory in file : `flask/__init__.py`\n\n```python\ndef create_app(test_config=None):\n    # create and configure the app\n    app = ...\n    # existing code omitted\n\n    # add blog blueprint\n    from . import blog\n    app.register_blueprint(blog.bp)\n    app.add_url_rule('/', endpoint='index')\n\n    return app\n```\n\n\n\nUnlike the auth blueprint, the blog blueprint does not have a `url_prefix`. So the index view will be at `/`, the create view at `/create`. The blog is the main feature of Flaskr app, so it makes sense that the blog index will be the main index.\n\n\n\nThe endpoint for index view in blog blueprint will be `blog.index`. The `app.add_url_rule('/', endpoint='index')` code associates the endpoint name 'index' with the `/` url so that `url_for('index')` or `url_for('blog.index')` will both work, generating the same `/` URL either way. \n\n\n\n#### Index: view and template\n\nThe index view shows all the posts of the logged-in user, order by created time desc. Use SQL's JOIN clause.\n\n\n\nDefine index view in file: `flaskr/blog.py`:\n\n```python\n@bp.route('/')\ndef index():\n    db = get_db()\n    posts = db.execute(\n        'SELECT p.id, title, body, created, author_id, username'\n        ' FROM post p JOIN user u ON p.author_id = u.id'\n        ' ORDER BY created DESC'\n    ).fetchall()\n    return render_template('blog/index.html', posts=posts)\n```\n\n\n\nDefine index template in file: `flaskr/templates/index.html`:\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Posts{% endblock %}\u003c/h1\u003e\n    {% if g.user %}\n        \u003ca class=\"action\" href=\"{{ url_for('blog.create') }}\" \u003eNew\u003c/a\u003e\n    {% endif %}\n{% endblock %}\n\n{% block content %}\n    {% for post in posts %}\n        \u003carticle class=\"post\"\u003e\n            \u003cheader\u003e\n                \u003cdiv\u003e\n                    \u003ch1\u003e{{ post['title'] }}\u003c/h1\u003e\n                    \u003cdiv class=\"about\"\u003eby {{ post['username'] }} on {{ post['created'].strftime('%Y-%m-%d') }}\u003c/div\u003e\n                \u003c/div\u003e\n                {% if g.user['id'] == post['author_id'] %}\n                    \u003ca class=\"action\" href=\"{{ url_for('blog.update', id=post['id']) }}\"\u003eEdit\u003c/a\u003e\n                {% endif %}\n            \u003c/header\u003e\n        \u003c/article\u003e\n        {% if not loop.last %}\n            \u003chr\u003e\n        {% endif %}\n    {% endfor %}\n{% endblock %}\n```\n\nTips:\n\n* when a user is logged in, the `header` block adds a link to the `create` view.\n* when the user is the author of a post, an `Edit` link to the `update` view will be seen.\n* `loop.last` is a special variable of Jinja's loop.\n\n\n\n#### Create: view and template\n\nThe `blog.create` view acts the similar way as `auth.register` view.\n\nDefine `blog.create` view in file: `flaskr/blog.py`:\n\n```python\n@bp.route('/create', methods=('GET', 'POST'))\n@login_required\ndef create():\n    if request.method == 'POST':\n        title = request.form['title']\n        body = request.form['body']\n        error = None\n\n        if not title:\n            error = 'Title is requested.'\n        \n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                'INSERT INTO post (title, body, author_id)'\n                ' VALUES (?, ?, ?)',\n                (title, body, g.user['id'])\n            )\n            db.commit()\n            return redirect(url_for('blog.index'))\n            \n    return render_template('blog/create.html')\n```\n\n\n\nTips:\n\n* if a new post is POSTed, add it into the database.\n* Or display the form of creating post in the `create` template.\n* Login_required decorator is used here to insure the user is logged-in before a new post is created.\n\n\n\nDefine the `create` template in file: `flaskr/templates/blog/create.html`\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}New Post{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"POST\"\u003e\n        \u003clabel for=\"title\"\u003eTitle\u003c/label\u003e\n        \u003cinput name=\"title\" id=\"title\" value=\"{{ request.form['title'] }}\" required\u003e\n        \u003clabel for=\"body\"\u003eBody\u003c/label\u003e\n        \u003ctextarea name=\"body\" id=\"body\"\u003e{{ request.form['body'] }}\u003c/textarea\u003e\n        \u003cinput type=\"submit\" value=\"Save\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n\n#### Update: view and template\n\nWrite a `get_post()` function to fetch a post by id and check if the author equals the logged in user. This function will be used in `update` and `delete` view.\n\n\n\nDefine get_post() function in file: `flaskr/blog.py`:\n\n```python\ndef get_post(id, check_author=True):\n    post = get_db().execute(\n        'SELECT p.id, title, body, created, author_id, user_name'\n        ' FROM post p JOIN user u ON p.author_id=u.id'\n        ' WHERE p.id = ?',\n        (id,)\n    ).fetchone()\n\n    if post is None:\n        abort(404, f'Post id {id} does not exist.')\n\n    if check_author and post['author_id'] != g.user['id']:\n        abort(403)\n    \n    return post\n```\n\nTips:\n\n* `abort()` will raise a exception that returns an HTTP status code like 404 (Not Found) or 403 (Forbidden).\n\n\n\nDefine `update` view in file: `flaskr/blog.py`:\n\n```python\n\n@bp.route('/\u003cint:id\u003e/update', methods=('GET', 'POST'))\n@login_required\ndef update(id):\n    post = get_post(id)\n\n    if request.method == 'POST':\n        title = request.form['title']\n        body = request.form['body']\n        error = None\n\n        if not title:\n            error = 'Title is required.'\n        \n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                'UPDATE post SET title = ?, body = ?'\n                ' WHERE id = ?',\n                (title, body, id)\n            )\n            db.commit()\n            return redirect(url_for('blog.index'))\n        \n    return render_template('blog/update.html', post=post)\n```\n\nTips:\n\n* `update` view takes an argument `id` , which corresponds to the `\u003cint:id\u003e` in the route. A real URL will look like `/1/update`. And the url_for() function also needs to be passed the `id` argument in the way of `url_for('blog.update', id=post['id'])`.\n\n\n\nDefine the update template in file: `flaskr/templates/blog/update.html`\n\n```html\n{% extends 'base.html' %}\n\n{% block header %}\n    \u003ch1\u003e{% block title %}Edit \"{{ post['title'] }}\"{% endblock %}\u003c/h1\u003e\n{% endblock %}\n\n{% block content %}\n    \u003cform method=\"POST\"\u003e\n        \u003clabel for=\"title\"\u003eTitle\u003c/label\u003e\n        \u003cinput name=\"title\" id=\"title\" value=\"{{ request.form['title'] or post['title'] }}\" required\u003e\n        \u003clabel for=\"body\"\u003eBody\u003c/label\u003e\n        \u003ctextarea name=\"body\" id=\"body\"\u003e{{ request.form['body'] or post['body'] }}\u003c/textarea\u003e\n        \u003cinput type=\"submit\" value=\"Save\"\u003e\n    \u003c/form\u003e\n    \u003chr\u003e\n    \u003cform action=\"{{ url_for('blog.delete', id=post['id']) }}\" method=\"post\"\u003e\n        \u003cinput type=\"submit\" value=\"Delete\" class=\"danger\" onclick=\"return confirm('Are you sure?');\"\u003e\n    \u003c/form\u003e\n{% endblock %}\n```\n\n\n\nTips:\n\n* This templates has two forms:\n  * The first one to edit the current post(`/\u003cid\u003e/update`)\n  * The other one to delete the post\n* The pattern `{{ request.form['title'] or post['title'] }}` is used to choose what data appears in the form. \n  * When the form hasnâ€™t been submitted, the original `post` data appears\n  * but if invalid form data was posted you want to display that so the user can fix the error, so `request.form` is used instead.\n\n\n\n#### Delete: view\n\nThe delete view has no template. Define it:\n\n```python\n@bp.route('/\u003cint:id\u003e/delete', methods=('POST',))\n@login_required\ndef delete(id):\n    get_post(id)\n    db = get_db()\n    db.execute('DELETE FROM post WHERE id = ?', (id,))\n    db.commit()\n    return redirect(url_for('blog.index'))\n\n```\n\n\n\nNow all code are finished. Try it!\n\n\n\nlog in \n\n![image-20221203225939183](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203225939183.png)\n\n\n\nLog out\n\n![image-20221203230028831](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230028831.png)\n\n\n\nCreate a post\n\n![image-20221203230103978](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230103978.png)\n\n\n\nWriting\n\n![image-20221203230232647](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230232647.png)\n\n\n\nsaved\n\n![image-20221203230339421](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230339421.png)\n\nedit\n\n![image-20221203230548770](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221203230548770.png)\n\n\n\nGreat!\n\n\n\n### Step 9. Make the Project Installable\n\n#### Describe the Project\n\nIn order to make the project Installabe, write a `setup.py` file to describe the project and its dependencies.\n\n```python\nfrom setuptools import find_packages, setup\n\nsetup(\n    name='flaskr',\n    version='1.0.0',\n    description='a simple blog post app based on Flask',\n    packages=find_packages(),\n    include_package_data=True,\n    requires=[\n        'flask'\n    ]\n)\n```\n\nTips:\n\n* `packages` tells Python what package directories to include, and `find_packages()` function finds them automatically.\n* To include other files, such as the static and templates directories, `include_package_data` is set. \n* `requires` tells what modules need to be installed as the project's dependencies.\n* Python needs another file named `MANIFEST.in` to tell what this other data is.\n\n\n\nFile `MANIFEST.in`\n\n```\ninclude flaskr/schema.sql\ngraft flaskr/static\ngraft flaskr/templates\nglobal-exclude *.pyc\n```\n\nThis tells Python to copy everything in the `static` and `templates` directories, and the `schema.sql` file, but to exclude all bytecode files.\n\n\n\n#### Install the Project\n\nUse pip to install your project in the virtual environment.\n\n```shell\n$ pip install -e .\nLooking in indexes: https://pypi.douban.com/simple\nObtaining file:///Users/yangls06/work/flask/flask-tutorial/flaskr\n  Preparing metadata (setup.py) ... done\nInstalling collected packages: flaskr\n  Running setup.py develop for flaskr\nSuccessfully installed flaskr-1.0.0\n```\n\nThis tells pip to find `setup.py` in the current directory and install it in *editable* or *development* mode. \n\nEditable mode means that as you make changes to your local code, youâ€™ll only need to re-install if you change the metadata about the project, such as its dependencies.\n\n\n\nYou can observe that the project is now installed with `pip list`.\n\n```shell\n$ pip list\nPackage            Version Editable project location\n------------------ ------- ------------------------------------------------\nclick              8.1.3\nFlask              2.2.2\nflaskr             1.0.0   /Users/yangls06/work/flask/flask-tutorial/flaskr\nimportlib-metadata 5.1.0\nitsdangerous       2.1.2\nJinja2             3.1.2\nMarkupSafe         2.1.1\npip                22.3.1\nsetuptools         58.1.0\nWerkzeug           2.2.2\nzipp               3.11.0\n```\n\nNothing changes from how youâ€™ve been running your project so far. `--app` is still set to `flaskr` and `flask run` still runs the application, but you can call it from **anywhere**, not just the `flask-tutorial` directory.\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-html-css":{"title":"Hands on HTML \u0026 CSS","content":"\n# Hands on HTML \u0026 CSS\n\n\n\n\u003e This is my practices on HTML\u0026CSS in order to develop the front-end of websites.\n\n## References\n\n[MDN HTML](https://developer.mozilla.org/zh-CN/docs/Web/HTML)\n\n[HTML basics](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics)\n\n[CSS basics](https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/CSS_basics)\n\n\n\n## HTML Basics\n\n### HTML element\n\nA HTML document describes the content of a web page, which contains many [HTML element](https://developer.mozilla.org/zh-CN/docs/Glossary/Element)s:\n\n![Example: in \u003cp class=\"nice\"\u003eHello world!\u003c/p\u003e, '\u003cp class=\"nice\"\u003e' is an opening tag, 'class=\"nice\"' is an attribute and its value, 'Hello world!' is enclosed text content, and '\u003c/p\u003e' is a closing tag.](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/anatomy-of-an-html-element.png)\n\n\n\n### Basic HTML\n\nA basic `index.html` is:\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eMy test html page\u003c/title\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cp\u003e\u003cstrong\u003eHTML tag: h1~h5\u003c/strong\u003e\u003c/p\u003e\n    \u003ch1\u003eHTML Basics\u003c/h1\u003e\n    \u003ch2\u003e2023å¹´ï¼Œå´­æ–°çš„ä¸€å¹´\u003c/h2\u003e\n    \u003ch3\u003e2023å¹´ï¼Œå´­æ–°çš„ä¸€å¹´\u003c/h3\u003e\n    \u003c!-- \u003ch4\u003e2023å¹´ï¼Œå´­æ–°çš„ä¸€å¹´\u003c/h4\u003e\n    \u003ch5\u003e2023å¹´ï¼Œå´­æ–°çš„ä¸€å¹´\u003c/h5\u003e\n    \u003ch6\u003e2023å¹´ï¼Œå´­æ–°çš„ä¸€å¹´\u003c/h6\u003e --\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: img\u003c/strong\u003e\u003c/p\u003e\n    \u003cimg src=\"images/2023.jpg\" alt=\"my test image\" width=\"600\" height=\"400\"\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: p\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eæ–°çš„è½¬æœºå’Œé—ªé—ªçš„æ˜Ÿæ–—ï¼Œæ­£åœ¨ç¼€æ»¡æ²¡æœ‰é®æ‹¦çš„å¤©ç©ºã€‚è¿™æ˜¯5000å¹´çš„è±¡å½¢æ–‡å­—ï¼Œè¿™æ˜¯æœªæ¥äººä»¬å‡è§†çš„çœ¼ç›ğŸ‘ã€‚\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: strong\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eæ–°çš„è½¬æœºå’Œé—ªé—ªçš„æ˜Ÿæ–—ï¼Œæ­£åœ¨ç¼€æ»¡æ²¡æœ‰é®æ‹¦çš„å¤©ç©ºã€‚è¿™æ˜¯5000å¹´çš„è±¡å½¢æ–‡å­—ï¼Œ\u003cstrong\u003eè¿™æ˜¯æœªæ¥äººä»¬å‡è§†çš„çœ¼ç›ğŸ‘ã€‚\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: Unordered List\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eAt Mozilla, we're a global community of\u003c/p\u003e\n    \u003cul\u003e\n        \u003cli\u003etechnologists\u003c/li\u003e\n        \u003cli\u003ethinkers\u003c/li\u003e\n        \u003cli\u003ebuilders\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003eworking togetherâ€¦\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: Ordered List\u003c/strong\u003e\u003c/p\u003e\n    \u003cp\u003eAt Mozilla, we're a global community of\u003c/p\u003e\n    \u003col\u003e\n        \u003cli\u003etechnologists\u003c/li\u003e\n        \u003cli\u003ethinkers\u003c/li\u003e\n        \u003cli\u003ebuilders\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003eworking togetherâ€¦\u003c/p\u003e\n\n    \u003cp\u003e\u003cstrong\u003eHTML tag: a\u003c/strong\u003e\u003c/p\u003e\n    \u003ca href=\"https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/HTML_basics\"\u003emdn HTML Basics\u003c/a\u003e\n\u003c/body\u003e\n```\n\n\n\nThe page based on it:\n\n![e3ed2a84-b999-4113-b138-310a22c9ab35](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/e3ed2a84-b999-4113-b138-310a22c9ab35.jpeg) \n\n## CSS Basics\n\n[CSS basics](https://developer.mozilla.org/zh-CN/docs/Learn/Getting_started_with_the_web/CSS_basics)\n\nCSS is all about `style` (how it looks) of web content. It concerns the following questions:\n\n* How do I make text red? \n* How do I make content display at a certain location in the (webpage) layout? \n* How do I decorate my webpage with background images and colors?\n\n\n\n### File Layout\n\n```shell\n$ tree -L 2\n.\nâ”œâ”€â”€ images\nâ”‚Â Â  â””â”€â”€ 2023.jpg\nâ”œâ”€â”€ index.html\nâ””â”€â”€ styles\n    â””â”€â”€ style.css\n```\n\nWe store the CSS content in a `.css` file under the `styles` folder. The content of `style.css` is like:\n\n```css\np {\n    color: red;\n}\n```\n\nand imports it in `index.html`:\n\n```html\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eMy test html page\u003c/title\u003e\n  \n    \u003c!-- import css here --\u003e\n    \u003clink href=\"styles/style.css\" rel=\"stylesheet\"\u003e \n\u003c/head\n```\n\nwhich makes all the `\u003cp\u003e` elements red, as following:\n\n![image-20230105145429781](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105145429781.png)\n\n\n\n### CSS Rulesets\n\nThe basic element of CSS is called `ruleset`, which is explained as following:\n\n![CSS p declaration color red](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/css-declaration-small.png)\n\n* SelectorIt defines the element(s) to be styled (in this example, `\u003cp\u003e` elements). To style a different element, change the selector.\n\n* Declaration. It specifies which of the element's **properties** you want to style, like `color: red;`.\n\n\n\nYou can change the style:\n\n```css\np {\n    color: blue;\n    width: 500px;\n    border: 1px solid black;\n  }\n```\n\n\n\n![image-20230105160938495](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105160938495.png)\n\nYou can select multiple elements using `,`:\n\n```css\np,\nli,\nh1 {\n  color: red;\n}\n```\n\nwhich generates:\n\n![image-20230105161248652](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105161248652.png)\n\n### Different Types of Selectors\n\nThere are 5 basic types of selectors.\n\n| Selector name                                              | What does it select                                          | Example                                                      |\n| :--------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n| Element selector (sometimes called a tag or type selector) | All HTML elements of the specified type.                     | `p` selects `\u003cp\u003e`                                            |\n| ID selector                                                | The element on the page with the specified ID. On a given HTML page, each id value should be unique. | `#my-id` selects `\u003cp id=\"my-id\"\u003e` or `\u003ca id=\"my-id\"\u003e`        |\n| Class selector                                             | The element(s) on the page with the specified class. Multiple instances of the same class can appear on a page. | `.my-class` selects `\u003cp class=\"my-class\"\u003e` and `\u003ca class=\"my-class\"\u003e` |\n| Attribute selector                                         | The element(s) on the page with the specified attribute.     | `img[src]` selects `\u003cimg src=\"myimage.png\"\u003e` but not `\u003cimg\u003e` |\n| Pseudo-class selector                                      | The specified element(s), but only when in the specified state. (For example, when a cursor hovers over a link.) | `a:hover` selects `\u003ca\u003e`, but only when the mouse pointer is hovering over the link. |\n\n### Fonts and Text\n\nFirst, find the [output from Google Fonts](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/What_will_your_website_look_like#font) that you previously saved from [What will your website look like?](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/What_will_your_website_look_like). Add the `link` element somewhere inside your `index.html`'s head (anywhere between the `\u003chead\u003e` tags). It looks something like this:\n\n```html\n\u003clink href=\"https://fonts.googleapis.com/css?family=Open+Sans\" rel=\"stylesheet\" /\u003e\n```\n\n\n\nNext, delete the existing rule you have in your `style.css` file. It was a good test, but let's not continue with lots of red text.\n\n\n\nAdd the following lines (shown below), replacing the `font-family` assignment with your `font-family` selection. The property `font-family` refers to the font(s) you want to use for text. This rule defines a global base font and font size for the whole page. Since `\u003chtml\u003e` is the parent element of the whole page, all elements inside it inherit the same `font-size` and `font-family`.\n\n```css\nhtml {\n  font-size: 10px; /* px means \"pixels\": the base font size is now 10 pixels high */\n  font-family: \"Open Sans\", sans-serif; /* this should be the rest of the output you got from Google Fonts */\n}\n```\n\n\n\nThe page is like this now:\n\n![image-20230105164921181](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105164921181.png)\n\n\n\nNow let's set font sizes for elements that will have text inside the HTML body (`\u003ch1\u003e`, `\u003cli\u003e` and `\u003cp\u003e`). We'll also center the heading. Finally, let's expand the second ruleset (below) with settings for line height and letter spacing to make body content more readable.\n\n```css\nh1 {\n  font-size: 60px;\n  text-align: center;\n}\n\np,\nli {\n  font-size: 16px;\n  line-height: 2;\n  letter-spacing: 1px;\n}\n```\n\nwhich changes the font-size and other styles:\n\n![image-20230105165727652](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105165727652.png)\n\n\n\n### The Box Model\n\nMost HTML elements can be thought of as boxes sitting on the top of other boxes. CSS is about setting the size, color and position of boxes.\n\n![Three boxes sat inside one another. From outside to in they are labelled margin, border and padding](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/CSS_basics/box-model.png)\n\n Each box taking up space on your page has properties like:\n\n- `padding`, the space around the content. In the example below, it is the space around the paragraph text.\n- `border`, the solid line that is just outside the padding.\n- `margin`, the space around the outside of the border.\n\nIn this section we also use:\n\n- `width` (of an element).\n- `background-color`, the color behind an element's content and padding.\n- `color`, the color of an element's content (usually text).\n- `text-shadow` sets a drop shadow on the text inside an element.\n- `display` sets the display mode of an element. (keep reading to learn more)\n\n\n\n### Changing the Page Color\n\n```css\nhtml {\n  background-color: #00539f;\n}\n```\n\n### Styling the Body\n\n```css\nbody {\n  width: 600px;\n  margin: 0 auto;\n  background-color: #ff9500;\n  padding: 0 20px 20px 20px;\n  border: 5px solid black;\n}\n```\n\n- `width: 600px;` This forces the body to always be 600 pixels wide.\n- `margin: 0 auto;` When you set two values on a property like `margin` or `padding`, the first value affects the element's top *and* bottom side (setting it to `0` in this case); the second value affects the left *and* right side. (Here, `auto` is a special value that divides the available horizontal space evenly between left and right). You can also use one, two, three, or four values, as documented in [Margin Syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/margin#syntax).\n- `background-color: #FF9500;` This sets the element's background color. This project uses a reddish orange for the body background color, as opposed to dark blue for the [``](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/html) element. (Feel free to experiment.)\n- `padding: 0 20px 20px 20px;` This sets four values for padding. The goal is to put some space around the content. In this example, there is no padding on the top of the body, and 20 pixels on the right, bottom and left. The values set **top, right, bottom, left**, in that order. As with `margin`, you can use one, two, three, or four values, as documented in [Padding Syntax](https://developer.mozilla.org/en-US/docs/Web/CSS/padding#syntax).\n- `border: 5px solid black;` This sets values for the width, style and color of the border. In this case, it's a five-pixelâ€“wide, solid black border, on all sides of the body.\n\n### Positioning and Styling the Main Page Title\n\n```css\nh1 {\n  margin: 0;\n  padding: 20px 0;\n  color: #00539f;\n  text-shadow: 3px 3px 1px black;\n}\n```\n\n`text-shadow` applies a shadow to the text content of the element. Its four values are:\n\n- The first pixel value sets the **horizontal offset** of the shadow from the text: how far it moves across.\n- The second pixel value sets the **vertical offset** of the shadow from the text: how far it moves down.\n- The third pixel value sets the **blur radius** of the shadow. A larger value produces a more fuzzy-looking shadow.\n- The fourth value sets the base color of the shadow.\n\n### Centering the Image\n\n```css\nimg {\n  display: block;\n  margin: 0 auto;\n}\n```\n\nNext, we center the image to make it look better. We could use the `margin: 0 auto` trick again as we did for the body. But there are differences that require an additional setting to make the CSS work.\n\nThe `body` is a **block** element, meaning it takes up space on the page. The margin applied to a block element will be respected by other elements on the page. In contrast, images are **inline** elements, for the auto margin trick to work on this image, we must give it block-level behavior using `display: block;`\n\n\n\nNow, the page looks like this:\n\n![image-20230105174108098](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230105174108098.png)\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-javascript":{"title":"Hands on JavaScript","content":"\n\n\n# JavaScriptåŸºç¡€\n\næœ€è¿‘å·¥ä½œä¸­ä¼šæ¶‰åŠåˆ°å‰ç«¯å·¥ä½œï¼Œå› æ­¤è¦ç”¨åˆ°JavaScriptï¼Œæ•…è€Œå­¦ä¹ ä¸€æ³¢ã€‚å‚è€ƒï¼š\n\n* https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_Overview\n\n## ç¯å¢ƒå‡†å¤‡ï¼šJupyter Notebook + IJavaScript\n\næ‰“ç®—åœ¨Jupyter Notebookä¸­ä½¿ç”¨JavaScriptï¼Œä¾¿äºäº¤äº’å¼çš„å­¦ä¹ å’Œæ¢æŸ¥ã€‚äºæ˜¯æ‰¾åˆ°äº†IJavascriptè¿™ä¸ªJupyter Notebookçš„JavaScript Kernel. \nå…¶å®‰è£…å’Œä½¿ç”¨å‚è€ƒä¸»é¡µï¼šhttp://n-riesco.github.io/ijavascript/\n\n\u003e æ³¨æ„ğŸ“¢ï¼šåœ¨å®‰è£…çš„è¿‡ç¨‹ä¸­ï¼ˆbrew install pkg-config node zeromqï¼‰ï¼Œå¯èƒ½ä¼šå‡ºç°â€œError: No such file or directory @ rb_sysopenâ€çš„é”™è¯¯ï¼Œå¯ä»¥å‚è€ƒï¼šhttps://blog.csdn.net/weixin_43770545/article/details/127715990, åšç›¸åº”çš„é—®é¢˜æ’æŸ¥å’Œä¿®å¤ã€‚\n\nå®‰è£…å¥½IJavaScriptä¹‹åï¼Œå¯ä»¥ç›´æ¥é€šè¿‡jupyter-labï¼ˆæˆ–è€…jupyter notebookï¼‰å‘½ä»¤å¯åŠ¨ï¼Œç„¶åé€‰æ‹©JavaScriptå†…æ ¸çš„Notebookã€‚\n\n![image-20221222100114102](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221222100114102.png)\n\nç„¶åå°±èƒ½åœ¨å…¶ä¸­æµ‹è¯•å’Œæ¢ç´¢JavaScriptçš„å„ç§åŠŸèƒ½ã€‚\n\nä¸‹é¢æ˜¯è·Ÿéšhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_Overview æ•™ç¨‹ä¸€è·¯runä¸‹æ¥çš„ç»“æœã€‚\n\n## Data typesï¼šæ•°æ®ç±»å‹\nJavaScriptä¸­æœ‰7ä¸­åŸç”Ÿç±»å‹ï¼š\n* Numberï¼šé™¤äº†éå¸¸éå¸¸å¤§çš„æ•´æ•°ï¼ŒNumberå¯ä»¥è¡¨è¾¾ä»»æ„æ•°å­—ï¼ˆæ•´æ•°ã€æµ®ç‚¹æ•°å‡å¯ï¼‰ã€‚\n* Bigintï¼šéå¸¸éå¸¸å¤§çš„æ•´æ•°ã€‚\n* Stringï¼šå­—ç¬¦ä¸²ã€‚\n* Booleanï¼šå¸ƒå°”å‹ï¼Œtrue/falseã€‚\n* Symbolï¼šç±»ä¼¼uuidï¼Œcreating unique identifiers that won't collideã€‚\n* Undefinedï¼šæ²¡æœ‰è¢«èµ‹å€¼çš„å˜é‡ã€‚\n* Nullï¼šnon-value\n\nå…¶ä»–çš„æ•°æ®éƒ½å±äº`Object`ï¼ŒåŒ…æ‹¬ï¼š\n* Functionï¼šå‡½æ•°ä¸æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ•°æ®ç»“æ„ï¼Œå®ƒåªæ˜¯å¯ä»¥è¢«è°ƒç”¨çš„ä¸€ç§ç‰¹æ®Šçš„objectã€‚\n* Array\n* Date\n* RegExp\n* Error\n\n\u003e ç–‘é—®â“ï¼šæ²¡æœ‰dictæˆ–è€…mapå—ï¼Ÿ\n\n### Numbers\n\nJavaScriptæœ‰ä¸¤ç§æ•°å­—ç±»å‹ï¼šNumberå’ŒBigintã€‚\n\nNumberæ—¢èƒ½è¡¨ç¤ºæ•´æ•°ï¼ˆ-(2^53-1)~2^53-1ï¼‰,åˆèƒ½è¡¨ç¤ºæµ®ç‚¹æ•°ï¼ˆæœ€å¤§å€¼ï¼š1.79\\*10^308ï¼‰\n\n\n```javascript\nconsole.log(3/2); // 1.5, not 1\n```\n\n    1.5\n\n\n\n```javascript\n// æµ®ç‚¹æ•°å¯ä»¥å­˜åœ¨ä¸ç²¾ç¡®çš„æƒ…å†µ\nconsole.log(0.1+0.2)\n```\n\n    0.30000000000000004\n\n\n\n```javascript\n// Number literals can also have prefixes to indicate the base \n// (binary, octal, decimal, or hexadecimal), or an exponent suffix.\n\nconsole.log(0b111110111); // 503\nconsole.log(0o767); // 503\nconsole.log(0x1f7); // 503\nconsole.log(5.03e2); // 503\n\n```\n\n    503\n    503\n    503\n    503\n\n\nBigintç”¨æ¥æŒ‡å®šæ•°å€¼æ˜¯æ•´æ•°ï¼Œå®ƒæ˜¯åœ¨æ•°å­—åé¢è·Ÿä¸€ä¸ªåç¼€ï¼š`n`\n\n\n```javascript\nconsole.log(-3n)\n```\n\n    -3n\n\n\n\n```javascript\n// console.log(-3.1n)\n```\n\n\n```javascript\nconsole.log(-3n/2n)\n```\n\n    -1n\n\n\n\n```javascript\n// Bigintä¸Numberä¸èƒ½æ··åˆè¿ç®—\n// console.log(-3n + 2); //TypeError: Cannot mix BigInt and other types, use explicit conversions\n```\n\nMathæ˜¯ä¸€ä¸ªæä¾›æ ‡å‡†æ•°æ®è¿ç®—çš„Objectã€‚\n\n\n```javascript\nMath.sin(3.5);\n```\n\n\n\n\n    -0.35078322768961984\n\n\n\n\n```javascript\nvar r0 = 2;\nconst circumference0 = 2 * Math.PI * r0;\n```\n\n\n```javascript\ncircumference0\n```\n\n\n\n\n    12.566370614359172\n\n\n\nå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„æ–¹å¼åšæ•°å­—å’Œå­—ç¬¦ä¸²ä¹‹é—´çš„è½¬æ¢ï¼š\n* parseInt()ï¼šæŠŠå­—ç¬¦ä¸²è§£ææˆæ•´æ•°ã€‚\n* parseFloat()ï¼šå°†å­—ç¬¦ä¸²è§£ææˆä¸€ä¸ªæµ®ç‚¹æ•°ã€‚\n* Number()ï¼šå°†è¡¨ç¤ºNumberæ•°å€¼çš„å­—ç¬¦ä¸²è§£ææˆNumberç±»å‹çš„å€¼ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨`+/-`ç¬¦å·æ¥ä»£æ›¿Number()å‡½æ•°ã€‚\n\n\n```javascript\nparseInt('123');\n```\n\n\n\n\n    123\n\n\n\n\n```javascript\nparseInt('123n');\n```\n\n\n\n\n    123\n\n\n\n\n```javascript\nparseInt('-123');\n```\n\n\n\n\n    -123\n\n\n\n\n```javascript\nparseInt('123.4'); // å¯ä»¥è§£æå°æ•°ï¼Œåªæ˜¯å¾—åˆ°å…¶æ•´æ•°éƒ¨åˆ†\n```\n\n\n\n\n    123\n\n\n\n\n```javascript\nparseFloat('123.4'); \n```\n\n\n\n\n    123.4\n\n\n\n\n```javascript\nparseInt('0b111110111'); // ä¸èƒ½æ­£ç¡®è§£æå…¶ä»–è¿›åˆ¶çš„æ•°\n```\n\n\n\n\n    0\n\n\n\n\n```javascript\nNumber('0b111110111'); \n```\n\n\n\n\n    503\n\n\n\n\n```javascript\nNumber('0o767'); \n```\n\n\n\n\n    503\n\n\n\n\n```javascript\n+'0o767'\n```\n\n\n\n\n    503\n\n\n\n\n```javascript\n-'0b111110111'\n```\n\n\n\n\n    -503\n\n\n\n`NaN`è¡¨ç¤ºNot a Numberï¼Œæ¯”å¦‚è§£æä¸€ä¸ªéæ•°å­—è¡¨è¾¾ï¼›ä¼ å…¥`NaN`åšè¿ç®—ï¼Œä¼šè¿”å›`NaN`ã€‚\n\n`Infinity`è¡¨ç¤ºæ— ç©·å¤§ï¼Œé™¤ä»¥0ä¼šäº§ç”Ÿè¿™ä¸ªå€¼ã€‚å®ƒæœ‰æ­£è´Ÿä¹‹åˆ†ã€‚\n\n\n```javascript\nparseInt('Not a Number');\n```\n\n\n\n\n    NaN\n\n\n\n\n```javascript\nNaN + 1\n```\n\n\n\n\n    NaN\n\n\n\n\n```javascript\n1/0\n```\n\n\n\n\n    Infinity\n\n\n\n\n```javascript\n-1/0\n```\n\n\n\n\n    -Infinity\n\n\n\n### String\n\nJavaScriptä¸­çš„å­—ç¬¦ä¸²å°±æ˜¯Unicodeï¼ˆå‡†ç¡®è¯´æ˜¯UTF-16ç¼–ç ï¼‰å­—ç¬¦çš„åºåˆ—ã€‚\n\n\n\n```javascript\nconsole.log('Hello, world');\nconsole.log('ä½ å¥½ï¼Œä¸–ç•Œï¼');\n```\n\n    Hello, world\n    ä½ å¥½ï¼Œä¸–ç•Œï¼\n\n\n\n```javascript\n// å•å¼•å·å’ŒåŒå¼•å·éƒ½OK\nconsole.log(\"Hello, world\");\nconsole.log(\"ä½ å¥½ï¼Œä¸–ç•Œï¼\");\n```\n\n    Hello, world\n    ä½ å¥½ï¼Œä¸–ç•Œï¼\n\n\n\n```javascript\n// å­—ç¬¦å’Œå­—ç¬¦ä¸²ä¹‹é—´ä¹Ÿæ²¡æœ‰å·®åˆ«ï¼šå­—ç¬¦å°±æ˜¯é•¿åº¦ä¸º1çš„å­—ç¬¦ä¸²\n'Hello'[1] === 'e'\n```\n\n\n\n\n    true\n\n\n\n\n```javascript\n// å­—ç¬¦ä¸²é•¿åº¦ï¼šlengthå±æ€§\n'Hello'.length\n```\n\n\n\n\n    5\n\n\n\n\n```javascript\n// å­—ç¬¦ä¸²ç›¸åŠ \nconst age = 25;\nconsole.log('I am ' + age + ' years old.') // String concatenation\n```\n\n    I am 25 years old.\n\n\n\n```javascript\n// ä¹Ÿå¯ç”¨å­—ç¬¦ä¸²æ¨¡æ¿ï¼šä½¿ç”¨åå¼•å· `` + ${}\nconsole.log(`I am ${age} years old.`)\n```\n\n    I am 25 years old.\n\n\n### å…¶ä»–ç±»å‹\n\nåœ¨JavaScriptä¸­ï¼Œ`null`è¡¨ç¤ºdeliberate non-valueï¼ˆæ•…æ„çš„ç©ºå€¼ï¼‰ï¼Œ`undefined`è¡¨ç¤ºabsence of valueï¼ˆæ²¡æœ‰å®šä¹‰çš„å€¼ï¼‰ã€‚`null`åªèƒ½é€šè¿‡nullå…³é”®å­—è·å–ï¼Œè€Œ`undefined`å¯ä»¥é€šè¿‡ä¸‹é¢çš„å¤šç§æ–¹å¼è·å–ï¼š\n* returnè¯­å¥ä¸å¸¦å€¼ï¼ˆreturn;ï¼‰\n* è®¿é—®objectä¸€ä¸ªå¹¶ä¸å­˜åœ¨çš„å±æ€§ï¼ˆobj.iDontExistï¼‰\n* ç”³æ˜ä¸€ä¸ªå˜é‡ä½†ä¸èµ‹å€¼(let x;)\n\n\n```javascript\nfunction returnNothing() {\n    return\n}\n\nconsole.log(returnNothing())\n```\n\n    undefined\n\n\n\n```javascript\nvar arr = [1, 2, 3]\n\nconsole.log(arr.iDontExist)\n```\n\n    undefined\n\n\n\n```javascript\nlet xxx;\nconsole.log(xxx)\n```\n\n    undefined\n\n\nJavaScriptçš„å¸ƒå°”å€¼`true`å’Œ`false`ï¼Œä»»ä½•å€¼éƒ½å¯ä»¥è½¬æ¢æˆå¸ƒå°”å€¼ï¼Œå…¶è§„åˆ™å¦‚ä¸‹ï¼š\n* `false`, `0`, ç©ºå­—ç¬¦ä¸²`\"\"`, `NaN`, `null`ä»¥åŠ`undefined`è¢«è½¬æ¢æˆ`false`\n* å…¶ä»–å€¼éƒ½è¢«è½¬æ¢æˆ`true`\n\n\n```javascript\nBoolean(\"\")\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean(0)\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean(0.0)\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean(undefined)\n```\n\n\n\n\n    false\n\n\n\n\n```javascript\nBoolean('Hello')\n```\n\n\n\n\n    true\n\n\n\n## Variables: å˜é‡\n\nJavaScriptä¸­ï¼Œå˜é‡å¯ä»¥ç”±ä¸‹é¢çš„ä¸‰ä¸ªå…³é”®å­—ç”³æ˜ï¼š`let`, `const`, `var`ã€‚ä»–ä»¬ä¹‹é—´çš„å·®åˆ«å¯ä»¥å‚è€ƒï¼š\n\n[JavaScript ä¸­çš„ Varã€Let å’Œ Const æœ‰ä»€ä¹ˆåŒºåˆ«](https://www.freecodecamp.org/chinese/news/javascript-var-let-and-const/#:~:text=var%20%E5%A3%B0%E6%98%8E%E6%98%AF%E5%85%A8%E5%B1%80%E4%BD%9C%E7%94%A8,%E5%85%B6%E4%BD%9C%E7%94%A8%E5%9F%9F%E7%9A%84%E9%A1%B6%E7%AB%AF%E3%80%82)ã€‚\n\næ€»ç»“å…¶å¼‚åŒç‚¹ï¼š\n* `var`å£°æ˜æ˜¯å…¨å±€ä½œç”¨åŸŸæˆ–å‡½æ•°ä½œç”¨åŸŸï¼Œè€Œ`let`å’Œ`const`æ˜¯å—ä½œç”¨åŸŸã€‚\n* `var`å˜é‡å¯ä»¥åœ¨å…¶èŒƒå›´å†…æ›´æ–°å’Œé‡æ–°å£°æ˜ï¼› `let`å˜é‡å¯ä»¥è¢«æ›´æ–°ä½†ä¸èƒ½é‡æ–°å£°æ˜ï¼› `const`å˜é‡æ—¢ä¸èƒ½æ›´æ–°ä¹Ÿä¸èƒ½é‡æ–°å£°æ˜ã€‚\n* å®ƒä»¬éƒ½è¢«æå‡åˆ°å…¶ä½œç”¨åŸŸçš„é¡¶ç«¯ã€‚ä½†æ˜¯ï¼Œè™½ç„¶ä½¿ç”¨å˜é‡undefinedåˆå§‹åŒ–äº†`var`å˜é‡ï¼Œä½†æœªåˆå§‹åŒ–`let`å’Œ`const`å˜é‡ã€‚\n* å°½ç®¡å¯ä»¥åœ¨ä¸åˆå§‹åŒ–çš„æƒ…å†µä¸‹å£°æ˜`var`å’Œ`let`ï¼Œä½†æ˜¯åœ¨å£°æ˜æœŸé—´å¿…é¡»åˆå§‹åŒ–`const`ã€‚\n\nç°åœ¨`let`æ˜¯ä¸»æµçš„ç”³æ˜æ–¹å¼ã€‚\n\nä¸‹é¢é€šè¿‡å‡ ä¸ªä¾‹å­æ¥è¯´æ˜ã€‚\n\n\n```javascript\n// varçš„ä½œç”¨åŸŸæ˜¯å…¨å±€æˆ–è€…å‡½æ•°èŒƒå›´\n\nvar greeter = 'hi';\n\nfunction newFunc() {\n    var hello = 'hello';\n}\n\nconsole.log(greeter);\n// console.log(hello); // ReferenceError: hello is not defined\n```\n\n    hi\n\n\n\n```javascript\n// varå˜é‡å¯ä»¥é‡æ–°ç”³æ˜å’Œä¿®æ”¹\nvar greeter = 'hi';\ngreeter = 'hello';\nconsole.log(greeter);\n\nvar greeter = 'hi hi hi';\nconsole.log(greeter);\n```\n\n    hello\n    hi hi hi\n\n\n\n```javascript\n// var çš„å˜é‡æå‡\n// å˜é‡æå‡æ˜¯ JavaScript çš„ä¸€ç§æœºåˆ¶:åœ¨æ‰§è¡Œä»£ç ä¹‹å‰ï¼Œå˜é‡å’Œå‡½æ•°å£°æ˜ä¼šç§»è‡³å…¶ä½œç”¨åŸŸçš„é¡¶éƒ¨ã€‚è¿™æ„å‘³ç€å¦‚æœæˆ‘ä»¬è¿™æ ·åš:\n\nconsole.log(greeter0);\nvar greeter0 = 'hi 0';\n```\n\n    undefined\n\n\n\n```javascript\n// ä¸Šé¢çš„ä»£ç ä¼šè¢«è§£é‡Šä¸ºï¼š\nvar greeter0;\nconsole.log(greeter0);\ngreeter0 = 'hi 0';\n```\n\n    hi 0\n\n\n\n\n\n    'hi 0'\n\n\n\n\n```javascript\n// varçš„é—®é¢˜ï¼šå› ä¸ºå…¨å±€ä½œç”¨åŸŸé€ æˆçš„ä¸å¸Œæœ›å‘ç”Ÿçš„èµ‹å€¼å’Œå¼•ç”¨\n\nvar greeter1 = 'hi';\nvar times = 4;\n\nif(times \u003e 3) {\n    var greeter1 = 'Hello'\n}\n\nconsole.log(greeter1)\n\n```\n\n    Hello\n\n\nä¸Šä¾‹ä¸­ï¼Œif blockä¸­çš„greeter1å½±å“äº†å…¨å±€greeter1çš„å€¼ã€‚è¿™æœ‰å¯èƒ½æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›çœ‹åˆ°çš„ã€‚è¿™å°±æ˜¯ä½¿ç”¨letçš„åŸå› ï¼š\n\n\n```javascript\n// letçš„ä½œç”¨åŸŸæ˜¯block {} çº§åˆ«çš„\n\nlet greeter3 = 'hi';\nlet times1 = 4;\n\nif(times1 \u003e 3) {\n    let greeter3 = 'Hello';\n    console.log(greeter3);\n}\n\nconsole.log(greeter3);\n```\n\n    Hello\n    hi\n\n\n\n```javascript\n// letå˜é‡èƒ½è¢«ä¿®æ”¹ï¼Œä½†ä¸èƒ½è¢«é‡æ–°ç”³æ˜\nlet greeter5 = 'hi';\ngreeter5 = 'hello';\n\nconsole.log(greeter5);\n```\n\n    hello\n\n\n\n```javascript\n// let greeter5 = 'hi'; //SyntaxError: Identifier 'greeter5' has already been declared\n```\n\n`const`ä¸`let`ç±»ä¼¼ï¼Œåªä¸è¿‡å…¶ç”³æ˜çš„å˜é‡å¿…é¡»ä¿æŒå¸¸é‡ï¼Œä¸èƒ½è¢«ä¿®æ”¹ã€‚\n\n\n```javascript\nconst greeter6 = 'hi';\n// greeter6 = 'hello'; //TypeError: Assignment to constant variable.\n```\n\nä¸è¿‡å…¶ç”³æ˜çš„objectç±»å‹çš„å˜é‡ï¼Œå…¶å±æ€§æ˜¯å¯ä»¥è¢«æ”¹å˜çš„ã€‚\n\n\n```javascript\nconst obj_greeter = {\n    message: 'hi',\n    times: 4\n}\n\nobj_greeter.message = 'hello'\n\nconsole.log(obj_greeter)\n```\n\n    { message: 'hello', times: 4 }\n\n\n\n```javascript\nobj_greeter.receiver = 'Jack'\n\nconsole.log(obj_greeter)\n```\n\n    { message: 'hello', times: 4, receiver: 'Jack' }\n\n\nJavaScriptæ˜¯åŠ¨æ€ç±»å‹ï¼Œæ„å‘³ç€åŒä¸€ä¸ªå˜é‡åå¯ä»¥æŒ‡å‘ä¸åŒç±»å‹çš„æ•°æ®ã€‚\n\n\n```javascript\nlet a = 1;\na = 'foo';\n```\n\n\n\n\n    'foo'\n\n\n\n## Operatorsï¼šè¿ç®—ç¬¦\n\nJavaScriptæ”¯æŒçš„è¿ç®—ç¬¦åŒ…æ‹¬ï¼š\n* `+`, `-`, `*`, `/`, `%`, `**`: æ•°å­¦è¿ç®—\n* `=`, `+=`, `-=`: èµ‹å€¼\n* `++`, `--`: è‡ªåŠ è‡ªå‡\n* `+`: å­—ç¬¦ä¸²æ‹¼æ¥\n* `\u003e`, `\u003c`, `\u003e=`, `\u003c=`: ä¸ç­‰æ¯”è¾ƒ\n* `==`: åŒç­‰å·ï¼Œä¸åŒç±»å‹ä¼šå¼ºåˆ¶è½¬æ¢ï¼ˆtype coercionï¼‰, å¯¹åº”çš„ä¸ç­‰å·æ˜¯`!=`\n* `===`: ä¸‰ç­‰å·ï¼Œä¸åŒç±»å‹ä¸ä¼šå¼ºåˆ¶è½¬æ¢, å¯¹åº”çš„ä¸ç­‰å·æ˜¯`!==`\n* `\u0026\u0026`, `||`, `!`: é€»è¾‘è¿ç®—ï¼šä¸æˆ–é\n* `\u0026`, `|`, `~`: ä½è¿ç®—ï¼šä¸æˆ–é\n\næ›´è¯¦å°½çš„è¿ç®—ç¬¦è¯´æ˜ï¼Œè§[é“¾æ¥](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators)ã€‚\n\nä¸‹é¢æ˜¯ä¸€äº›ç¤ºä¾‹ã€‚\n\n\n```javascript\n// æ±‚ä½™æ•°\n123 % (3.5)\n```\n\n\n\n\n    0.5\n\n\n\n\n```javascript\n// æ³¨æ„é¡ºåº\n\"3\" + 4 + 5;\n```\n\n\n\n\n    '345'\n\n\n\n\n```javascript\n3 + 4 + '5';\n```\n\n\n\n\n    '75'\n\n\n\n\n```javascript\n// åŒç­‰å· vs ä¸‰ç­‰å·\n\nconsole.log(123 == '123');\nconsole.log(1 == true);\n\nconsole.log(123 === '123');\nconsole.log(1 === true);\n```\n\n    true\n    true\n    false\n    false\n\n\n\n```javascript\n// é€»è¾‘è¿ç®—\nconst a1 = 0 \u0026\u0026 'Hello'; // 0 because 0 is \"falsy\"\nconsole.log(a1);\n```\n\n    0\n\n\n\n```javascript\nconst b1 = \"Hello\" || \"world\"; // \"Hello\" because both \"Hello\" and \"world\" are \"truthy\"\nconsole.log(b1);\n```\n\n    Hello\n\n\n## Grammar: è¯­æ³•é£æ ¼\n\nJavaScriptçš„è¯­æ³•é£æ ¼æ¥è¿‘äºCè¯­è¨€ã€‚æœ‰ä¸‹é¢å‡ ç‚¹å€¼å¾—æ³¨æ„ï¼š\n* æ³¨é‡Šï¼šå•è¡Œç”¨`//`ï¼Œå¤šè¡Œç”¨`/* */`\n* è¡¨è¾¾å¼çš„ç»“å°¾ç”¨';'ï¼šä½†è¿™ä¸ªåˆ†å·ä¹Ÿæ˜¯å¯é€‰çš„\n\n## Control Structure: æ§åˆ¶ç»“æ„\n\nå’Œå¤§å¤šæ•°è¯­è¨€ä¸€æ ·ï¼ŒJavaScriptåŒ…æ‹¬ä¸‹é¢çš„æ§åˆ¶ç»“æ„ï¼š\n* `if`, `else`: æ¡ä»¶\n* `while`, `do...while`: å¾ªç¯\n* `for`, é™¤äº†å¸¸è§„çš„forå¾ªç¯ï¼Œè¿˜è¡ç”Ÿå‡ºä¸¤ç§ç‰¹æ®Šçš„éå†ï¼š\n    * `for...of`: éå†æ•°ç»„çš„å„ä¸ªå…ƒç´ \n    * `for...in`: éå†objectçš„å„ä¸ªå±æ€§\n* `switch`: åˆ†æ”¯\n* `try...catch`, `throw`: å¼‚å¸¸\n\n## Objects: å¯¹è±¡ç±»å‹\n\nJavaScriptçš„objectç±»å‹ç”¨æ¥æ”¾é”®å€¼å¯¹key-valueæ•°æ®ï¼Œç›¸å½“äºPythonä¸­çš„dict. objectæ˜¯éå¸¸åŠ¨æ€çš„, å®ƒçš„å±æ€§å¯ä»¥éšæ—¶è¢«æ·»åŠ ã€åˆ é™¤ã€é‡æ’ã€çªå˜mutatedã€‚objectçš„keyæ€»æ˜¯stringæˆ–è€…symbolã€‚\n\n\n```javascript\nconst obj = {\n    name: 'Carrot',\n    for: 'Max',\n    details: {\n        color: 'orange',\n        size: 12\n    }\n}\n\nobj\n```\n\n\n\n\n    { name: 'Carrot', for: 'Max', details: { color: 'orange', size: 12 } }\n\n\n\n\n```javascript\n// dot notation: åªèƒ½æ˜¯ä¸€ä¸ªé™æ€çš„æ ‡è¯†ç¬¦\nconsole.log(obj.name);\n\n// bracket notationï¼šå¯ä»¥æ˜¯ä¸€ä¸ªåŠ¨æ€çš„å˜é‡\nconsole.log(obj['name'])\n```\n\n    Carrot\n    Carrot\n\n\n\n```javascript\n// ç”¨å˜é‡æ¥æ ‡è¯†ä¸€ä¸ªkey\nlet userName1 = 'nick';\nobj[userName1] = 'Catty';\nobj\n```\n\n\n\n\n    {\n      name: 'Carrot',\n      for: 'Max',\n      details: { color: 'orange', size: 12 },\n      nick: 'Catty'\n    }\n\n\n\n\n```javascript\n// é“¾å¼è®¿é—®\nconsole.log(obj.details.color);\nconsole.log(obj['details']['size']);\n```\n\n    orange\n    12\n\n\n\n```javascript\n// objectæ˜¯æŒ‰ç…§å¼•ç”¨ä¼ å€¼\nconst obj1 = {}\n\nfunction doSth(o) {\n    o.x = 1;\n}\n\ndoSth(obj1);\n\nobj1\n```\n\n\n\n\n    { x: 1 }\n\n\n\n\n```javascript\n// å¼•ç”¨\nconst obj2 = obj1;\nobj1.y = 2;\n\nobj2\n```\n\n\n\n\n    { x: 1, y: 2 }\n\n\n\n## Arrays: æ•°ç»„\n\nJavaScriptä¸­çš„æ•°ç»„æ˜¯ä¸€ç§ç‰¹æ®Šçš„objectï¼Œé€šè¿‡`[index]`æ¥è®¿é—®å…ƒç´ ã€‚æ•°ç»„é•¿åº¦é€šè¿‡`.length`æ¥è·å–ã€‚\n\n\n```javascript\nconst arr1 = ['dog', 'cat', 'hen'];\na.length\n```\n\n\n\n\n    3\n\n\n\n\n```javascript\n// å¯ä»¥ç»™ä»»æ„éè´Ÿæ•´æ•°çš„ä¸‹æ ‡èµ‹å€¼\narr1[10] = 'fox'\n\narr1\n```\n\n\n\n\n    [ 'dog', 'cat', 'hen', \u003c7 empty items\u003e, 'fox' ]\n\n\n\n\n```javascript\n// æ•°ç»„è®¿é—®è¶Šç•Œæ˜¯ä¸ä¼šæŠ¥é”™çš„ï¼Œåªä¼šæ”¾å›ä¸€ä¸ªundefined\nconsole.log(arr1[100])\n```\n\n    undefined\n\n\n\n```javascript\n// æ•°ç»„å…ƒç´ å¯ä»¥æ˜¯ä»»æ„ç±»å‹çš„\narr1.push\narr1.push(false);\narr1.push(null);\narr1.push(101);\narr1.push({});\narr1.push([]);\n\narr1\n```\n\n\n\n\n    [\n      'dog',\n      'cat',\n      'hen',\n      \u003c7 empty items\u003e,\n      'fox',\n      false,\n      null,\n      101,\n      {},\n      []\n    ]\n\n\n\n\n```javascript\narr1[14].x = 1;\narr1[15].push('ok');\n\narr1\n```\n\n\n\n\n    [\n      'dog',\n      'cat',\n      'hen',\n      \u003c7 empty items\u003e,\n      'fox',\n      false,\n      null,\n      101,\n      { x: 1 },\n      [ 'ok' ]\n    ]\n\n\n\n\n```javascript\n// éå†ï¼š é€šè¿‡for\n\nfor(let i = 0; i \u003c arr1.length; i++) {\n    console.log('#' + i + ' : ' + arr1[i])\n}\n```\n\n    #0 : dog\n    #1 : cat\n    #2 : hen\n    #3 : undefined\n    #4 : undefined\n    #5 : undefined\n    #6 : undefined\n    #7 : undefined\n    #8 : undefined\n    #9 : undefined\n    #10 : fox\n    #11 : false\n    #12 : null\n    #13 : 101\n    #14 : [object Object]\n    #15 : ok\n\n\n\n```javascript\n// éå†ï¼š for...of\nfor(const a of arr1) {\n    console.log(a);\n}\n```\n\n    dog\n    cat\n    hen\n    undefined\n    undefined\n    undefined\n    undefined\n    undefined\n    undefined\n    undefined\n    fox\n    false\n    null\n    101\n    { x: 1 }\n    [ 'ok' ]\n\n\nArrayæœ‰ä¸€ç³»åˆ—çš„æ–¹æ³•ï¼Œæ¯”å¦‚cancat, map, filter, sliceç­‰ï¼Œè¯¦è§[Array](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array)ã€‚\n\n\n```javascript\n// map\nconst babies = ['dog', 'cat', 'hen', 'fox'].map((name) =\u003e 'baby ' + name);\nbabies\n```\n\n\n\n\n    [ 'baby dog', 'baby cat', 'baby hen', 'baby fox' ]\n\n\n\n\n```javascript\n// concat: æ•°ç»„æ‹¼æ¥\nconst arr2 = ['a', 'b', 'c'];\nconst arr3 = ['d', 'e', 'f'];\nconst arr4 = arr2.concat(arr3);\n\narr4\n```\n\n\n\n\n    [ 'a', 'b', 'c', 'd', 'e', 'f' ]\n\n\n\n## Functions: å‡½æ•°\n\nå‡½æ•°åœ¨JavaScriptä¸­éå¸¸é‡è¦ï¼Œæ˜¯å…¶æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªåŸºç¡€çš„å‡½æ•°å£°æ˜ï¼š\n\n\n```javascript\nfunction add(x, y) {\n    const total = x + y;\n    return total;\n}\n```\n\n\n```javascript\n// å‡½æ•°ä¼ å‚ä¸ªæ•°å¯ä»¥å°‘äºå®šä¹‰çš„å‚æ•°ä¸ªæ•°ï¼Œç¼ºå°‘çš„å‚æ•°ä¼šè¢«å®šä¹‰æˆundefined\nadd(); // Equivalent to add(undefined, undefined)\n```\n\n\n\n\n    NaN\n\n\n\n\n```javascript\n// å¦‚æœä¼ å…¥çš„å‚æ•°ä¸ªæ•°å¤šä½™å®šä¹‰çš„ï¼Œå¤šä½™çš„å‚æ•°è¢«ç›´æ¥å¿½ç•¥ã€‚æ€»ä¹‹ï¼Œå‡½æ•°ä¸ä¼šå› ä¸ºå‚æ•°çš„å¤šå°‘è€ŒæŠ¥è¯­æ³•é”™\nadd(1, 2, 3, 4);\n```\n\n\n\n\n    3\n\n\n\nå‡½æ•°çš„å‚æ•°å¯ä»¥æ˜¯rest parameterè¯­æ³•ï¼Œé€šè¿‡ä¸€ä¸ªæ•°ç»„æ¥å­˜å‚¨æœªæ˜ç¡®æŒ‡å®šçš„å‚æ•°å€¼ï¼Œç±»ä¼¼Pythonçš„`*args`ï¼ˆæ³¨ï¼šåœ¨è¯­æ³•å±‚é¢æ²¡æœ‰`**kwargs`ï¼‰ã€‚ \n\n\n```javascript\nfunction avg(...args) {\n    let sum = 0;\n    for (const item of args) {\n        sum += item;\n    }\n    return sum / args.length;\n}\n\navg(1, 2, 4, 5)\n```\n\n\n\n\n    3\n\n\n\n\n```javascript\n// è¿˜å¯ä»¥è¿™ä¹ˆè°ƒç”¨\nconst arr5 = [5, 6, 7, 8]\navg(...arr5)\n```\n\n\n\n\n    6.5\n\n\n\nè™½ç„¶JavaScriptçš„å‡½æ•°ä¸æ”¯æŒ`**kwargs`è¿™æ ·çš„å‘½åå‚æ•°ï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ä¼ å…¥objectç±»å‹å‚æ•°ï¼Œé€šè¿‡`object destructuring`æ¥`pack/unpack`ã€‚\n\n\n```javascript\n// Note the { } braces: this is destructuring an object\nfunction area({ width, height }) {\n  return width * height;\n}\n\n// The { } braces here create a new object\nconsole.log(area({ width: 2, height: 3 }));\n```\n\n    6\n\n\n\n```javascript\nconsole.log(area({ width1: 2, height1: 3 }));\n```\n\n    NaN\n\n\n\n```javascript\n// ä¸å…¶ä»–è¯­è¨€ä¸€æ ·ï¼ŒJavaScriptçš„å‡½æ•°æ”¯æŒå‚æ•°çš„é»˜è®¤å€¼\nfunction avg3(v1, v2, v3=0) {\n    return (v1 + v2 + v3)/3\n}\n\navg3(1, 2)\n```\n\n\n\n\n    1\n\n\n\n### Anonymous functions: åŒ¿åå‡½æ•°\n\nåŒ¿åå‡½æ•°å°±æ˜¯æ²¡æœ‰åå­—çš„å‡½æ•°ã€‚åœ¨å®è·µä¸­ï¼ŒåŒ¿åå‡½æ•°å¸¸å¸¸ä¼šä½œä¸ºå‚æ•°ä¼ é€’ç»™å…¶ä»–å‡½æ•°ï¼Œæˆ–è€…ç«‹åˆ»ä¼ å…¥å‚æ•°è®©å‡½æ•°ç«‹åˆ»è¢«è°ƒç”¨ï¼ˆé€šå¸¸åªè¢«è°ƒç”¨ä¸€æ¬¡ï¼‰ï¼ŒæŠ‘æˆ–è¢«å¦ä¸€ä¸ªå‡½æ•°ä½œä¸ºè¿”å›å€¼è¿”å›ã€‚\n\nä¸‹é¢æ˜¯ä¸€ä¸ªåŒ¿åå‡½æ•°çš„å®šä¹‰æ–¹å¼ï¼šfunctionåé¢ä¸å¸¦å‡½æ•°åã€‚\n\n\n```javascript\nconst avgFunc = function (...args) {\n    let sum = 0;\n    for (const item of args) {\n        sum += item;\n    }\n    return sum / args.length;  \n};\n\navgFunc(1,2,4);\n```\n\n\n\n\n    2.3333333333333335\n\n\n\nå¦ä¸€ç§å®šä¹‰çš„æ–¹å¼æ˜¯é€šè¿‡arrow function expressionï¼Œä¹Ÿå°±æ˜¯`=\u003e`: \n\n\n```javascript\nconst avgFunc2 = (...args) =\u003e {\n    let sum = 0;\n    for (const item of args) {\n        sum += item;\n    }\n    return sum / args.length;  \n};\n\navgFunc2(1,2,4);\n```\n\n\n\n\n    2.3333333333333335\n\n\n\n\n```javascript\n// å¯¹ç®€å•çš„è¡¨è¾¾å¼ï¼Œå¯ä»¥ä¸ç”¨return\nconst sumFunc = (a, b, c) =\u003e a + b + c;\nsumFunc(1, 2, 3);\n```\n\n\n\n\n    6\n\n\n\n\n```javascript\nconst sumFunc1 = (a, b, c) =\u003e {return a + b + c;};\nsumFunc1(1, 2, 3);\n```\n\n\n\n\n    6\n\n\n\n### Functions are first-class objects: å‡½æ•°æ˜¯å¤´ç­‰å…¬æ°‘\n\nå‡½æ•°åƒå…¶ä»–ç±»å‹ä¸€æ ·ï¼Œå¯ä»¥è¢«èµ‹å€¼ç»™å…¶ä»–å‚æ•°ï¼Œæˆ–è€…ä¼ å‚ç»™å…¶ä»–å‡½æ•°ï¼Œä»¥åŠè¢«å…¶ä»–å‡½æ•°ä½œä¸ºè¿”å›å€¼è¿”å›ã€‚\n\n\n```javascript\nconst addxy = (x) =\u003e (y) =\u003e x + y;\n```\n\n\n```javascript\naddxy(1)\n```\n\n\n\n\n    [Function (anonymous)]\n\n\n\n\n```javascript\naddxy(1)(2)\n```\n\n\n\n\n    3\n\n\n\n\n```javascript\n// Function accepting function\nconst babies2 = [\"dog\", \"cat\", \"hen\"].map((name) =\u003e `baby ${name}`);\nbabies2\n```\n\n\n\n\n    [ 'baby dog', 'baby cat', 'baby hen' ]\n\n\n\n### Inner functions: å†…éƒ¨å‡½æ•°\n\nåœ¨å‡½æ•°å†…éƒ¨å®šä¹‰çš„å‡½æ•°ï¼Œå†…éƒ¨å‡½æ•°å¯ä»¥è®¿é—®ä¸Šå±‚å‡½æ•°ä½œç”¨åŸŸå†…çš„å˜é‡ã€‚è¿™ä¸ªç‰¹æ€§å¯ä»¥åœ¨å†…éƒ¨å‡½æ•°é—´å…±äº«ä¸Šå±‚å‡½æ•°çš„å˜é‡ï¼Œä»è€Œé¿å…æ±¡æŸ“å…¨å±€å˜é‡ã€‚\n\n\n```javascript\nfunction parentFunc() {\n    const a = 1;\n    \n    function innerFunc() {\n        const b = 4;\n        return a + b;\n    };\n    \n    return innerFunc();\n};\n\nparentFunc();\n```\n\n\n\n\n    5\n\n\n\n## Classes: ç±»\n\nJavaScriptçš„ç±»å®šä¹‰ç±»ä¼¼Javaï¼š\n\n\n```javascript\nclass Person {\n    constructor(name) {\n        this.name = name;\n    };\n    \n    sayHello() {\n        return `Hello, I am ${this.name}!`;\n    };\n};\n\nconst p = new Person('Maria');\np.sayHello();\n```\n\n\n\n\n    'Hello, I am Maria!'\n\n\n\n## Asynchronous programming: å¼‚æ­¥ç¼–ç¨‹\n\nJavaScript is single-threaded by nature. There's *no paralleling; only concurrency*. Asynchronous programming is powered by an event loop, which allows a set of tasks to be queued and polled for completion.\n\nThere are three idiomatic ways to write asynchronous code in JavaScript:\n* Callback-based (such as setTimeout())\n* Promise-based\n* async/await, which is a syntactic sugar for Promises\n\nFor example, here's how a file-read operation may look like in JavaScript:\n\n\n```javascript\n// Callback-based\n\n/*\nfs.readFile(filename, (err, content) =\u003e {\n  // This callback is invoked when the file is read, which could be after a while\n  if (err) {\n    throw err;\n  }\n  console.log(content);\n});\n// Code here will be executed while the file is waiting to be read\n*/\n\n\n// Promise-based\n/*\nfs.readFile(filename)\n  .then((content) =\u003e {\n    // What to do when the file is read\n    console.log(content);\n  }).catch((err) =\u003e {\n    throw err;\n  });\n// Code here will be executed while the file is waiting to be read\n*/\n\n// Async/await\n\n/*\nasync function readFile(filename) {\n  const content = await fs.readFile(filename);\n  console.log(content);\n}\n*/\n```\n\n## Modules: æ¨¡å—\n\næ¨¡å—é€šå¸¸æ˜¯ä¸€ä¸ªjsæ–‡ä»¶ï¼Œå¯ä»¥è¢«ä¸€ä¸ªæ–‡ä»¶è·¯å¾„æˆ–è€…URLæŒ‡å®šã€‚å¯ä»¥é€šè¿‡importæˆ–è€…exportåœ¨moduleä¹‹é—´äº¤æ¢æ•°æ®ã€‚\n\n\n```javascript\n/*\n\nimport { foo } from \"./foo.js\";\n\n// Unexported variables are local to the module\nconst b = 2;\n\nexport const a = 1;\n\n*/\n```\n\n\n\n\n\n**è¿™å°±æ˜¯JavaScriptçš„åŸºæœ¬è¯­æ³•çŸ¥è¯†ã€‚Keep going!**\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-python-crud":{"title":"","content":"# Hands on Python CRUD","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-vue":{"title":"","content":"# Hands on Vue.js\n\n## Quick Start\n\nFollowing: https://vuejs.org/guide/quick-start.html\n\n\n\n```bash\n\n$ npm init vue@latest\n\nNeed to install the following packages:\n  create-vue@3.6.3\nOk to proceed? (y) y\n\nVue.js - The Progressive JavaScript Framework\n\nâœ” Project name: â€¦ vue-quick-start\nâœ” Add TypeScript? â€¦ No / Yes\nâœ” Add JSX Support? â€¦ No / Yes\nâœ” Add Vue Router for Single Page Application development? â€¦ No / Yes\nâœ” Add Pinia for state management? â€¦ No / Yes\nâœ” Add Vitest for Unit Testing? â€¦ No / Yes\nâœ” Add an End-to-End Testing Solution? â€º No\nâœ” Add ESLint for code quality? â€¦ No / Yes\n\nScaffolding project in /Users/yangls06/work/frontend/vue-quick-start...\n\nDone. Now run:\n\n  cd vue-quick-start\n  npm install\n  npm run dev\n\n```\n\n\n\n\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230512165933542.png\" alt=\"image-20230512165933542\" style=\"zoom:50%;\" /\u003e","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/hands-on-zeppelin":{"title":"Hands on Zeppelin","content":"\n\n\n# Hands on Zeppelin: Step by Step\n\n\u003e In order to build a data playground enabling the engineers to explore data collected by autonomous cars, I am trying to deploy Apache Zeppelin as a component of data platform.\n\n## Introduction\n\n[Apache Zeppelin](https://zeppelin.apache.org/) is: \"Web-based notebook that enables data-driven,\ninteractive data analytics and collaborative documents with SQL, Scala, Python, R and more.\"\n\n\n\n## Install\n\nAccording to [installation document](https://zeppelin.apache.org/docs/0.10.0/quickstart/install.html), I choose to install Zeppelin using the offical docker on a server (http://10.10.32.4):\n\n```sh\n$ mkdir Zeppelin \u0026 cd Zeppelin\n$ docker run -u $(id -u) -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n           -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n           --name zeppelin apache/zeppelin:0.10.0\n\nUnable to find image 'apache/zeppelin:0.10.0' locally\n0.10.0: Pulling from apache/zeppelin\n16ec32c2132b: Pull complete\naafd5bdc2bb7: Pull complete\n0bb58b150809: Pull complete\n68d71ea3a296: Pull complete\n9c7277321f0c: Downloading [=============================================\u003e     ]   2.59GB/2.816GB\n6be3e4488900: Download complete\n622d30c2f649: Download complete\nd10a38bf471f: Download complete\n4006c4346d45: Download complete\n4f4fb700ef54: Download complete\n\n...\n\nERROR [2022-12-13 08:13:15,873] ({main} ZeppelinServer.java[main]:262) - Error while running jettyServer\njava.lang.Exception: A MultiException has 2 exceptions.  They are:\n1. java.io.IOException: Creating directories for /notebook/.git failed\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.zeppelin.notebook.repo.NotebookRepoSync\n\n\tat org.apache.zeppelin.server.ZeppelinServer.main(ZeppelinServer.java:256)\n\n```\n\n\n\nThese two error was raised because of [Volume mapping issue with Zeppelin](https://forums.docker.com/t/volume-mapping-issue-with-zeppelin/121917), you can fix it to add\n\n`-u 0` parameter to set the user to 0 (root) in docker run.\n\n```sh\n$ docker run -u $(id -u) -p 8080:8080 -u 0 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n           -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n           --name zeppelin apache/zeppelin:0.10.0\n```\n\n\n\nor you can change the owner of dir `logs` and `notebook` using:\n\n```sh\n$ sudo chown -R 1000:1000 notebook\n$ sudo chown -R 1000:1000 logs\n$ docker run -u $(id -u) -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook \\\n           -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \\\n           --name zeppelin apache/zeppelin:0.10.0\n```\n\n\n\nThen visit http://10.10.32.4:8080/ to use Zeppelin.\n\n![image-20221213165658796](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221213165658796.png)\n\n\n\n## Create New Note\n\n![yshJJIkNnq](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/yshJJIkNnq.jpg)\n\nCreate a new note through these two entries by giving it a name and select a default interpreter.\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214150836370.png\" alt=\"image-20221214150836370\" style=\"zoom:50%;\" /\u003e\n\n\u003e You can use multiple interpreter in one zeppeline note to support different languages. You can get this via ChatGPT. See details in \"Q\u0026A (with ChatGPT on Zeppelin)\" part.\n\n![image-20221214151356720](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214151356720.png)\n\nI write 3 different code snippet in 3 different languages of Markdown, Python and sql. Run them and we will get:\n\n![image-20221214151758144](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214151758144.png)\n\nYou can see: \n\n* %md and %python work fine.\n* %sql gives an error: Interpreter sql not found.\n\n## Interpreters\n\nWe can see and manage Zeppelin's interpreters via the 'interpreter' menu below: \n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214151959692.png\" alt=\"image-20221214151959692\" style=\"zoom:50%;\" /\u003e\n\n\n\nAll interpreters are here:\n\n![image-20221214154255076](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214154255076.png)\n\n## Add a SQL Interpreter (for ADB)\n\nIn order to access data in mysql, add a SQL interpreter:\n\nReference: \n\n* [SQL support in Zeppelin](https://zeppelin.apache.org/docs/0.8.0/quickstart/sql_with_zeppelin.html)\n* [Generic JDBC Interpreter for Apache Zeppelin - mysql](https://zeppelin.apache.org/docs/0.8.0/interpreter/jdbc.html#mysql)\n\n\u003e If you want to connect other databases such as `Mysql`, `Redshift` and `Hive`, you need to edit the property values. You can also use [Credential](https://zeppelin.apache.org/docs/latest/setup/security/datasource_authorization.html) for JDBC authentication. If `default.user` and `default.password` properties are deleted(using X button) for database connection in the interpreter setting page, the JDBC interpreter will get the account information from [Credential](https://zeppelin.apache.org/docs/latest/setup/security/datasource_authorization.html).\n\n* [Data Source Authorization in Apache Zeppelin](https://zeppelin.apache.org/docs/0.7.0/security/datasource_authorization.html)\n* [Tutorial: Using Apache Zeppelin with MySQL](https://thedataist.com/tutorial-using-apache-zeppelin-with-mysql/)\n\n\n\nFirst, add a credential info, which stores safely the username and password of the mysql connection:\n\n![image-20221214164153294](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214164153294.png)\n\n\n\nSecond, add a new interpreter `adbsql` as \n\n![image-20221214164336089](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214164336089.png)\n\nRemember to add dependencies to locate the jar used to run the mysql driver:\n\n![image-20221214164508044](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214164508044.png)\n\n\n\n## Access Data using SQL\n\nThen I can access the data in mysql using sql like this:\n\n![image-20221214171612554](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214171612554.png)\n\n\n\n## Add MySQL Interpreter\n\n![image-20221215144609848](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215144609848.png)\n\nThe dependency should rather be `mysql:mysql-connector-java:5.1.44` than `5.1.41` used before, because \n\n[java.sql.SQLException: Unknown system variable 'query_cache_size'](https://stackoverflow.com/questions/49984267/java-sql-sqlexception-unknown-system-variable-query-cache-size)\n\n\u003e `query_cache_size` was removed in MySQL 8. Check the [docs](https://dev.mysql.com/doc/refman/5.7/en/query-cache.html).\n\u003e\n\u003e It works with JDBC driver 5.1.44.\n\n![image-20221215170718539](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215170718539.png)\n\nIf `mysql:mysql-connector-java:5.1.41` is used, `query_cache_size` error will be raised.\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215171148557.png\" alt=\"image-20221215171148557\" style=\"zoom:50%;\" /\u003e\n\n\n\nThen you can access the data in mysql db:\n\n![image-20221215171439961](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215171439961.png)\n\n## Store Zeppelin Notes to GitLab Repo\n\n[How To: Store Zeppelin Notes in GitHub repo](https://community.cloudera.com/t5/Community-Articles/How-To-Store-Zeppelin-Notes-in-GitHub-repo/ta-p/247398)\n\nYou can set the local dir for Zeppelin notes to be a git repo and use Zeppelin's version control feature to manage versions of a notes.\n\nAdd a new version and commit\n\n![image-20221215110158669](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215110158669.png)\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221215110238902.png\" alt=\"image-20221215110238902\" style=\"zoom:50%;\" /\u003e\n\nThen `git log` will show the history of version:\n\n```bash\ngit log\ncommit 1f029d8073928a3a9e67e7728cd293472595d94a (HEAD -\u003e master)\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Thu Dec 15 03:03:04 2022 +0000\n\n    add dist stat\n\ncommit 836ca519affbc28ac97d998676b060f826b6073b (origin/master)\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Thu Dec 15 02:43:25 2022 +0000\n\n    test\n\ncommit a24ac32075750272a696cb0641693ff0040290ab\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Thu Dec 15 01:40:06 2022 +0000\n\n    test\n\ncommit f26c7f85d4b36a59bb7ec79152cbfc389c907a68\nAuthor: zeppelin \u003czeppelin@cffe349e8d4c\u003e\nDate:   Wed Dec 14 09:17:09 2022 +0000\n\n    Move note 2HPWKGXXY from test_note_of_zeppelin_2HPWKGXXY.zpln to æ•°æ®è®¿é—®demo_2HPWKGXXY.zpln\n```\n\nUse `git push` to push local version to remote repo.\n\n\n\n## Q\u0026A (with ChatGPT on Zeppelin)\n\n### Can I use python and sql in one zeppelin note?\n\nChatGPT:\n\n\u003e Yes, it is possible to use Python and SQL together in a Zeppelin notebook. In fact, many data analysis tasks can benefit from combining the capabilities of both languages.\n\u003e\n\u003e \n\u003e\n\u003e To use Python and SQL together in a Zeppelin notebook, you will first need to make sure that you have the appropriate interpreters installed and configured. In Zeppelin, an interpreter is a plug-in that enables the notebook to execute code in a specific programming language. For Python and SQL, you will need to install and configure the Python and SQL interpreters, respectively.\n\u003e\n\u003e \n\u003e\n\u003e Once the interpreters are installed and configured, you can use the `%python` and `%sql` magic commands in your Zeppelin notebook to switch between Python and SQL code blocks. For example, you could use Python to load and clean your data, and then use SQL to analyze and visualize the cleaned data.\n\u003e\n\u003e Here is an example of how you might use Python and SQL together in a Zeppelin notebook:\n\n```python\n%python\n# load and clean data using Python\n\nimport pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf = df.dropna()\n\n%sql\n-- analyze and visualize data using SQL\n\nSELECT * FROM df LIMIT 10\n\n```\n\n\u003e I hope this helps! Let me know if you have any other questions.\n\n\n\n### SQL: how to add a sql interpreter to apache zeppelin\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214155408956.png\" alt=\"image-20221214155408956\" style=\"zoom:50%;\" /\u003e\n\n### Java: hello world in apache zeppelin\n\n[Java interpreter for Apache Zeppelin](https://zeppelin.apache.org/docs/0.9.0/interpreter/java.html)\n\nChatGPT:\n\n![image-20221214152806322](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20221214152806322.png)\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/how-to-create-a-chatgpt-clone":{"title":"","content":"# How to Create a ChatGPT Clone\n\n\u003e I want to explore the OpenAI API and ChatGPT magic by creating a Simple ChatGPT Clone. Here is all about it.\n\n\n\n## Reference\n\n[Youtube: Build ChatGPT Clone using Open AI API](https://www.youtube.com/watch?v=977mChpScU4\u0026ab_channel=ChatGPTTutorials) \n\n\n\n## Step by step\n\n### Step 1: Ask ChatGPT for coding\n\n\u003e Meï¼šcode a python flask server with '/prompt' route with post method and taking string input. use openai api and send string as prompt and return the response.\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's a sample code for a Flask server that implements the \"/prompt\" route with a POST method and uses the OpenAI API to generate a response based on a string input:\n\u003e\n\u003e ```python\n\u003e from flask import Flask, request\n\u003e import openai\n\u003e \n\u003e app = Flask(__name__)\n\u003e \n\u003e @app.route(\"/prompt\", methods=[\"POST\"])\n\u003e def prompt():\n\u003e     prompt_text = request.form.get(\"prompt\")\n\u003e     openai.api_key = \"YOUR_OPENAI_API_KEY\"\n\u003e     response = openai.Completion.create(\n\u003e         engine=\"text-davinci-002\",\n\u003e         prompt=prompt_text,\n\u003e         max_tokens=1024,\n\u003e         n=1,\n\u003e         stop=None,\n\u003e         temperature=0.5,\n\u003e     )\n\u003e     message = response[\"choices\"][0][\"text\"]\n\u003e     return message\n\u003e \n\u003e if __name__ == \"__main__\":\n\u003e     app.run()\n\u003e \n\u003e ```\n\n\u003e Me: generate the requirement.txt and setup.py for the above with 'Simple ChatGPT Clone' as the app name and yangls06 as my username.\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's the `requirements.txt` file for the above code:\n\u003e\n\u003e ```\n\u003e Flask==1.1.2\n\u003e openai==1.7.0\n\u003e \n\u003e ```\n\u003e\n\u003e And here's the `setup.py` file:\n\u003e\n\u003e ```python\n\u003e from setuptools import setup\n\u003e \n\u003e setup(\n\u003e     name='simple-chatgpt-clone',\n\u003e     version='0.1',\n\u003e     packages=[],\n\u003e     url='https://github.com/yangls06/simple-chatgpt-clone',\n\u003e     license='MIT',\n\u003e     author='yangls06',\n\u003e     author_email='your.email@example.com',\n\u003e     description='A Simple ChatGPT Clone built with Flask and OpenAI API',\n\u003e     install_requires=[\n\u003e         'Flask==1.1.2',\n\u003e         'openai==1.7.0'\n\u003e     ]\n\u003e )\n\u003e \n\u003e ```\n\u003e\n\u003e \n\u003e\n\u003e Note that you'll need to replace the email address with your own email address.\n\n\n\n\u003e Me: how to create a venv for this app and install it and run it in debug model using Flask cmd?\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's how you can create a virtual environment (venv) for this app and install it using Flask CLI in debug mode:\n\u003e\n\u003e ```\n\u003e bashCopy code\n\u003e # Create a virtual environment\n\u003e python3 -m venv venv\n\u003e \n\u003e # Activate the virtual environment\n\u003e source venv/bin/activate\n\u003e \n\u003e # Install the required packages\n\u003e pip install -r requirements.txt\n\u003e \n\u003e # Set the FLASK_APP and FLASK_ENV environment variables\n\u003e export FLASK_APP=app.py\n\u003e export FLASK_ENV=development\n\u003e \n\u003e # Run the Flask app in debug mode\n\u003e flask run\n\u003e ```\n\u003e\n\u003e Replace `app.py` with the name of the file that contains your Flask application, if it's different.\n\u003e\n\u003e With this configuration, you can now access the app in your web browser at `http://localhost:5000/`. Any changes you make to the code will be automatically reflected in the browser, without the need to restart the app. However, it's important to note that you should not use the Flask debugger in production, as it can expose sensitive information and allow arbitrary code execution.\n\n\n\n\u003e Me: create '/' default route with GET method and return index.html\n\u003e\n\u003e \n\n\n\n\u003e Me: code for 'index.html' with bootstrap, displaying input field and submit button. on click submit button check if input is not empty then send POST request to '/prompt' with text from input field as string and display the response.\n\u003e\n\u003e \n\u003e\n\u003e ChatGPT: Here's an example of how you can code 'index.html' with Bootstrap, an input field, and a submit button. The HTML file will look for an onclick event for the submit button, and if the input field is not empty, it will send a POST request to the '/prompt' route. Finally, it will display the response from the server.\n\u003e\n\u003e ```html\n\u003e \u003c!DOCTYPE html\u003e\n\u003e \u003chtml\u003e\n\u003e   \u003chead\u003e\n\u003e     \u003cmeta charset=\"utf-8\"\u003e\n\u003e     \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n\u003e     \u003clink rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\"\u003e\n\u003e     \u003cscript src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"\u003e\u003c/script\u003e\n\u003e   \u003c/head\u003e\n\u003e   \u003cbody\u003e\n\u003e     \u003cdiv class=\"container\"\u003e\n\u003e       \u003ch2\u003eSimple ChatGPT Clone\u003c/h2\u003e\n\u003e       \u003cform\u003e\n\u003e         \u003cdiv class=\"form-group\"\u003e\n\u003e           \u003clabel for=\"prompt\"\u003eEnter a prompt:\u003c/label\u003e\n\u003e           \u003cinput type=\"text\" class=\"form-control\" id=\"prompt\"\u003e\n\u003e         \u003c/div\u003e\n\u003e         \u003cbutton type=\"button\" class=\"btn btn-default\" id=\"submit\"\u003eSubmit\u003c/button\u003e\n\u003e       \u003c/form\u003e\n\u003e       \u003cp id=\"response\"\u003e\u003c/p\u003e\n\u003e     \u003c/div\u003e\n\u003e     \u003cscript\u003e\n\u003e       $(document).ready(function() {\n\u003e         $(\"#submit\").click(function() {\n\u003e           var prompt = $(\"#prompt\").val();\n\u003e           if (prompt.length === 0) {\n\u003e             alert(\"Please enter a prompt\");\n\u003e             return;\n\u003e           }\n\u003e           $.post(\"/prompt\", {prompt: prompt}, function(data) {\n\u003e             $(\"#response\").text(data.response);\n\u003e           });\n\u003e         });\n\u003e       });\n\u003e     \u003c/script\u003e\n\u003e   \u003c/body\u003e\n\u003e \u003c/html\u003e\n\u003e ```\n\n\n\n### gitignore\n\n\u003e Me: create a gitignore file for this project\n\u003e\n\u003e ChatGPT:\n\u003e\n\u003e Here's an example of a `.gitignore` file for a Flask project:\n\u003e\n\u003e ```bash\n\u003e # .gitignore\n\u003e \n\u003e # Byte-compiled / optimized / DLL files\n\u003e __pycache__/\n\u003e *.pyc\n\u003e *.pyo\n\u003e *.egg-info/\n\u003e dist/\n\u003e build/\n\u003e \n\u003e # Flask environment\n\u003e instance/\n\u003e .env\n\u003e \n\u003e # venv\n\u003e venv/\n\u003e ```\n\u003e\n\u003e This `.gitignore` file will ignore any byte-compiled Python files (e.g., `*.pyc`), directories generated by Python eggs (e.g., `*.egg-info/`), and directories generated during the build process (e.g., `dist/`, `build/`). It also ignores the Flask `instance` directory, which is typically used to store sensitive information like database credentials. Finally, it ignores the virtual environment directory (e.g., `venv/`).\n\n### OpenAPI key\n\nhttps://platform.openai.com/docs/quickstart/build-your-application\n\nhttps://platform.openai.com/account/api-keys\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/python-data-structure-basics":{"title":"Data Structure Basics in Python","content":"\n# Data Structure Basics in Python\n\n\u003e This article records my practices on python data structures.\n\n\n\n## 0. References\n\nSee: \n\n1. https://realpython.com/python-data-structures/\n2. https://www.geeksforgeeks.org/python-collections-module/#ordereddict\n3. https://docs.python.org/3/library/collections.html#collections.OrderedDict\n\n\n\nThe following code snippets are initially run in jupyter notebook.\n\n```python\n%autosave 10\n```\n\n\n\n    Autosaving every 10 seconds\n\n\n## Part 1: Dictionaries, Maps, HashTables\n\n### dict: Your Go-To Dictionary\n\nBecause dictionaries are so important, Python features a robust dictionary implementation thatâ€™s built directly into the core language: the dict data type.\n\nPython also provides some useful syntactic sugar for working with dictionaries in your programs. For example, the curly-brace ({ }) dictionary expression syntax and dictionary comprehensions allow you to conveniently define new dictionary objects:\n\n\n\n\n```python\nphonebook = {'bob':7387, 'alice':3719, 'jack':7052}\nsqures = {x: x*x for x in range(6)}\n\nprint(phonebook['alice'])\nprint(squres)\n```\n\n    3719\n    {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n\nNotes:\n\n1. Python dict's keys: hashable objects which have hash values that never change during its life time.\n2. Dictionaries are highly optimized and underlie many parts of the language. For example, class attributes and variables in a stack frame are both stored internally in dictionaries.\n3. Performance: O(1) time complexity for lookup, insert, update, and delete operations in the average case\n4. Hash table implementation\n\nBesides plain *dict* objects, Pythonâ€™s standard library also includes a number of specialized dictionary implementations. These specialized dictionaries are all *based on the built-in dictionary class* (and share its performance characteristics) but also include some additional convenience features.\n\n### collections.OrderedDict: Remember the Insertion Order of Keys\n\n\n\n```python\nimport collections\n\nd = collections.OrderedDict(one=1, two=2, three=3)\nd\n```\n\n\n\n\n    OrderedDict([('one', 1), ('two', 2), ('three', 3)])\n\n\n\n\n```python\nd['four'] = 4\nd.keys()\n```\n\n\n\n\n    odict_keys(['one', 'two', 'three', 'four'])\n\n\n\n\n```python\n# remove and insert\nprint('Before Deleting')\nfor key, val in d.items():\n    print(key, val)\n\n# delete an element\nd.pop('one')\n\n# re-insert the same\nd['one'] = 1\n\n# remove and insert\nprint('\\nAfter Re-inserting')\nfor key, val in d.items():\n    print(key, val)\n```\n\n    Before Deleting\n    one 1\n    two 2\n    three 3\n    four 4\n    \n    After Re-inserting\n    two 2\n    three 3\n    four 4\n    one 1\n\n\n\n```python\n## popitem(): popitem(last=True)\n\n# pop last\np = d.popitem()\np\n```\n\n\n\n\n    ('one', 1)\n\n\n\n\n```python\n# pop first\np = d.popitem(last=False)\np\n```\n\n\n\n\n    ('two', 2)\n\n\n\n\n```python\n## move_to_end(key, last=True) \n# Move an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist:\n\nd['one'] = 1\nd['two'] = 2\nd\n\n```\n\n\n\n\n    OrderedDict([('three', 3), ('four', 4), ('one', 1), ('two', 2)])\n\n\n\n\n```python\nd.move_to_end('four')\nd\n```\n\n\n\n\n    OrderedDict([('three', 3), ('one', 1), ('two', 2), ('four', 4)])\n\n\n\n\n```python\nd.move_to_end('one', last=False)\nd\n```\n\n\n\n\n    OrderedDict([('one', 1), ('three', 3), ('two', 2), ('four', 4)])\n\n\n\n\n```python\n## reversed()\n# Python reversed() method returns an iterator that accesses the given sequence in the reverse order.\n# https://www.programiz.com/python-programming/methods/built-in/reversed\n\nfor key, val in d.items():\n    print(key, val)\n\nprint('\\nreversed')\nfor key, val in reversed(d.items()):\n    print(key, val)\n```\n\n    one 1\n    three 3\n    two 2\n    four 4\n    \n    reversed\n    four 4\n    two 2\n    three 3\n    one 1\n\n\n### collections.defaultdict: Return Default Values for Missing Keys\n\nThe defaultdict class is another dictionary subclass that accepts a callable in its constructor whose return value will be used if a requested key cannot be found.\n\nThis can save you some typing and make your intentions clearer as compared to using get() or catching a KeyError exception in regular dictionaries:\n\n\n\n```python\nfrom collections import defaultdict\n\n## use list() as default value function\ndd = defaultdict(list)\n\n# Accessing a missing key creates it and initializes it using the default factory, i.e. list() in this example.\ndd['dogs'].append('Rufus')\ndd['dogs'].append('Kathrin')\ndd\n```\n\n\n\n\n    defaultdict(list, {'dogs': ['Rufus', 'Kathrin']})\n\n\n\n\n```python\ndd['dogs']\n```\n\n\n\n\n    ['Rufus', 'Kathrin']\n\n\n\n### collections.ChainMap: Search Multiple Dictionaries as a Single Mapping\n\nThe collections.ChainMap data structure groups multiple dictionaries into a single mapping. Lookups search the underlying mappings one by one until a key is found. Insertions, updates, and deletions only affect the first mapping added to the chain:\n\n\n```python\nfrom collections import ChainMap\n\ndict1 = {'one':1, 'two':2}\ndict2 = {'three':3, 'four':4}\n\nchain = ChainMap(dict1, dict2)\nchain\n```\n\n\n\n\n    ChainMap({'one': 1, 'two': 2}, {'three': 3, 'four': 4})\n\n\n\n\n```python\nprint('\\nchain map')\nfor key, val in chain.items():\n    print(key, val)\n```\n\n\n    chain map\n    three 3\n    four 4\n    one 1\n    two 2\n\n\n\n```python\n# add new\ndict3 = {'five':5, 'six':6}\nchain.new_child(dict3)\nchain\n```\n\n\n\n\n    ChainMap({'one': 1, 'two': 2}, {'three': 3, 'four': 4})\n\n\n\n### types.MappingProxyType: A Wrapper for Making Read-Only Dictionaries\n\nMappingProxyType is a wrapper around a standard dictionary that provides a read-only view into the wrapped dictionaryâ€™s data. This class was added in Python 3.3 and can be used to create immutable proxy versions of dictionaries.\n\nMappingProxyType can be helpful if, for example, youâ€™d like to return a dictionary carrying internal state from a class or module while discouraging write access to this object. Using MappingProxyType allows you to put these restrictions in place without first having to create a full copy of the dictionary:\n\n\n```python\nfrom types import MappingProxyType\n\nwritable = {'one':1, 'two':2}\nread_only = MappingProxyType(writable)\n\n# The proxy is read-only:\nread_only['one']\n```\n\n\n\n\n    1\n\n\n\n\n```python\nread_only['one'] = 1\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-28-11d9e8a8c63d\u003e in \u003cmodule\u003e\n    ----\u003e 1 read_only['one'] = 1\n\n\n    TypeError: 'mappingproxy' object does not support item assignment\n\n\n\n```python\n# Updates to the original are reflected in the proxy:\nwritable['one'] = 42\nread_only\n```\n\n\n\n\n    mappingproxy({'one': 42, 'two': 2})\n\n\n\n## Part 2: Array Data Structures\n\nAn array is a fundamental data structure available in most programming languages, and it has a wide range of uses across different algorithms.\n\nIn this section, youâ€™ll take a look at array implementations in Python that use only core language features or functionality thatâ€™s included in the Python standard library. Youâ€™ll see the strengths and weaknesses of each approach so you can decide which implementation is right for your use case.\n\nBut before we jump in, letâ€™s cover some of the basics first. How do arrays work, and what are they used for? Arrays consist of fixed-size data records that allow each element to be efficiently located based on its index:\n\nVisual representation of an array\nBecause arrays store information in adjoining blocks of memory, theyâ€™re considered contiguous data structures (as opposed to linked data structures like linked lists, for example).\n\nA real-world analogy for an array data structure is a parking lot. You can look at the parking lot as a whole and treat it as a single object, but inside the lot there are parking spots indexed by a unique number. Parking spots are containers for vehiclesâ€”each parking spot can either be empty or have a car, a motorbike, or some other vehicle parked on it.\n\nBut not all parking lots are the same. Some parking lots may be restricted to only one type of vehicle. For example, a motor home parking lot wouldnâ€™t allow bikes to be parked on it. A restricted parking lot corresponds to a typed array data structure that allows only elements that have the same data type stored in them.\n\nPerformance-wise, itâ€™s very fast to look up an element contained in an array given the elementâ€™s index. A proper array implementation guarantees a constant O(1) access time for this case.\n\nPython includes several array-like data structures in its standard library that each have slightly different characteristics. Letâ€™s take a look.\n\n![Array](https://files.realpython.com/media/python-linked-list-array-visualization.5b9f4c4040cb.jpeg)\n\n### list: Mutable Dynamic Arrays\nLists are a part of the core Python language. Despite their name, Pythonâ€™s lists are implemented as dynamic arrays behind the scenes.\n\nThis means a list allows elements to be added or removed, and the list will automatically adjust the backing store that holds these elements by allocating or releasing memory.\n\nPython lists can hold arbitrary elementsâ€”everything is an object in Python, including functions. Therefore, you can mix and match different kinds of data types and store them all in a single list.\n\nThis can be a powerful feature, but the downside is that supporting multiple data types at the same time means that data is generally less tightly packed. As a result, the whole structure takes up more space:\n\n\n```python\narr = ['one', 'two', 'three']\narr[0]\n```\n\n\n\n\n    'one'\n\n\n\n\n```python\n# lists have a nice repr\narr\n```\n\n\n\n\n    ['one', 'two', 'three']\n\n\n\n\n```python\n# lists are mutable\narr[1] = 'hello'\narr \n```\n\n\n\n\n    ['one', 'hello', 'three']\n\n\n\n\n```python\ndel arr[1]\narr\n```\n\n\n\n\n    ['one', 'three']\n\n\n\n\n```python\n# lists can hold arbitrary data types\narr.append(2333)\narr\n```\n\n\n\n\n    ['one', 'three', 2333]\n\n\n\n### tuple: Immutable Containers\n\nJust like lists, tuples are part of the Python core language. Unlike lists, however, Pythonâ€™s tuple objects are immutable. This means elements canâ€™t be added or removed dynamicallyâ€”all elements in a tuple must be defined at creation time.\n\nTuples are another data structure that can hold elements of arbitrary data types. Having this flexibility is powerful, but again, it also means that data is less tightly packed than it would be in a typed array:\n\n\n```python\narr = ('one', 'two', 'three')\narr[0]\n```\n\n\n\n\n    'one'\n\n\n\n\n```python\narr\n```\n\n\n\n\n    ('one', 'two', 'three')\n\n\n\n\n```python\n# Tuples are immutable\narr[1] = \"hello\"\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-5-2f27284d12dc\u003e in \u003cmodule\u003e\n          1 # Tuples are immutable\n    ----\u003e 2 arr[1] = \"hello\"\n\n\n    TypeError: 'tuple' object does not support item assignment\n\n\n\n```python\ndel arr[1]\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-6-df001b61e2e0\u003e in \u003cmodule\u003e\n    ----\u003e 1 del arr[1]\n\n\n    TypeError: 'tuple' object doesn't support item deletion\n\n\n\n```python\n# Tuples can hold arbitrary data types:\n# (Adding elements creates a copy of the tuple)\narr + (23,)\n```\n\n\n\n\n    ('one', 'two', 'three', 23)\n\n\n\n### array.array: Basic Typed Arrays\n\nPythonâ€™s array module provides space-efficient storage of basic C-style data types like bytes, 32-bit integers, floating-point numbers, and so on.\n\nArrays created with the array.array class are mutable and behave similarly to lists except for one important difference: theyâ€™re typed arrays constrained to a single data type.\n\nBecause of this constraint, array.array objects with many elements are more space efficient than lists and tuples. The elements stored in them are tightly packed, and this can be useful if you need to store many elements of the same type.\n\nAlso, arrays support many of the same methods as regular lists, and you might be able to use them as a drop-in replacement without requiring other changes to your application code.\n\n\n```python\nimport array\n\narr = array.array('f', (1.0, 1.5, 2.0, 2.5))\narr[1]\n\n```\n\n\n\n\n    1.5\n\n\n\n\n```python\narr\n```\n\n\n\n\n    array('f', [1.0, 1.5, 2.0, 2.5])\n\n\n\n\n```python\n# Arrays are mutable\narr[1] = 23.0\narr\n```\n\n\n\n\n    array('f', [1.0, 23.0, 2.0, 2.5])\n\n\n\n\n```python\ndel arr[1]\n```\n\n\n```python\narr\n```\n\n\n\n\n    array('f', [1.0, 2.0, 2.5])\n\n\n\n\n```python\narr.append(42.0)\narr\n```\n\n\n\n\n    array('f', [1.0, 2.0, 2.5, 42.0])\n\n\n\n\n```python\n# Arrays are \"typed\":\narr[1] = \"hello\"\n\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-14-d4d93891e8c0\u003e in \u003cmodule\u003e\n          1 # Arrays are \"typed\":\n    ----\u003e 2 arr[1] = \"hello\"\n\n\n    TypeError: must be real number, not str\n\n\n### str: Immutable Arrays of Unicode Characters\n\nPython 3.x uses str objects to store textual data as immutable sequences of Unicode characters. Practically speaking, that means a str is an immutable array of characters. Oddly enough, itâ€™s also a recursive data structureâ€”each character in a string is itself a str object of length 1.\n\nString objects are space efficient because theyâ€™re tightly packed and they specialize in a single data type. If youâ€™re storing Unicode text, then you should use a string.\n\nBecause strings are immutable in Python, modifying a string requires creating a modified copy. The closest equivalent to a mutable string is storing individual characters inside a list:\n\n\n```python\narr = \"abcd\"\narr[1]\n```\n\n\n\n\n    'b'\n\n\n\n\n```python\ntype(arr[1])\n```\n\n\n\n\n    str\n\n\n\n\n```python\n# Strings are immutable:\narr[1] = 'e'\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-3-2e0d5fa2424f\u003e in \u003cmodule\u003e\n          1 # Strings are immutable:\n    ----\u003e 2 arr[1] = 'e'\n\n\n    TypeError: 'str' object does not support item assignment\n\n\n\n```python\ndel arr[1]\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-4-df001b61e2e0\u003e in \u003cmodule\u003e\n    ----\u003e 1 del arr[1]\n\n\n    TypeError: 'str' object doesn't support item deletion\n\n\n\n```python\n# Strings can be unpacked into a list to get a mutable representation:\narr = list('abcd')\n','.join(arr)\n```\n\n\n\n\n    'a,b,c,d'\n\n\n\n\n```python\n# Strings are recursive data structures:\narr = \"abcd\"\ntype('arr')\n```\n\n\n\n\n    str\n\n\n\n\n```python\ntype(arr[0])\n```\n\n\n\n\n    str\n\n\n\n### bytes: Immutable Arrays of Single Bytes\n\nbytes objects are immutable sequences of single bytes, or integers in the range 0 â‰¤ x â‰¤ 255. Conceptually, bytes objects are similar to str objects, and you can also think of them as immutable arrays of bytes.\n\nLike strings, bytes have their own literal syntax for creating objects and are space efficient. bytes objects are immutable, but unlike strings, thereâ€™s a dedicated mutable byte array data type called bytearray that they can be unpacked into:\n\n\n```python\narr = bytes((0, 1, 2, 3))\narr[1]\n```\n\n\n\n\n    1\n\n\n\n\n```python\n# Bytes literals have their own syntax:\narr\n```\n\n\n\n\n    b'\\x00\\x01\\x02\\x03'\n\n\n\n\n```python\narr = b\"\\x00\\x01\\x02\\x03\"\narr\n```\n\n\n\n\n    b'\\x00\\x01\\x02\\x03'\n\n\n\n\n```python\n# Only valid `bytes` are allowed:\nbytes((0, 300))\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    ValueError                                Traceback (most recent call last)\n    \n    \u003cipython-input-13-f644e09a09b0\u003e in \u003cmodule\u003e\n          1 # Only valid `bytes` are allowed:\n    ----\u003e 2 bytes((0, 300))\n\n\n    ValueError: bytes must be in range(0, 256)\n\n\n\n```python\n# Bytes are immutable:\narr[1] = 23\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-15-714056f42f40\u003e in \u003cmodule\u003e\n          1 # Bytes are immutable:\n    ----\u003e 2 arr[1] = 23\n\n\n    TypeError: 'bytes' object does not support item assignment\n\n\n\n```python\ndel arr[1]\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-16-df001b61e2e0\u003e in \u003cmodule\u003e\n    ----\u003e 1 del arr[1]\n\n\n    TypeError: 'bytes' object doesn't support item deletion\n\n\n### bytearray: Mutable Arrays of Single Bytes\n\nThe bytearray type is a mutable sequence of integers in the range 0 â‰¤ x â‰¤ 255. The bytearray object is closely related to the bytes object, with the main difference being that a bytearray can be modified freelyâ€”you can overwrite elements, remove existing elements, or add new ones. The bytearray object will grow and shrink accordingly.\n\nA bytearray can be converted back into immutable bytes objects, but this involves copying the stored data in fullâ€”a slow operation taking O(n) time:\n\n\n```python\n%autosave 5\n```\n\n\n\n    Autosaving every 5 seconds\n\n\n\n```python\narr = bytearray((0,1,2,3,4))\narr[0]\n```\n\n\n\n\n    0\n\n\n\n\n```python\narr\n```\n\n\n\n\n    bytearray(b'\\x00\\x01\\x02\\x03\\x04')\n\n\n\n\n```python\n# Bytearrays are mutable:\narr[1] = 23\narr\n```\n\n\n\n\n    bytearray(b'\\x00\\x17\\x02\\x03\\x04')\n\n\n\n\n```python\ndel arr[1]\n```\n\n\n```python\narr.append(42)\narr\n```\n\n\n\n\n    bytearray(b'\\x00\\x02\\x03\\x04*')\n\n\n\n\n```python\n# Bytearrays can only hold `bytes` (integers in the range 0 \u003c= x \u003c= 255)\narr[1] = 'Hello'\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    TypeError                                 Traceback (most recent call last)\n    \n    \u003cipython-input-7-42a4439943a8\u003e in \u003cmodule\u003e\n          1 # Bytearrays can only hold `bytes` (integers in the range 0 \u003c= x \u003c= 255)\n    ----\u003e 2 arr[1] = 'Hello'\n\n\n    TypeError: 'str' object cannot be interpreted as an integer\n\n\n\n```python\narr[1] = 300\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    ValueError                                Traceback (most recent call last)\n    \n    \u003cipython-input-8-6088e2f482e7\u003e in \u003cmodule\u003e\n    ----\u003e 1 arr[1] = 300\n\n\n    ValueError: byte must be in range(0, 256)\n\n\n\n```python\n# Bytearrays can be converted back into bytes objects:\n# (This will copy the data)\nbytes(arr)\n```\n\n\n\n\n    b'\\x00\\x02\\x03\\x04*'\n\n\n\n## Arrays in Python: Summary\nIf you want to restrict yourself to the array data structures included with Python, then here are a few guidelines:\n\n* If you need to store arbitrary objects, potentially with mixed data types, then use a list or a tuple, depending on whether or not you want an immutable data structure.\n\n* If you have numeric (integer or floating-point) data and tight packing and performance is important, then try out array.array.\n\n* If you have textual data represented as Unicode characters, then use Pythonâ€™s built-in str. If you need a mutable string-like data structure, then use a list of characters.\n\n* If you want to store a contiguous block of bytes, then use the immutable bytes type or a bytearray if you need a mutable data structure.\n\nIn most cases, I like to start out with a simple list. Iâ€™ll only specialize later on if performance or storage space becomes an issue. Most of the time, using a general-purpose array data structure like list gives you the fastest development speed and the most programming convenience.\n\n## Part 3. Records, Structs, and Data Transfer Objects\n\nCompared to arrays, record data structures provide a fixed number of fields. Each field can have a name and may also have a different type.\n\nIn this section, youâ€™ll see how to implement records, structs, and plain old data objects in Python using only built-in data types and classes from the standard library.\n\n\u003e Note: Iâ€™m using the definition of a record loosely here. For example, Iâ€™m also going to discuss types like Pythonâ€™s built-in tuple that may or may not be considered records in a strict sense because they donâ€™t provide named fields.\n\nPython offers several data types that you can use to implement records, structs, and data transfer objects. In this section, youâ€™ll get a quick look at each implementation and its unique characteristics. At the end, youâ€™ll find a summary and a decision-making guide that will help you make your own picks.\n\n\n### dict: Simple Data Objects\n\nAs mentioned previously, Python dictionaries store an arbitrary number of objects, each identified by a unique key. Dictionaries are also often called maps or associative arrays and allow for efficient lookup, insertion, and deletion of any object associated with a given key.\n\nUsing dictionaries as a record data type or data object in Python is possible. Dictionaries are easy to create in Python as they have their own syntactic sugar built into the language in the form of dictionary literals. The dictionary syntax is concise and quite convenient to type.\n\nData objects created using dictionaries are mutable, and thereâ€™s little protection against misspelled field names as fields can be added and removed freely at any time. Both of these properties can introduce surprising bugs, and thereâ€™s always a trade-off to be made between convenience and error resilience:\n\n\n```python\ncar1 = {'color':'red', 'mileage':3824, 'automatic':True}\ncar2 = {'color':'blue', 'mileage':40231, 'automatic':False}\ncar2\n```\n\n\n\n\n    {'color': 'blue', 'mileage': 40231, 'automatic': False}\n\n\n\n### tuple: Immutable Groups of Objects\n\nPythonâ€™s tuples are a straightforward data structure for grouping arbitrary objects. Tuples are immutableâ€”they canâ€™t be modified once theyâ€™ve been created.\n\nPerformance-wise, tuples take up slightly less memory than lists in CPython, and theyâ€™re also faster to construct.\n\nAs you can see in the bytecode disassembly below, constructing a tuple constant takes a single LOAD_CONST opcode, while constructing a list object with the same contents requires several more operations:\n\n\n```python\n# dis â€” Disassembler for Python bytecode\n# The dis module supports the analysis of CPython bytecode by disassembling it. The CPython bytecode which this module takes as an input is defined in the file Include/opcode.h and used by the compiler and the interpreter.\n\nimport dis\n\ndis.dis(compile(\"(23, 'a', 'b', 'c')\", \"\", \"eval\"))\n```\n\n      1           0 LOAD_CONST               0 ((23, 'a', 'b', 'c'))\n                  2 RETURN_VALUE\n\n\n\n```python\ndis.dis(compile(\"[23, 'a', 'b', 'c']\", \"\", \"eval\"))\n```\n\n      1           0 LOAD_CONST               0 (23)\n                  2 LOAD_CONST               1 ('a')\n                  4 LOAD_CONST               2 ('b')\n                  6 LOAD_CONST               3 ('c')\n                  8 BUILD_LIST               4\n                 10 RETURN_VALUE\n\n\nHowever, you shouldnâ€™t place too much emphasis on these differences. In practice, the performance difference will often be negligible, and trying to squeeze extra performance out of a program by switching from lists to tuples will likely be the wrong approach.\n\nA potential downside of plain tuples is that the data you store in them can only be pulled out by accessing it through integer indexes. You canâ€™t give names to individual properties stored in a tuple. This can impact code readability.\n\nAlso, a tuple is always an ad-hoc structure: itâ€™s difficult to ensure that two tuples have the same number of fields and the same properties stored in them.\n\nThis makes it easy to introduce slip-of-the-mind bugs, such as mixing up the field order. Therefore, I would recommend that you keep the number of fields stored in a tuple as low as possible:\n\n\n```python\ncar1 = (\"red\", 3812.4, True)\ncar2 = (\"blue\", 40231.0, False)\ncar1\n```\n\n\n\n\n    ('red', 3812.4, True)\n\n\n\n### Write a Custom Class: More Work, More Control\n\nClasses allow you to define reusable blueprints for data objects to ensure each object provides the same set of fields.\n\nUsing regular Python classes as record data types is feasible, but it also takes manual work to get the convenience features of other implementations. For example, adding new fields to the __init__ constructor is verbose and takes time.\n\nAlso, the default string representation for objects instantiated from custom classes isnâ€™t very helpful. To fix that, you may have to add your own __repr__ method, which again is usually quite verbose and must be updated each time you add a new field.\n\nFields stored on classes are mutable, and new fields can be added freely, which you may or may not like. Itâ€™s possible to provide more access control and to create read-only fields using the @property decorator, but once again, this requires writing more glue code.\n\nWriting a custom class is a great option whenever youâ€™d like to add business logic and behavior to your record objects using methods. However, this means that these objects are technically no longer plain data objects:\n\n\n```python\nclass Car:\n    def __init__(self, color, mileage, automatic):\n        self.color = color\n        self.mileage = mileage\n        self.automatic = automatic\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\ncar2\n```\n\n\n\n\n    \u003c__main__.Car at 0x7ff72e066ca0\u003e\n\n\n\n\n```python\n# Get the mileage:\ncar2.mileage\n```\n\n\n\n\n    40231.4\n\n\n\n\n```python\n# Classes are mutable:\ncar2.mileage = 12\ncar2.windshield = 'broken'\n```\n\n### dataclasses.dataclass: Python 3.7+ Data Classes\n\nData classes are available in Python 3.7 and above. They provide an excellent alternative to defining your own data storage classes from scratch.\n\nBy writing a data class instead of a plain Python class, your object instances get a few useful features out of the box that will save you some typing and manual implementation work:\n\nThe syntax for defining instance variables is shorter, since you donâ€™t need to implement the .__init__() method.\nInstances of your data class automatically get nice-looking string representation via an auto-generated .__repr__() method.\nInstance variables accept type annotations, making your data class self-documenting to a degree. Keep in mind that type annotations are just hints that are not enforced without a separate type-checking tool.\nData classes are typically created using the @dataclass decorator, as youâ€™ll see in the code example below:\n\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Car:\n    color: str\n    mileage: float\n    automatic: bool\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\n# Instances have a nice repr:\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=40231.4, automatic=False)\n\n\n\n\n```python\n# Accessing fields:\ncar1.mileage\n```\n\n\n\n\n    3812.4\n\n\n\n\n```python\n# Fields are mutable:\ncar2.mileage = 12\ncar2.windshield = 'broken'\n\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=12, automatic=False)\n\n\n\n\n```python\n# Type annotations are not enforced without a separate type checking tool like mypy\nCar(\"red\", \"NOT_A_FLOAT\", 99)\n```\n\n\n\n\n    Car(color='red', mileage='NOT_A_FLOAT', automatic=99)\n\n\n\n### collections.namedtuple: Convenient Data Objects\n\nThe namedtuple class available in Python 2.6+ provides an extension of the built-in tuple data type. Similar to defining a custom class, using namedtuple allows you to define reusable blueprints for your records that ensure the correct field names are used.\n\nnamedtuple objects are immutable, just like regular tuples. This means you canâ€™t add new fields or modify existing fields after the namedtuple instance is created.\n\nBesides that, namedtuple objects are, well . . . named tuples. Each object stored in them can be accessed through a unique identifier. This frees you from having to remember integer indexes or resort to workarounds like defining integer constants as mnemonics for your indexes.\n\nnamedtuple objects are implemented as regular Python classes internally. When it comes to memory usage, theyâ€™re also better than regular classes and just as memory efficient as regular tuples:\n\n\n```python\nfrom collections import namedtuple\nfrom sys import getsizeof\n\np1 = namedtuple('Point', 'x y z')(1, 2, 3)\np2 = (1, 2, 3)\n\np1\n```\n\n\n\n\n    Point(x=1, y=2, z=3)\n\n\n\n\n```python\np1.x, p1.y, p1.z\n```\n\n\n\n\n    (1, 2, 3)\n\n\n\n\n```python\ngetsizeof(p1)\n```\n\n\n\n\n    64\n\n\n\n\n```python\ngetsizeof(p2)\n```\n\n\n\n\n    64\n\n\n\nnamedtuple objects can be an **easy way to clean up your code and make it more readable** by enforcing a better structure for your data.\n\nI find that going from ad-hoc data types like dictionaries with a fixed format to namedtuple objects helps me to express the intent of my code more clearly. Often when I apply this refactoring, I magically come up with a better solution for the problem Iâ€™m facing.\n\nUsing namedtuple objects over regular (unstructured) tuples and dicts can also make your coworkersâ€™ lives easier by making the data thatâ€™s being passed around self-documenting, at least to a degree:\n\n\n```python\nCar = namedtuple('Car', 'color mileage automatic')\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=40231.4, automatic=False)\n\n\n\n\n```python\ntype(car1)\n```\n\n\n\n\n    __main__.Car\n\n\n\n\n```python\n# Fields are immtuable:\ncar2.mileage = 12\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-39-f7f9c602e34d\u003e in \u003cmodule\u003e\n          1 # Fields are immtuable:\n    ----\u003e 2 car2.mileage = 12\n\n\n    AttributeError: can't set attribute\n\n\n\n```python\ncar2.windshield = \"broken\"\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-40-72bb797ed6af\u003e in \u003cmodule\u003e\n    ----\u003e 1 car2.windshield = \"broken\"\n\n\n    AttributeError: 'Car' object has no attribute 'windshield'\n\n\n### typing.NamedTuple: Improved Namedtuples\n\nAdded in Python 3.6, typing.NamedTuple is the younger sibling of the namedtuple class in the collections module. Itâ€™s very similar to namedtuple, with the main difference being an updated syntax for defining new record types and added support for type hints.\n\nPlease note that type annotations are not enforced without a separate type-checking tool like mypy. But even without tool support, they can provide useful hints for other programmers (or be terribly confusing if the type hints become out of date):\n\n\n```python\nfrom typing import NamedTuple\n\nclass Car(NamedTuple):\n    color: str\n    mileage: float\n    automatic: bool\n\ncar1 = Car('red', 3812.4, True)\ncar2 = Car('blue', 40231.4, False)\n\ncar2\n```\n\n\n\n\n    Car(color='blue', mileage=40231.4, automatic=False)\n\n\n\n\n```python\n# Fields are immutable:\ncar1.mileage = 12\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-42-144528f7cc7a\u003e in \u003cmodule\u003e\n          1 # Fields are immutable:\n    ----\u003e 2 car1.mileage = 12\n\n\n    AttributeError: can't set attribute\n\n\n### struct.Struct: Serialized C Structs\n\nThe struct.Struct class converts between Python values and C structs serialized into Python bytes objects. For example, it can be used to handle binary data stored in files or coming in from network connections.\n\nStructs are defined using a mini language based on format strings that allows you to define the arrangement of various C data types like char, int, and long as well as their unsigned variants.\n\nSerialized structs are seldom used to represent data objects meant to be handled purely inside Python code. Theyâ€™re intended primarily as a **data exchange format** rather than as a way of holding data in memory thatâ€™s only used by Python code.\n\nIn some cases, packing primitive data into structs may use less memory than keeping it in other data types. However, in most cases that would be quite an advanced (and probably unnecessary) optimization:\n\n\n```python\nfrom struct import Struct\n\nMyStruct = Struct('i?f')\ndata = MyStruct.pack(23, False, 42.0)\n\n# All you get is a blob of data:\ndata\n```\n\n\n\n\n    b'\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00(B'\n\n\n\n\n```python\nMyStruct.unpack(data)\n```\n\n\n\n\n    (23, False, 42.0)\n\n\n\n### types.SimpleNamespace: Fancy Attribute Access\n\nHereâ€™s one more slightly obscure choice for implementing data objects in Python: types.SimpleNamespace. This class was added in Python 3.3 and provides attribute access to its namespace.\n\nThis means SimpleNamespace instances expose all of their keys as class attributes. You can use obj.key dotted attribute access instead of the obj['key'] square-bracket indexing syntax thatâ€™s used by regular dicts. All instances also include a meaningful __repr__ by default.\n\nAs its name proclaims, SimpleNamespace is simple! Itâ€™s basically a dictionary that allows attribute access and prints nicely. Attributes can be added, modified, and deleted freely:\n\n\n```python\nfrom types import SimpleNamespace\n\ncar1 = SimpleNamespace(color=\"red\", mileage=3812.4, automatic=True)\ncar1\n```\n\n\n\n\n    namespace(color='red', mileage=3812.4, automatic=True)\n\n\n\n\n```python\n# Instances support attribute access and are mutable:\ncar1.mileage = 12\ncar1.windshield = \"broken\"\ndel car1.automatic\n\ncar1\n```\n\n\n\n\n    namespace(color='red', mileage=12, windshield='broken')\n\n\n\n## Records, Structs, and Data Objects in Python: Summary\n\nAs youâ€™ve seen, thereâ€™s quite a number of different options for implementing records or data objects. Which type should you use for data objects in Python? Generally your decision will depend on your use case:\n\n* If you have only a few fields, then using a plain tuple object may be okay if the field order is easy to remember or field names are superfluous. For example, think of an (x, y, z) point in three-dimensional space.\n\n* If you need immutable fields, then plain tuples, collections.namedtuple, and typing.NamedTuple are all good options.\n\n* If you need to lock down field names to avoid typos, then collections.namedtuple and typing.NamedTuple are your friends.\n\n* If you want to keep things simple, then a plain dictionary object might be a good choice due to the convenient syntax that closely resembles JSON.\n\n* If you need full control over your data structure, then itâ€™s time to write a custom class with @property setters and getters.\n\n* If you need to add behavior (methods) to the object, then you should write a custom class, either from scratch, or using the dataclass decorator, or by extending collections.namedtuple or typing.NamedTuple.\n\n* If you need to pack data tightly to serialize it to disk or to send it over the network, then itâ€™s time to read up on struct.Struct because this is a great use case for it!\n\nIf youâ€™re looking for a safe default choice, then my general recommendation for implementing a plain record, struct, or data object in Python would be to use collections.namedtuple in Python 2.x and its younger sibling, typing.NamedTuple in Python 3.\n\n## Part 4. Sets and Multisets\n\nIn this section, youâ€™ll see how to implement mutable and immutable set and multiset (bag) data structures in Python using built-in data types and classes from the standard library.\n\nA set is an unordered collection of objects that doesnâ€™t allow duplicate elements. Typically, sets are used to quickly test a value for membership in the set, to insert or delete new values from a set, and to compute the union or intersection of two sets.\n\nIn a proper set implementation, membership tests are expected to run in fast **O(1)** time. Union, intersection, difference, and subset operations should take O(n) time on average. The set implementations included in Pythonâ€™s standard library follow these performance characteristics.\n\nJust like dictionaries, sets get special treatment in Python and have some syntactic sugar that makes them easy to create. For example, the curly-brace set expression syntax and set comprehensions allow you to conveniently define new set instances:\n\n\n```python\nvowels = {'a', 'e', 'i', 'o', 'u'}\nsquares = {x * x for x in range(10)}\n\nprint(vowels)\nprint(squares)\n```\n\n    {'e', 'a', 'i', 'o', 'u'}\n    {0, 1, 64, 4, 36, 9, 16, 49, 81, 25}\n\n\nBut be careful: To create an empty set youâ€™ll need to call the set() constructor. Using empty curly-braces ({}) is ambiguous and will create an empty dictionary instead.\n\n\n```python\n# null set using set() instead of {}\n\ndict_empty = {}\nset_empty = set()\n\nprint(type(dict_empty))\nprint(type(set_empty))\n```\n\n    \u003cclass 'dict'\u003e\n    \u003cclass 'set'\u003e\n\n\n### set: Your Go-To Set\n\nThe set type is the built-in set implementation in Python. Itâ€™s mutable and allows for the dynamic insertion and deletion of elements.\n\nPythonâ€™s sets are backed by the dict data type and share the same performance characteristics. Any hashable object can be stored in a set:\n\n\n```python\nvowels = {'a', 'e', 'i', 'o', 'u'}\n\n'e' in vowels\n```\n\n\n\n\n    True\n\n\n\n\n```python\nletters = set('alice')\nletters.intersection(vowels)\n```\n\n\n\n\n    {'a', 'e', 'i'}\n\n\n\n\n```python\nvowels.difference(letters)\n```\n\n\n\n\n    {'o', 'u'}\n\n\n\n\n```python\nletters_copy = letters.copy()\nletters_copy\n```\n\n\n\n\n    {'a', 'c', 'e', 'i', 'l'}\n\n\n\n\n```python\n# The isdisjoint() method returns True if two sets are disjoint sets. If not, it returns False\nletters_copy.isdisjoint(vowels)\n```\n\n\n\n\n    False\n\n\n\n\n```python\n![disjoint](https://cdn.programiz.com/sites/tutorial2program/files/python-disjoint-sets_0.png)\n```\n\n    zsh:1: unknown file attribute: h\n\n\n\n```python\nletters_copy.issubset(letters)\n```\n\n\n\n\n    True\n\n\n\n\n```python\nletters_copy.issuperset(letters)\n```\n\n\n\n\n    True\n\n\n\n\n```python\n# add elements\nvowels.add('x')\nvowels\n```\n\n\n\n\n    {'a', 'e', 'i', 'o', 'u', 'x'}\n\n\n\n\n```python\nvowels.union(letters)\n```\n\n\n\n\n    {'a', 'c', 'e', 'i', 'l', 'o', 'u', 'x'}\n\n\n\n\n```python\nlen(vowels)\n```\n\n\n\n\n    6\n\n\n\n\n```python\nvowels.discard('x')\nvowels\n```\n\n\n\n\n    {'a', 'e', 'i', 'o', 'u'}\n\n\n\n### frozenset: Immutable Sets\n\nThe frozenset class implements an immutable version of set that canâ€™t be changed after itâ€™s been constructed.\n\nfrozenset objects are static and allow only query operations on their elements, not inserts or deletions. Because frozenset objects are static and hashable, they can be used as dictionary keys or as elements of another set, something that isnâ€™t possible with regular (mutable) set objects:\n\n\n```python\nfs = frozenset({'a', 'e', 'i', 'o', 'u'})\nfs\n```\n\n\n\n\n    frozenset({'a', 'e', 'i', 'o', 'u'})\n\n\n\n\n```python\nfs.add('x')\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    AttributeError                            Traceback (most recent call last)\n    \n    \u003cipython-input-67-cb5eaf111fad\u003e in \u003cmodule\u003e\n    ----\u003e 1 fs.add('x')\n\n\n    AttributeError: 'frozenset' object has no attribute 'add'\n\n\n\n```python\n# Frozensets are hashable and can be used as dictionary keys:\nd = {frozenset({'a', 'e', 'i', 'o', 'u'}):'vowels'}\nd\n```\n\n\n\n\n    {frozenset({'a', 'e', 'i', 'o', 'u'}): 'vowels'}\n\n\n\n### collections.Counter: Multisets\n\nThe collections.Counter class in the Python standard library implements a multiset, or bag, type that allows elements in the set to have more than one occurrence.\n\nThis is useful if you need to keep track of not only if an element is part of a set, but also how many times itâ€™s included in the set:\n\n\n\n```python\nfrom collections import Counter\n\ninventory = Counter()\nloot = {'sward': 1, 'bread': 3}\ninventory.update(loot)\ninventory\n```\n\n\n\n\n    Counter({'sward': 1, 'bread': 3})\n\n\n\n\n```python\nmore_loot = {'sward': 2, 'apple': 3}\ninventory.update(more_loot)\ninventory\n```\n\n\n\n\n    Counter({'sward': 3, 'bread': 3, 'apple': 3})\n\n\n\nOne caveat for the Counter class is that youâ€™ll want to be careful when counting the number of elements in a Counter object. Calling len() returns the number of unique elements in the multiset, whereas the total number of elements can be retrieved using sum():\n\n\n\n```python\nlen(inventory)\n```\n\n\n\n\n    3\n\n\n\n\n```python\nsum(inventory.values())\n```\n\n\n\n\n    9\n\n\n\n### Sets and Multisets in Python: Summary\n\nSets are another useful and commonly used data structure included with Python and its standard library. Here are a few guidelines for deciding which one to use:\n\n* If you need a mutable set, then use the built-in set type.\n* If you need hashable objects that can be used as dictionary or set keys, then use a frozenset.\n* If you need a multiset, or bag, data structure, then use collections.Counter.\n\n\n## Part 5. Stacks (LIFOs)\n\nA stack is a collection of objects that supports fast **Last-In/First-Out (LIFO)** semantics for inserts and deletes. Unlike lists or arrays, stacks typically donâ€™t allow for random access to the objects they contain. The insert and delete operations are also often called **push and pop**.\n\nA useful real-world analogy for a stack data structure is a stack of plates. New plates are added to the top of the stack, and because the plates are precious and heavy, only the topmost plate can be moved. In other words, the last plate on the stack must be the first one removed (LIFO). To reach the plates that are lower down in the stack, the topmost plates must be removed one by one.\n\nPerformance-wise, a proper stack implementation is expected to take O(1) time for insert and delete operations.\n\nStacks have a wide range of uses in algorithms. For example, theyâ€™re used in language parsing as well as runtime memory management, which relies on a call stack. A short and beautiful algorithm using a stack is depth-first search (DFS) on a tree or graph data structure.\n\nPython ships with several stack implementations that each have slightly different characteristics. Letâ€™s take a look at them and compare their characteristics.\n\n\n### list: Simple, Built-In Stacks\n\nPythonâ€™s **built-in list** type makes a decent stack data structure as **it supports push and pop operations in amortized O(1) time**.\n\nPythonâ€™s lists are implemented as dynamic arrays internally, which means they occasionally need to resize the storage space for elements stored in them when elements are added or removed. The list over-allocates its backing storage so that not every push or pop requires resizing. As a result, you get an amortized O(1) time complexity for these operations.\n\nThe downside is that this makes their performance less consistent than the stable O(1) inserts and deletes provided by a linked listâ€“based implementation (as youâ€™ll see below with collections.deque). On the other hand, lists do provide fast O(1) time random access to elements on the stack, and this can be an added benefit.\n\nThereâ€™s an important performance caveat that you should be aware of when using lists as stacks: To get the amortized O(1) performance for inserts and deletes, new items must be added to the end of the list with the append() method and removed again from the end using pop(). For optimum performance, stacks based on Python lists should grow towards higher indexes and shrink towards lower ones.\n\nAdding and removing from the front is much slower and takes O(n) time, as the existing elements must be shifted around to make room for the new element. This is a performance antipattern that you should avoid as much as possible:\n\n\n```python\ns = []\ns.append('eat')\ns.append('sleep')\ns.append('code')\n\ns\n```\n\n\n\n\n    ['eat', 'sleep', 'code']\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'code'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\ns.pop()\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    IndexError                                Traceback (most recent call last)\n    \n    \u003cipython-input-79-c88c8c48122b\u003e in \u003cmodule\u003e\n    ----\u003e 1 s.pop()\n\n\n    IndexError: pop from empty list\n\n\n### collections.deque: Fast and Robust Stacks\n\nThe deque class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks.\n\nPythonâ€™s deque objects are implemented as **doubly-linked lists**, which gives them excellent and consistent performance for inserting and deleting elements but poor O(n) performance for randomly accessing elements in the middle of a stack.\n\nOverall, collections.deque is a great choice if youâ€™re looking for a stack data structure in Pythonâ€™s standard library that has the performance characteristics of a linked-list implementation:\n\n\n```python\nfrom collections import deque\n\ns = deque()\ns.append('eat')\ns.append('sleep')\ns.append('code')\n\ns\n```\n\n\n\n\n    deque(['eat', 'sleep', 'code'])\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'code'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\ns.pop()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\ns.pop()\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    IndexError                                Traceback (most recent call last)\n    \n    \u003cipython-input-84-c88c8c48122b\u003e in \u003cmodule\u003e\n    ----\u003e 1 s.pop()\n\n\n    IndexError: pop from an empty deque\n\n\n\n```python\ns.appendleft('a')\ns.appendleft('b')\ns.appendleft('c')\n\ns\n```\n\n\n\n\n    deque(['c', 'b', 'a'])\n\n\n\n\n```python\ns.popleft()\n```\n\n\n\n\n    'c'\n\n\n\n\n```python\ns.popleft()\n```\n\n\n\n\n    'b'\n\n\n\n\n```python\ns.popleft()\n```\n\n\n\n\n    'a'\n\n\n\n\n```python\ns.appendleft('a')\ns.appendleft('b')\ns.appendleft('c')\n\ns\n```\n\n\n\n\n    deque(['c', 'b', 'a'])\n\n\n\n\n```python\ns.reverse()\n```\n\n\n```python\ns\n```\n\n\n\n\n    deque(['a', 'b', 'c'])\n\n\n\n### queue.LifoQueue: Locking Semantics for Parallel Computing\n\nThe LifoQueue stack implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers.\n\nBesides LifoQueue, the queue module contains several other classes that implement multi-producer, multi-consumer queues that are useful for parallel computing.\n\nDepending on your use case, the locking semantics might be helpful, or they might just incur unneeded overhead. In this case, youâ€™d be better off using a list or a deque as a general-purpose stack:\n\n\n```python\nfrom queue import LifoQueue\n\ns = LifoQueue()\ns.put('eat')\ns.put('sleep')\ns.put('code')\n\ns\n```\n\n\n\n\n    \u003cqueue.LifoQueue at 0x7ff72efbcf70\u003e\n\n\n\n\n```python\ns.get()\n```\n\n\n\n\n    'code'\n\n\n\n\n```python\ns.get()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\ns.get()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\ns.get_nowait()\n```\n\n\n    ---------------------------------------------------------------------------\n    \n    Empty                                     Traceback (most recent call last)\n    \n    \u003cipython-input-96-0bd7ad76be38\u003e in \u003cmodule\u003e\n    ----\u003e 1 s.get_nowait()\n\n\n    ~/opt/anaconda3/lib/python3.8/queue.py in get_nowait(self)\n        196         raise the Empty exception.\n        197         '''\n    --\u003e 198         return self.get(block=False)\n        199 \n        200     # Override these methods to implement other queue organizations\n\n\n    ~/opt/anaconda3/lib/python3.8/queue.py in get(self, block, timeout)\n        165             if not block:\n        166                 if not self._qsize():\n    --\u003e 167                     raise Empty\n        168             elif timeout is None:\n        169                 while not self._qsize():\n\n\n    Empty: \n\n\n## Stack Implementations in Python: Summary\nAs youâ€™ve seen, Python ships with several implementations for a stack data structure. All of them have slightly different characteristics as well as performance and usage trade-offs.\n\nIf youâ€™re not looking for parallel processing support (or if you donâ€™t want to handle locking and unlocking manually), then your choice comes down to the built-in list type or collections.deque. The difference lies in the data structure used behind the scenes and overall ease of use.\n\nlist is backed by a dynamic array, which makes it great for fast random access but requires occasional resizing when elements are added or removed.\n\nThe list over-allocates its backing storage so that not every push or pop requires resizing, and you get an amortized O(1) time complexity for these operations. But you do need to be careful to only insert and remove items using append() and pop(). Otherwise, performance slows down to O(n).\n\ncollections.deque is backed by a doubly-linked list, which optimizes appends and deletes at both ends and provides consistent O(1) performance for these operations. Not only is its performance more stable, the deque class is also easier to use because you donâ€™t have to worry about adding or removing items from the wrong end.\n\nIn summary, collections.deque is an excellent choice for implementing a stack (LIFO queue) in Python.\n\n## Part 6. Queues (FIFOs)\n\nIn this section, youâ€™ll see how to implement a **First-In/First-Out (FIFO)** queue data structure using only built-in data types and classes from the Python standard library.\n\nA queue is a collection of objects that supports fast FIFO semantics for inserts and deletes. The insert and delete operations are sometimes called enqueue and dequeue. Unlike lists or arrays, queues typically donâ€™t allow for random access to the objects they contain.\n\nHereâ€™s a real-world analogy for a FIFO queue:\n\nImagine a line of Pythonistas waiting to pick up their conference badges on day one of PyCon registration. As new people enter the conference venue and queue up to receive their badges, they join the line (enqueue) at the back of the queue. Developers receive their badges and conference swag bags and then exit the line (dequeue) at the front of the queue.\n\nAnother way to memorize the characteristics of a queue data structure is to think of it as a pipe. You add ping-pong balls to one end, and they travel to the other end, where you remove them. While the balls are in the queue (a solid metal pipe) you canâ€™t get at them. The only way to interact with the balls in the queue is to add new ones at the back of the pipe (enqueue) or to remove them at the front (dequeue).\n\nQueues are similar to stacks. The difference between them lies in how items are removed. With a queue, you remove the item least recently added (FIFO) but with a stack, you remove the item most recently added (LIFO).\n\nPerformance-wise, a proper queue implementation is expected to take O(1) time for insert and delete operations. These are the two main operations performed on a queue, and in a correct implementation, they should be fast.\n\nQueues have a wide range of applications in algorithms and often help solve scheduling and parallel programming problems. A short and beautiful algorithm using a queue is breadth-first search (BFS) on a tree or graph data structure.\n\nScheduling algorithms often use priority queues internally. These are specialized queues. Instead of retrieving the next element by insertion time, a priority queue retrieves the highest-priority element. The priority of individual elements is decided by the queue based on the ordering applied to their keys.\n\nA regular queue, however, wonâ€™t reorder the items it carries. Just like in the pipe example, you get out what you put in, and in exactly that order.\n\nPython ships with several queue implementations that each have slightly different characteristics. Letâ€™s review them.\n\n### list: Terribly Sloooow Queues\n\nItâ€™s possible to use a regular list as a queue, but this is not ideal from a performance perspective. **Lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all the other elements by one, requiring O(n) time**.\n\nTherefore, I would **not recommend** using a list as a makeshift queue in Python unless youâ€™re dealing with only a small number of elements:\n\n\n```python\nq = []\nq.append('eat')\nq.append('sleep')\nq.append('code')\nq\n```\n\n\n\n\n    ['eat', 'sleep', 'code']\n\n\n\n\n```python\nq.pop(0)\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.pop(0)\n```\n\n\n\n\n    'sleep'\n\n\n\n### collections.deque: Fast and Robust Queues\n\nThe deque class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks.\n\nPythonâ€™s deque objects are implemented as doubly-linked lists. This gives them excellent and consistent performance for inserting and deleting elements, but poor O(n) performance for randomly accessing elements in the middle of the stack.\n\nAs a result, collections.deque is a great default choice if youâ€™re looking for a queue data structure in Pythonâ€™s standard library:\n\n\n```python\nfrom collections import deque\n\nq = deque()\nq.append('eat')\nq.append('sleep')\nq.append('code')\nq\n\n```\n\n\n\n\n    deque(['eat', 'sleep', 'code'])\n\n\n\n\n```python\nq.popleft()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.popleft()\n```\n\n\n\n\n    'sleep'\n\n\n\n### queue.Queue: Locking Semantics for Parallel Computing\n\nThe queue.Queue implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers.\n\nThe queue module contains several other classes implementing multi-producer, multi-consumer queues that are useful for parallel computing.\n\nDepending on your use case, the locking semantics might be helpful or just incur unneeded overhead. In this case, youâ€™d be better off using collections.deque as a general-purpose queue:\n\n\n```python\nfrom queue import Queue\n\nq = Queue()\nq.put('eat')\nq.put('sleep')\nq.put('code')\nq\n```\n\n\n\n\n    \u003cqueue.Queue at 0x7ff72f279130\u003e\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'sleep'\n\n\n\n\n```python\nq.get_nowait()\n```\n\n\n\n\n    'code'\n\n\n\n### multiprocessing.Queue: Shared Job Queues\n\nmultiprocessing.Queue is a shared job queue implementation that allows queued items to be processed in parallel by multiple concurrent workers. Process-based parallelization is popular in CPython due to the global interpreter lock (GIL) that prevents some forms of parallel execution on a single interpreter process.\n\nAs a specialized queue implementation meant for sharing data between processes, multiprocessing.Queue makes it easy to distribute work across multiple processes in order to work around the GIL limitations. This type of queue can store and transfer any pickleable object across process boundaries:\n\n\n\n\n```python\nfrom multiprocessing import Queue\n```\n\n\n```python\nq = Queue()\nq.put('eat')\nq.put('sleep')\nq.put('code')\nq\n```\n\n\n\n\n    \u003cmultiprocessing.queues.Queue at 0x7ff72efbe8b0\u003e\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'eat'\n\n\n\n\n```python\nq.get()\n```\n\n\n\n\n    'sleep'\n\n\n\n## Queues in Python: Summary\n\nPython includes several queue implementations as part of the core language and its standard library.\n\nlist objects can be used as queues, but this is generally not recommended due to slow performance.\n\nIf youâ€™re not looking for parallel processing support, then the implementation offered by **collections.deque** is an excellent default choice for implementing a FIFO queue data structure in Python. It provides the performance characteristics youâ€™d expect from a good queue implementation and can also be used as a stack (LIFO queue).\n\n## Part 7. Priority Queues\n\nA priority queue is a container data structure that manages a set of records with totally-ordered keys to provide quick access to the record with the smallest or largest key in the set.\n\nYou can think of a priority queue as a modified queue. Instead of retrieving the next element by insertion time, it retrieves the highest-priority element. The priority of individual elements is decided by the order applied to their keys.\n\nPriority queues are commonly used for dealing with scheduling problems. For example, you might use them to give precedence to tasks with higher urgency.\n\nThink about the job of an operating system task scheduler:\n\nIdeally, higher-priority tasks on the system (such as playing a real-time game) should take precedence over lower-priority tasks (such as downloading updates in the background). By organizing pending tasks in a priority queue that uses task urgency as the key, the task scheduler can quickly select the highest-priority tasks and allow them to run first.\n\nIn this section, youâ€™ll see a few options for how you can implement priority queues in Python using built-in data structures or data structures included in Pythonâ€™s standard library. Each implementation will have its own upsides and downsides, but in my mind thereâ€™s a clear winner for most common scenarios. Letâ€™s find out which one it is.\n\n### list: Manually Sorted Queues\n\nYou can use a sorted list to quickly identify and delete the smallest or largest element. The downside is that inserting new elements into a list is a slow O(n) operation.\n\nWhile the insertion point can be found in O(log n) time using bisect.insort in the standard library, this is always dominated by the slow insertion step.\n\nMaintaining the order by appending to the list and re-sorting also takes at least O(n log n) time. Another downside is that you must manually take care of re-sorting the list when new elements are inserted. Itâ€™s easy to introduce bugs by missing this step, and the burden is always on you, the developer.\n\nThis means sorted lists are only suitable as priority queues when there will be few insertions:\n\n\n```python\nq = []\nq.append((2, 'code'))\nq.append((1, 'eat'))\nq.append((3, 'sleep'))\n\n# Remember to re-sort every time a new element is inserted, or use bisect.insort()\nq.sort(reverse=True)\nq\n```\n\n\n\n\n    [(3, 'sleep'), (2, 'code'), (1, 'eat')]\n\n\n\n\n```python\nwhile q:\n    next_item = q.pop()\n    print(next_item)\n```\n\n    (1, 'eat')\n    (2, 'code')\n    (3, 'sleep')\n\n\n### heapq: List-Based Binary Heaps\n\nheapq is a binary heap implementation usually backed by a plain list, and it supports insertion and extraction of the smallest element in O(log n) time.\n\nThis module is a good choice for implementing priority queues in Python. Since heapq technically provides **only a min-heap implementation**, extra steps must be taken to ensure sort stability and other features typically expected from a practical priority queue:\n\n\n```python\nimport heapq\n\nq = []\nheapq.heappush(q, (2, \"code\"))\nheapq.heappush(q, (1, \"eat\"))\nheapq.heappush(q, (3, \"sleep\"))\nq\n```\n\n\n\n\n    [(1, 'eat'), (2, 'code'), (3, 'sleep')]\n\n\n\n\n```python\nwhile q:\n    next_item = heapq.heappop(q)\n    print(next_item)\n```\n\n    (1, 'eat')\n    (2, 'code')\n    (3, 'sleep')\n\n\n\n```python\nq = []\nheapq.heappush(q, (2, \"code\"))\nheapq.heappush(q, (1, \"eat\"))\nheapq.heappush(q, (3, \"sleep\"))\nheapq.heappush(q, (7, \"run\"))\nheapq.heappush(q, (6, \"drive\"))\nheapq.heappush(q, (4, \"swim\"))\nheapq.heappush(q, (5, \"idle\"))\nq\n\n```\n\n\n\n\n    [(1, 'eat'),\n     (2, 'code'),\n     (3, 'sleep'),\n     (7, 'run'),\n     (6, 'drive'),\n     (4, 'swim'),\n     (5, 'idle')]\n\n\n\n\n```python\nheapq.nlargest(3, q)\n```\n\n\n\n\n    [(7, 'run'), (6, 'drive'), (5, 'idle')]\n\n\n\n\n```python\nheapq.nsmallest(3, q)\n```\n\n\n\n\n    [(1, 'eat'), (2, 'code'), (3, 'sleep')]\n\n\n\n### queue.PriorityQueue: Beautiful Priority Queues\n\nqueue.PriorityQueue uses heapq internally and shares the same time and space complexities. The difference is that PriorityQueue is synchronized and provides locking semantics to support multiple concurrent producers and consumers.\n\nDepending on your use case, this might be helpful, or it might just slow your program down slightly. In any case, you might prefer the class-based interface provided by PriorityQueue over the function-based interface provided by heapq:\n\n\n```python\nfrom queue import PriorityQueue\n\nq = PriorityQueue()\nq.put((2, \"code\"))\nq.put((1, \"eat\"))\nq.put((3, \"sleep\"))\nq\n```\n\n\n\n\n    \u003cqueue.PriorityQueue at 0x7ff72e431400\u003e\n\n\n\n\n```python\nwhile not q.empty():\n    next_item = q.get()\n    print(next_item)\n```\n\n    (1, 'eat')\n    (2, 'code')\n    (3, 'sleep')\n\n\n## Priority Queues in Python: Summary\n\nPython includes several priority queue implementations ready for you to use.\n\nqueue.PriorityQueue stands out from the pack with a nice object-oriented interface and a name that clearly states its intent. It should be your preferred choice.\n\nIf youâ€™d like to avoid the locking overhead of queue.PriorityQueue, then using the heapq module directly is also a good option.\n\n\n\n# Conclusion: Python Data Structures\n\nThat concludes your tour of common data structures in Python. With the knowledge youâ€™ve gained here, youâ€™re ready to implement efficient data structures that are just right for your specific algorithm or use case.\n\nIn this tutorial, youâ€™ve learned:\n\nWhich common abstract data types are built into the Python standard library\nHow the most common abstract data types map to Pythonâ€™s naming scheme\nHow to put abstract data types to practical use in various algorithms\n\n\n```python\n\n```\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/python-programming":{"title":"Python Programming","content":"\n# Python Programming\n\n## Data Structure\n\n* [Python Data Structure Basics](notes/python-data-structure-basics.md)\n\n## Flask\n\n* [Hands on Flask](notes/hands-on-flask.md)\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/notes/web-development":{"title":"Web Development","content":"\n# Web Development\n\n## HTML \u0026 CSS\n\n* [Hands on HTML\u0026CSS](notes/hands-on-html-css.md)\n\n  \n\n## JavaScript\n\n* [Hands on JavaScript](notes/hands-on-javascript.md)\n\n  \n\n## Flask\n\n* [Hands on Flask](notes/hands-on-flask.md)\n\n","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null},"/oss_imgs":{"title":"","content":"# OSS IMGS\n\n![GPTçš„èƒ½åŠ›](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230510134612561.png) \n\n\n\n![å„ç§AIåº”ç”¨](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230510134941650.png)\n\n   \n\n![image-20230513170513009](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513170513009.png)\n\n\n\n![image-20230513170928664](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513170928664.png)\n\n![image-20230513171427469](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513171427469.png)\n\n![image-20230513174315520](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230513174315520.png)\n\n![image-20230514103513486](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230514103513486.png)\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230515181007117.png\" alt=\"image-20230515181007117\" style=\"zoom:50%;\" /\u003e\n\n\n\n\u003cimg src=\"https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230515181156709.png\" alt=\"image-20230515181156709\" style=\"zoom:50%;\" /\u003e\n\n![image-20230517174249859](https://happy3-data.oss-cn-hangzhou.aliyuncs.com/content-images/image-20230517174249859.png)","lastmodified":"2023-06-17T01:48:17.62862541Z","tags":null}}